---
base_model: meta-llama/Llama-3.2-3B-Instruct
library_name: peft
license: other
tags:
- llama-factory
- lora
- generated_from_trainer
model-index:
- name: train_llama_3_2_v3
  results: []
---

<!-- This model card has been generated automatically according to the information the Trainer had access to. You
should probably proofread and complete it, then remove this comment. -->

# train_llama_3_2_v3

This model is a fine-tuned version of [meta-llama/Llama-3.2-3B-Instruct](https://huggingface.co/meta-llama/Llama-3.2-3B-Instruct) on the identity dataset.
It achieves the following results on the evaluation set:
- Loss: 1.7460

## Model description

More information needed

## Intended uses & limitations

More information needed

## Training and evaluation data

More information needed

## Training procedure

### Training hyperparameters

The following hyperparameters were used during training:
- learning_rate: 5e-05
- train_batch_size: 2
- eval_batch_size: 2
- seed: 42
- gradient_accumulation_steps: 8
- total_train_batch_size: 16
- optimizer: Use OptimizerNames.ADAMW_TORCH with betas=(0.9,0.999) and epsilon=1e-08 and optimizer_args=No additional optimizer arguments
- lr_scheduler_type: cosine
- num_epochs: 3.0

### Training results

| Training Loss | Epoch  | Step | Validation Loss |
|:-------------:|:------:|:----:|:---------------:|
| 2.5154        | 0.0345 | 100  | 2.5854          |
| 2.5308        | 0.0690 | 200  | 2.4571          |
| 2.4777        | 0.1035 | 300  | 2.3708          |
| 2.3957        | 0.1381 | 400  | 2.3136          |
| 2.3156        | 0.1726 | 500  | 2.2640          |
| 2.2053        | 0.2071 | 600  | 2.2296          |
| 2.2898        | 0.2416 | 700  | 2.1912          |
| 2.1766        | 0.2761 | 800  | 2.1628          |
| 2.044         | 0.3106 | 900  | 2.1374          |
| 2.1752        | 0.3451 | 1000 | 2.1106          |
| 2.0766        | 0.3796 | 1100 | 2.0917          |
| 2.0813        | 0.4142 | 1200 | 2.0687          |
| 2.0522        | 0.4487 | 1300 | 2.0539          |
| 2.0214        | 0.4832 | 1400 | 2.0349          |
| 2.0858        | 0.5177 | 1500 | 2.0196          |
| 1.9037        | 0.5522 | 1600 | 2.0039          |
| 1.9161        | 0.5867 | 1700 | 1.9880          |
| 2.0706        | 0.6212 | 1800 | 1.9773          |
| 2.0887        | 0.6557 | 1900 | 1.9653          |
| 1.6599        | 0.6903 | 2000 | 1.9549          |
| 1.8803        | 0.7248 | 2100 | 1.9418          |
| 1.9146        | 0.7593 | 2200 | 1.9297          |
| 1.9548        | 0.7938 | 2300 | 1.9209          |
| 1.8417        | 0.8283 | 2400 | 1.9119          |
| 1.8702        | 0.8628 | 2500 | 1.8977          |
| 1.9438        | 0.8973 | 2600 | 1.8913          |
| 1.8781        | 0.9318 | 2700 | 1.8822          |
| 2.0422        | 0.9664 | 2800 | 1.8701          |
| 1.9024        | 1.0009 | 2900 | 1.8637          |
| 1.4639        | 1.0354 | 3000 | 1.8689          |
| 1.6678        | 1.0699 | 3100 | 1.8614          |
| 1.5044        | 1.1044 | 3200 | 1.8535          |
| 1.7035        | 1.1389 | 3300 | 1.8503          |
| 1.5468        | 1.1734 | 3400 | 1.8459          |
| 1.587         | 1.2079 | 3500 | 1.8355          |
| 1.6104        | 1.2425 | 3600 | 1.8404          |
| 1.7821        | 1.2770 | 3700 | 1.8304          |
| 1.487         | 1.3115 | 3800 | 1.8235          |
| 1.7095        | 1.3460 | 3900 | 1.8232          |
| 1.6268        | 1.3805 | 4000 | 1.8147          |
| 1.5578        | 1.4150 | 4100 | 1.8076          |
| 1.6169        | 1.4495 | 4200 | 1.8040          |
| 1.6035        | 1.4840 | 4300 | 1.7965          |
| 1.5906        | 1.5186 | 4400 | 1.7917          |
| 1.6043        | 1.5531 | 4500 | 1.7863          |
| 1.5725        | 1.5876 | 4600 | 1.7856          |
| 1.5248        | 1.6221 | 4700 | 1.7764          |
| 1.5631        | 1.6566 | 4800 | 1.7731          |
| 1.3422        | 1.6911 | 4900 | 1.7707          |
| 1.6494        | 1.7256 | 5000 | 1.7659          |
| 1.7179        | 1.7601 | 5100 | 1.7606          |
| 1.6765        | 1.7947 | 5200 | 1.7568          |
| 1.6113        | 1.8292 | 5300 | 1.7510          |
| 1.619         | 1.8637 | 5400 | 1.7485          |
| 1.5246        | 1.8982 | 5500 | 1.7455          |
| 1.4021        | 1.9327 | 5600 | 1.7405          |
| 1.6958        | 1.9672 | 5700 | 1.7376          |
| 1.2991        | 2.0017 | 5800 | 1.7353          |
| 1.3191        | 2.0362 | 5900 | 1.7638          |
| 1.4244        | 2.0708 | 6000 | 1.7622          |
| 1.2649        | 2.1053 | 6100 | 1.7668          |
| 1.4013        | 2.1398 | 6200 | 1.7670          |
| 1.1802        | 2.1743 | 6300 | 1.7632          |
| 1.3795        | 2.2088 | 6400 | 1.7630          |
| 1.4736        | 2.2433 | 6500 | 1.7611          |
| 1.3391        | 2.2778 | 6600 | 1.7593          |
| 1.2583        | 2.3123 | 6700 | 1.7569          |
| 1.3043        | 2.3469 | 6800 | 1.7578          |
| 1.461         | 2.3814 | 6900 | 1.7541          |
| 1.2073        | 2.4159 | 7000 | 1.7557          |
| 1.2022        | 2.4504 | 7100 | 1.7539          |
| 1.5023        | 2.4849 | 7200 | 1.7515          |
| 1.4705        | 2.5194 | 7300 | 1.7501          |
| 1.2087        | 2.5539 | 7400 | 1.7503          |
| 1.3631        | 2.5884 | 7500 | 1.7471          |
| 1.302         | 2.6230 | 7600 | 1.7483          |
| 1.4864        | 2.6575 | 7700 | 1.7477          |
| 1.2736        | 2.6920 | 7800 | 1.7463          |
| 1.2744        | 2.7265 | 7900 | 1.7467          |
| 1.2621        | 2.7610 | 8000 | 1.7460          |
| 1.2879        | 2.7955 | 8100 | 1.7459          |
| 1.3778        | 2.8300 | 8200 | 1.7459          |
| 1.1425        | 2.8645 | 8300 | 1.7462          |
| 1.2659        | 2.8991 | 8400 | 1.7460          |
| 1.2662        | 2.9336 | 8500 | 1.7460          |
| 1.3609        | 2.9681 | 8600 | 1.7459          |


### Framework versions

- PEFT 0.12.0
- Transformers 4.46.1
- Pytorch 2.4.0+cu121
- Datasets 3.0.0
- Tokenizers 0.20.3