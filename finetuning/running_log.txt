[INFO|2025-03-19 16:18:50] logging.py:157 >> Resuming training from saves/Llama-3.2-3B-Instruct/lora/train_llama_3_2_v3/checkpoint-700.

[INFO|2025-03-19 16:18:50] logging.py:157 >> Change `output_dir` or use `overwrite_output_dir` to avoid.

[INFO|2025-03-19 16:18:50] parser.py:355 >> Process rank: 0, device: cuda:0, n_gpu: 1, distributed training: False, compute dtype: torch.bfloat16

[INFO|2025-03-19 16:18:52] configuration_utils.py:679 >> loading configuration file config.json from cache at /home/quest/.cache/huggingface/hub/models--meta-llama--Llama-3.2-3B-Instruct/snapshots/0cb88a4f764b7a12671c53f0838cd831a0843b95/config.json

[INFO|2025-03-19 16:18:52] configuration_utils.py:746 >> Model config LlamaConfig {
  "_name_or_path": "meta-llama/Llama-3.2-3B-Instruct",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": [
    128001,
    128008,
    128009
  ],
  "head_dim": 128,
  "hidden_act": "silu",
  "hidden_size": 3072,
  "initializer_range": 0.02,
  "intermediate_size": 8192,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 24,
  "num_hidden_layers": 28,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 32.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": true,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.46.1",
  "use_cache": true,
  "vocab_size": 128256
}


[INFO|2025-03-19 16:18:52] tokenization_utils_base.py:2211 >> loading file tokenizer.json from cache at /home/quest/.cache/huggingface/hub/models--meta-llama--Llama-3.2-3B-Instruct/snapshots/0cb88a4f764b7a12671c53f0838cd831a0843b95/tokenizer.json

[INFO|2025-03-19 16:18:52] tokenization_utils_base.py:2211 >> loading file tokenizer.model from cache at None

[INFO|2025-03-19 16:18:52] tokenization_utils_base.py:2211 >> loading file added_tokens.json from cache at None

[INFO|2025-03-19 16:18:52] tokenization_utils_base.py:2211 >> loading file special_tokens_map.json from cache at /home/quest/.cache/huggingface/hub/models--meta-llama--Llama-3.2-3B-Instruct/snapshots/0cb88a4f764b7a12671c53f0838cd831a0843b95/special_tokens_map.json

[INFO|2025-03-19 16:18:52] tokenization_utils_base.py:2211 >> loading file tokenizer_config.json from cache at /home/quest/.cache/huggingface/hub/models--meta-llama--Llama-3.2-3B-Instruct/snapshots/0cb88a4f764b7a12671c53f0838cd831a0843b95/tokenizer_config.json

[INFO|2025-03-19 16:18:53] tokenization_utils_base.py:2475 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.

[INFO|2025-03-19 16:18:54] configuration_utils.py:679 >> loading configuration file config.json from cache at /home/quest/.cache/huggingface/hub/models--meta-llama--Llama-3.2-3B-Instruct/snapshots/0cb88a4f764b7a12671c53f0838cd831a0843b95/config.json

[INFO|2025-03-19 16:18:54] configuration_utils.py:746 >> Model config LlamaConfig {
  "_name_or_path": "meta-llama/Llama-3.2-3B-Instruct",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": [
    128001,
    128008,
    128009
  ],
  "head_dim": 128,
  "hidden_act": "silu",
  "hidden_size": 3072,
  "initializer_range": 0.02,
  "intermediate_size": 8192,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 24,
  "num_hidden_layers": 28,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 32.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": true,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.46.1",
  "use_cache": true,
  "vocab_size": 128256
}


[INFO|2025-03-19 16:18:55] tokenization_utils_base.py:2211 >> loading file tokenizer.json from cache at /home/quest/.cache/huggingface/hub/models--meta-llama--Llama-3.2-3B-Instruct/snapshots/0cb88a4f764b7a12671c53f0838cd831a0843b95/tokenizer.json

[INFO|2025-03-19 16:18:55] tokenization_utils_base.py:2211 >> loading file tokenizer.model from cache at None

[INFO|2025-03-19 16:18:55] tokenization_utils_base.py:2211 >> loading file added_tokens.json from cache at None

[INFO|2025-03-19 16:18:55] tokenization_utils_base.py:2211 >> loading file special_tokens_map.json from cache at /home/quest/.cache/huggingface/hub/models--meta-llama--Llama-3.2-3B-Instruct/snapshots/0cb88a4f764b7a12671c53f0838cd831a0843b95/special_tokens_map.json

[INFO|2025-03-19 16:18:55] tokenization_utils_base.py:2211 >> loading file tokenizer_config.json from cache at /home/quest/.cache/huggingface/hub/models--meta-llama--Llama-3.2-3B-Instruct/snapshots/0cb88a4f764b7a12671c53f0838cd831a0843b95/tokenizer_config.json

[INFO|2025-03-19 16:18:55] tokenization_utils_base.py:2475 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.

[INFO|2025-03-19 16:18:55] logging.py:157 >> Replace eos token: <|eot_id|>

[INFO|2025-03-19 16:18:55] logging.py:157 >> Add pad token: <|eot_id|>

[INFO|2025-03-19 16:18:55] logging.py:157 >> Loading dataset identity.json...

[INFO|2025-03-19 16:18:56] configuration_utils.py:679 >> loading configuration file config.json from cache at /home/quest/.cache/huggingface/hub/models--meta-llama--Llama-3.2-3B-Instruct/snapshots/0cb88a4f764b7a12671c53f0838cd831a0843b95/config.json

[INFO|2025-03-19 16:18:56] configuration_utils.py:746 >> Model config LlamaConfig {
  "_name_or_path": "meta-llama/Llama-3.2-3B-Instruct",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": [
    128001,
    128008,
    128009
  ],
  "head_dim": 128,
  "hidden_act": "silu",
  "hidden_size": 3072,
  "initializer_range": 0.02,
  "intermediate_size": 8192,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 24,
  "num_hidden_layers": 28,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 32.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": true,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.46.1",
  "use_cache": true,
  "vocab_size": 128256
}


[INFO|2025-03-19 16:18:56] modeling_utils.py:3937 >> loading weights file model.safetensors from cache at /home/quest/.cache/huggingface/hub/models--meta-llama--Llama-3.2-3B-Instruct/snapshots/0cb88a4f764b7a12671c53f0838cd831a0843b95/model.safetensors.index.json

[INFO|2025-03-19 16:18:56] modeling_utils.py:1670 >> Instantiating LlamaForCausalLM model under default dtype torch.bfloat16.

[INFO|2025-03-19 16:18:56] configuration_utils.py:1096 >> Generate config GenerationConfig {
  "bos_token_id": 128000,
  "eos_token_id": [
    128001,
    128008,
    128009
  ]
}


[INFO|2025-03-19 16:18:59] modeling_utils.py:4800 >> All model checkpoint weights were used when initializing LlamaForCausalLM.


[INFO|2025-03-19 16:18:59] modeling_utils.py:4808 >> All the weights of LlamaForCausalLM were initialized from the model checkpoint at meta-llama/Llama-3.2-3B-Instruct.
If your task is similar to the task the model of the checkpoint was trained on, you can already use LlamaForCausalLM for predictions without further training.

[INFO|2025-03-19 16:18:59] configuration_utils.py:1051 >> loading configuration file generation_config.json from cache at /home/quest/.cache/huggingface/hub/models--meta-llama--Llama-3.2-3B-Instruct/snapshots/0cb88a4f764b7a12671c53f0838cd831a0843b95/generation_config.json

[INFO|2025-03-19 16:18:59] configuration_utils.py:1096 >> Generate config GenerationConfig {
  "bos_token_id": 128000,
  "do_sample": true,
  "eos_token_id": [
    128001,
    128008,
    128009
  ],
  "temperature": 0.6,
  "top_p": 0.9
}


[INFO|2025-03-19 16:18:59] logging.py:157 >> Gradient checkpointing enabled.

[INFO|2025-03-19 16:18:59] logging.py:157 >> Using torch SDPA for faster training and inference.

[INFO|2025-03-19 16:18:59] logging.py:157 >> Upcasting trainable params to float32.

[INFO|2025-03-19 16:18:59] logging.py:157 >> Fine-tuning method: LoRA

[INFO|2025-03-19 16:18:59] logging.py:157 >> Found linear modules: v_proj,q_proj,gate_proj,o_proj,down_proj,k_proj,up_proj

[INFO|2025-03-19 16:18:59] logging.py:157 >> trainable params: 12,156,928 || all params: 3,224,906,752 || trainable%: 0.3770

[INFO|2025-03-19 16:18:59] trainer.py:698 >> Using auto half precision backend

[INFO|2025-03-19 16:18:59] trainer.py:2716 >> Loading model from saves/Llama-3.2-3B-Instruct/lora/train_llama_3_2_v3/checkpoint-700.

[INFO|2025-03-19 16:19:00] logging.py:157 >> Using LoRA+ optimizer with loraplus lr ratio 16.00.

[INFO|2025-03-19 16:19:00] trainer.py:2313 >> ***** Running training *****

[INFO|2025-03-19 16:19:00] trainer.py:2314 >>   Num examples = 46,360

[INFO|2025-03-19 16:19:00] trainer.py:2315 >>   Num Epochs = 3

[INFO|2025-03-19 16:19:00] trainer.py:2316 >>   Instantaneous batch size per device = 2

[INFO|2025-03-19 16:19:00] trainer.py:2319 >>   Total train batch size (w. parallel, distributed & accumulation) = 16

[INFO|2025-03-19 16:19:00] trainer.py:2320 >>   Gradient Accumulation steps = 8

[INFO|2025-03-19 16:19:00] trainer.py:2321 >>   Total optimization steps = 8,691

[INFO|2025-03-19 16:19:00] trainer.py:2322 >>   Number of trainable parameters = 12,156,928

[INFO|2025-03-19 16:19:00] trainer.py:2344 >>   Continuing training from checkpoint, will skip to saved global_step

[INFO|2025-03-19 16:19:00] trainer.py:2345 >>   Continuing training from epoch 0

[INFO|2025-03-19 16:19:00] trainer.py:2346 >>   Continuing training from global step 700

[INFO|2025-03-19 16:19:00] trainer.py:2348 >>   Will skip the first 0 epochs then the first 5600 batches in the first epoch.

[INFO|2025-03-19 16:19:14] logging.py:157 >> {'loss': 2.2466, 'learning_rate': 4.9193e-05, 'epoch': 0.24}

[INFO|2025-03-19 16:19:28] logging.py:157 >> {'loss': 2.0960, 'learning_rate': 4.9181e-05, 'epoch': 0.25}

[INFO|2025-03-19 16:19:42] logging.py:157 >> {'loss': 2.0803, 'learning_rate': 4.9170e-05, 'epoch': 0.25}

[INFO|2025-03-19 16:19:56] logging.py:157 >> {'loss': 2.2210, 'learning_rate': 4.9158e-05, 'epoch': 0.25}

[INFO|2025-03-19 16:20:11] logging.py:157 >> {'loss': 2.2833, 'learning_rate': 4.9146e-05, 'epoch': 0.25}

[INFO|2025-03-19 16:20:25] logging.py:157 >> {'loss': 2.3213, 'learning_rate': 4.9135e-05, 'epoch': 0.25}

[INFO|2025-03-19 16:20:38] logging.py:157 >> {'loss': 2.2672, 'learning_rate': 4.9123e-05, 'epoch': 0.25}

[INFO|2025-03-19 16:20:53] logging.py:157 >> {'loss': 2.0955, 'learning_rate': 4.9111e-05, 'epoch': 0.26}

[INFO|2025-03-19 16:21:08] logging.py:157 >> {'loss': 2.2688, 'learning_rate': 4.9099e-05, 'epoch': 0.26}

[INFO|2025-03-19 16:21:23] logging.py:157 >> {'loss': 2.1893, 'learning_rate': 4.9087e-05, 'epoch': 0.26}

[INFO|2025-03-19 16:21:36] logging.py:157 >> {'loss': 2.0338, 'learning_rate': 4.9075e-05, 'epoch': 0.26}

[INFO|2025-03-19 16:21:52] logging.py:157 >> {'loss': 2.0570, 'learning_rate': 4.9063e-05, 'epoch': 0.26}

[INFO|2025-03-19 16:22:08] logging.py:157 >> {'loss': 2.0493, 'learning_rate': 4.9050e-05, 'epoch': 0.26}

[INFO|2025-03-19 16:22:24] logging.py:157 >> {'loss': 2.2591, 'learning_rate': 4.9038e-05, 'epoch': 0.27}

[INFO|2025-03-19 16:22:38] logging.py:157 >> {'loss': 2.0581, 'learning_rate': 4.9025e-05, 'epoch': 0.27}

[INFO|2025-03-19 16:22:51] logging.py:157 >> {'loss': 2.2549, 'learning_rate': 4.9013e-05, 'epoch': 0.27}

[INFO|2025-03-19 16:23:06] logging.py:157 >> {'loss': 2.1140, 'learning_rate': 4.9000e-05, 'epoch': 0.27}

[INFO|2025-03-19 16:23:20] logging.py:157 >> {'loss': 2.3829, 'learning_rate': 4.8988e-05, 'epoch': 0.27}

[INFO|2025-03-19 16:23:34] logging.py:157 >> {'loss': 2.0212, 'learning_rate': 4.8975e-05, 'epoch': 0.27}

[INFO|2025-03-19 16:23:49] logging.py:157 >> {'loss': 2.1766, 'learning_rate': 4.8962e-05, 'epoch': 0.28}

[INFO|2025-03-19 16:23:49] trainer.py:4117 >> 
***** Running Evaluation *****

[INFO|2025-03-19 16:23:49] trainer.py:4119 >>   Num examples = 5152

[INFO|2025-03-19 16:23:49] trainer.py:4122 >>   Batch size = 2

[INFO|2025-03-19 16:28:47] trainer.py:3801 >> Saving model checkpoint to saves/Llama-3.2-3B-Instruct/lora/train_llama_3_2_v3/checkpoint-800

[INFO|2025-03-19 16:28:51] configuration_utils.py:679 >> loading configuration file config.json from cache at /home/quest/.cache/huggingface/hub/models--meta-llama--Llama-3.2-3B-Instruct/snapshots/0cb88a4f764b7a12671c53f0838cd831a0843b95/config.json

[INFO|2025-03-19 16:28:51] configuration_utils.py:746 >> Model config LlamaConfig {
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": [
    128001,
    128008,
    128009
  ],
  "head_dim": 128,
  "hidden_act": "silu",
  "hidden_size": 3072,
  "initializer_range": 0.02,
  "intermediate_size": 8192,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 24,
  "num_hidden_layers": 28,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 32.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": true,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.46.1",
  "use_cache": true,
  "vocab_size": 128256
}


[INFO|2025-03-19 16:28:51] tokenization_utils_base.py:2646 >> tokenizer config file saved in saves/Llama-3.2-3B-Instruct/lora/train_llama_3_2_v3/checkpoint-800/tokenizer_config.json

[INFO|2025-03-19 16:28:51] tokenization_utils_base.py:2655 >> Special tokens file saved in saves/Llama-3.2-3B-Instruct/lora/train_llama_3_2_v3/checkpoint-800/special_tokens_map.json

[INFO|2025-03-19 16:29:05] logging.py:157 >> {'loss': 2.1126, 'learning_rate': 4.8949e-05, 'epoch': 0.28}

[INFO|2025-03-19 16:29:20] logging.py:157 >> {'loss': 2.2462, 'learning_rate': 4.8936e-05, 'epoch': 0.28}

[INFO|2025-03-19 16:29:36] logging.py:157 >> {'loss': 2.1222, 'learning_rate': 4.8923e-05, 'epoch': 0.28}

[INFO|2025-03-19 16:29:50] logging.py:157 >> {'loss': 2.0995, 'learning_rate': 4.8910e-05, 'epoch': 0.28}

[INFO|2025-03-19 16:30:03] logging.py:157 >> {'loss': 2.1781, 'learning_rate': 4.8897e-05, 'epoch': 0.28}

[INFO|2025-03-19 16:30:19] logging.py:157 >> {'loss': 2.1221, 'learning_rate': 4.8883e-05, 'epoch': 0.29}

[INFO|2025-03-19 16:30:34] logging.py:157 >> {'loss': 2.1311, 'learning_rate': 4.8870e-05, 'epoch': 0.29}

[INFO|2025-03-19 16:30:48] logging.py:157 >> {'loss': 1.9454, 'learning_rate': 4.8856e-05, 'epoch': 0.29}

[INFO|2025-03-19 16:31:03] logging.py:157 >> {'loss': 2.1065, 'learning_rate': 4.8843e-05, 'epoch': 0.29}

[INFO|2025-03-19 16:31:18] logging.py:157 >> {'loss': 2.1358, 'learning_rate': 4.8829e-05, 'epoch': 0.29}

[INFO|2025-03-19 16:31:32] logging.py:157 >> {'loss': 2.1951, 'learning_rate': 4.8815e-05, 'epoch': 0.30}

[INFO|2025-03-19 16:31:45] logging.py:157 >> {'loss': 2.0331, 'learning_rate': 4.8802e-05, 'epoch': 0.30}

[INFO|2025-03-19 16:31:59] logging.py:157 >> {'loss': 2.2122, 'learning_rate': 4.8788e-05, 'epoch': 0.30}

[INFO|2025-03-19 16:32:14] logging.py:157 >> {'loss': 2.0360, 'learning_rate': 4.8774e-05, 'epoch': 0.30}

[INFO|2025-03-19 16:32:28] logging.py:157 >> {'loss': 2.1637, 'learning_rate': 4.8760e-05, 'epoch': 0.30}

[INFO|2025-03-19 16:32:43] logging.py:157 >> {'loss': 2.1569, 'learning_rate': 4.8746e-05, 'epoch': 0.30}

[INFO|2025-03-19 16:32:58] logging.py:157 >> {'loss': 2.2446, 'learning_rate': 4.8732e-05, 'epoch': 0.31}

[INFO|2025-03-19 16:33:13] logging.py:157 >> {'loss': 2.2542, 'learning_rate': 4.8717e-05, 'epoch': 0.31}

[INFO|2025-03-19 16:33:25] logging.py:157 >> {'loss': 2.0549, 'learning_rate': 4.8703e-05, 'epoch': 0.31}

[INFO|2025-03-19 16:33:40] logging.py:157 >> {'loss': 2.0440, 'learning_rate': 4.8689e-05, 'epoch': 0.31}

[INFO|2025-03-19 16:33:40] trainer.py:4117 >> 
***** Running Evaluation *****

[INFO|2025-03-19 16:33:40] trainer.py:4119 >>   Num examples = 5152

[INFO|2025-03-19 16:33:40] trainer.py:4122 >>   Batch size = 2

[INFO|2025-03-19 16:38:39] trainer.py:3801 >> Saving model checkpoint to saves/Llama-3.2-3B-Instruct/lora/train_llama_3_2_v3/checkpoint-900

[INFO|2025-03-19 16:38:41] configuration_utils.py:679 >> loading configuration file config.json from cache at /home/quest/.cache/huggingface/hub/models--meta-llama--Llama-3.2-3B-Instruct/snapshots/0cb88a4f764b7a12671c53f0838cd831a0843b95/config.json

[INFO|2025-03-19 16:38:41] configuration_utils.py:746 >> Model config LlamaConfig {
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": [
    128001,
    128008,
    128009
  ],
  "head_dim": 128,
  "hidden_act": "silu",
  "hidden_size": 3072,
  "initializer_range": 0.02,
  "intermediate_size": 8192,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 24,
  "num_hidden_layers": 28,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 32.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": true,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.46.1",
  "use_cache": true,
  "vocab_size": 128256
}


[INFO|2025-03-19 16:38:41] tokenization_utils_base.py:2646 >> tokenizer config file saved in saves/Llama-3.2-3B-Instruct/lora/train_llama_3_2_v3/checkpoint-900/tokenizer_config.json

[INFO|2025-03-19 16:38:41] tokenization_utils_base.py:2655 >> Special tokens file saved in saves/Llama-3.2-3B-Instruct/lora/train_llama_3_2_v3/checkpoint-900/special_tokens_map.json

[INFO|2025-03-19 16:38:55] logging.py:157 >> {'loss': 2.1711, 'learning_rate': 4.8674e-05, 'epoch': 0.31}

[INFO|2025-03-19 16:39:10] logging.py:157 >> {'loss': 2.0948, 'learning_rate': 4.8660e-05, 'epoch': 0.31}

[INFO|2025-03-19 16:39:26] logging.py:157 >> {'loss': 2.0326, 'learning_rate': 4.8645e-05, 'epoch': 0.32}

[INFO|2025-03-19 16:39:41] logging.py:157 >> {'loss': 2.0287, 'learning_rate': 4.8630e-05, 'epoch': 0.32}

[INFO|2025-03-19 16:39:55] logging.py:157 >> {'loss': 2.0706, 'learning_rate': 4.8615e-05, 'epoch': 0.32}

[INFO|2025-03-19 16:40:08] logging.py:157 >> {'loss': 2.0982, 'learning_rate': 4.8601e-05, 'epoch': 0.32}

[INFO|2025-03-19 16:40:22] logging.py:157 >> {'loss': 1.9799, 'learning_rate': 4.8586e-05, 'epoch': 0.32}

[INFO|2025-03-19 16:40:36] logging.py:157 >> {'loss': 2.1802, 'learning_rate': 4.8571e-05, 'epoch': 0.32}

[INFO|2025-03-19 16:40:50] logging.py:157 >> {'loss': 1.9173, 'learning_rate': 4.8556e-05, 'epoch': 0.33}

[INFO|2025-03-19 16:41:03] logging.py:157 >> {'loss': 2.3385, 'learning_rate': 4.8540e-05, 'epoch': 0.33}

[INFO|2025-03-19 16:41:18] logging.py:157 >> {'loss': 1.9890, 'learning_rate': 4.8525e-05, 'epoch': 0.33}

[INFO|2025-03-19 16:41:32] logging.py:157 >> {'loss': 2.0081, 'learning_rate': 4.8510e-05, 'epoch': 0.33}

[INFO|2025-03-19 16:41:48] logging.py:157 >> {'loss': 2.0374, 'learning_rate': 4.8494e-05, 'epoch': 0.33}

[INFO|2025-03-19 16:42:03] logging.py:157 >> {'loss': 2.1058, 'learning_rate': 4.8479e-05, 'epoch': 0.33}

[INFO|2025-03-19 16:42:19] logging.py:157 >> {'loss': 2.2172, 'learning_rate': 4.8463e-05, 'epoch': 0.34}

[INFO|2025-03-19 16:42:34] logging.py:157 >> {'loss': 2.0587, 'learning_rate': 4.8448e-05, 'epoch': 0.34}

[INFO|2025-03-19 16:42:49] logging.py:157 >> {'loss': 2.1366, 'learning_rate': 4.8432e-05, 'epoch': 0.34}

[INFO|2025-03-19 16:43:03] logging.py:157 >> {'loss': 2.0369, 'learning_rate': 4.8416e-05, 'epoch': 0.34}

[INFO|2025-03-19 16:43:18] logging.py:157 >> {'loss': 2.3653, 'learning_rate': 4.8400e-05, 'epoch': 0.34}

[INFO|2025-03-19 16:43:33] logging.py:157 >> {'loss': 2.1752, 'learning_rate': 4.8384e-05, 'epoch': 0.35}

[INFO|2025-03-19 16:43:33] trainer.py:4117 >> 
***** Running Evaluation *****

[INFO|2025-03-19 16:43:33] trainer.py:4119 >>   Num examples = 5152

[INFO|2025-03-19 16:43:33] trainer.py:4122 >>   Batch size = 2

[INFO|2025-03-19 16:48:35] trainer.py:3801 >> Saving model checkpoint to saves/Llama-3.2-3B-Instruct/lora/train_llama_3_2_v3/checkpoint-1000

[INFO|2025-03-19 16:48:35] configuration_utils.py:679 >> loading configuration file config.json from cache at /home/quest/.cache/huggingface/hub/models--meta-llama--Llama-3.2-3B-Instruct/snapshots/0cb88a4f764b7a12671c53f0838cd831a0843b95/config.json

[INFO|2025-03-19 16:48:35] configuration_utils.py:746 >> Model config LlamaConfig {
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": [
    128001,
    128008,
    128009
  ],
  "head_dim": 128,
  "hidden_act": "silu",
  "hidden_size": 3072,
  "initializer_range": 0.02,
  "intermediate_size": 8192,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 24,
  "num_hidden_layers": 28,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 32.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": true,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.46.1",
  "use_cache": true,
  "vocab_size": 128256
}


[INFO|2025-03-19 16:48:36] tokenization_utils_base.py:2646 >> tokenizer config file saved in saves/Llama-3.2-3B-Instruct/lora/train_llama_3_2_v3/checkpoint-1000/tokenizer_config.json

[INFO|2025-03-19 16:48:36] tokenization_utils_base.py:2655 >> Special tokens file saved in saves/Llama-3.2-3B-Instruct/lora/train_llama_3_2_v3/checkpoint-1000/special_tokens_map.json

[INFO|2025-03-19 16:48:51] logging.py:157 >> {'loss': 2.1316, 'learning_rate': 4.8368e-05, 'epoch': 0.35}

[INFO|2025-03-19 16:49:05] logging.py:157 >> {'loss': 2.0874, 'learning_rate': 4.8352e-05, 'epoch': 0.35}

[INFO|2025-03-19 16:49:21] logging.py:157 >> {'loss': 1.9885, 'learning_rate': 4.8336e-05, 'epoch': 0.35}

[INFO|2025-03-19 16:49:37] logging.py:157 >> {'loss': 1.8906, 'learning_rate': 4.8320e-05, 'epoch': 0.35}

[INFO|2025-03-19 16:49:51] logging.py:157 >> {'loss': 2.0319, 'learning_rate': 4.8304e-05, 'epoch': 0.35}

[INFO|2025-03-19 16:50:05] logging.py:157 >> {'loss': 2.1344, 'learning_rate': 4.8287e-05, 'epoch': 0.36}

[INFO|2025-03-19 16:50:20] logging.py:157 >> {'loss': 2.1988, 'learning_rate': 4.8271e-05, 'epoch': 0.36}

[INFO|2025-03-19 16:50:34] logging.py:157 >> {'loss': 2.1588, 'learning_rate': 4.8254e-05, 'epoch': 0.36}

[INFO|2025-03-19 16:50:50] logging.py:157 >> {'loss': 1.9679, 'learning_rate': 4.8237e-05, 'epoch': 0.36}

[INFO|2025-03-19 16:51:05] logging.py:157 >> {'loss': 2.0148, 'learning_rate': 4.8221e-05, 'epoch': 0.36}

[INFO|2025-03-19 16:51:19] logging.py:157 >> {'loss': 2.1805, 'learning_rate': 4.8204e-05, 'epoch': 0.36}

[INFO|2025-03-19 16:51:34] logging.py:157 >> {'loss': 2.0601, 'learning_rate': 4.8187e-05, 'epoch': 0.37}

[INFO|2025-03-19 16:51:48] logging.py:157 >> {'loss': 1.9234, 'learning_rate': 4.8170e-05, 'epoch': 0.37}

[INFO|2025-03-19 16:52:03] logging.py:157 >> {'loss': 1.9845, 'learning_rate': 4.8153e-05, 'epoch': 0.37}

[INFO|2025-03-19 16:52:19] logging.py:157 >> {'loss': 1.9463, 'learning_rate': 4.8136e-05, 'epoch': 0.37}

[INFO|2025-03-19 16:52:33] logging.py:157 >> {'loss': 1.9322, 'learning_rate': 4.8119e-05, 'epoch': 0.37}

[INFO|2025-03-19 16:52:48] logging.py:157 >> {'loss': 2.0132, 'learning_rate': 4.8102e-05, 'epoch': 0.37}

[INFO|2025-03-19 16:53:04] logging.py:157 >> {'loss': 2.0649, 'learning_rate': 4.8084e-05, 'epoch': 0.38}

[INFO|2025-03-19 16:53:18] logging.py:157 >> {'loss': 2.2614, 'learning_rate': 4.8067e-05, 'epoch': 0.38}

[INFO|2025-03-19 16:53:32] logging.py:157 >> {'loss': 2.0766, 'learning_rate': 4.8050e-05, 'epoch': 0.38}

[INFO|2025-03-19 16:53:32] trainer.py:4117 >> 
***** Running Evaluation *****

[INFO|2025-03-19 16:53:32] trainer.py:4119 >>   Num examples = 5152

[INFO|2025-03-19 16:53:32] trainer.py:4122 >>   Batch size = 2

[INFO|2025-03-19 16:58:31] trainer.py:3801 >> Saving model checkpoint to saves/Llama-3.2-3B-Instruct/lora/train_llama_3_2_v3/checkpoint-1100

[INFO|2025-03-19 16:58:32] configuration_utils.py:679 >> loading configuration file config.json from cache at /home/quest/.cache/huggingface/hub/models--meta-llama--Llama-3.2-3B-Instruct/snapshots/0cb88a4f764b7a12671c53f0838cd831a0843b95/config.json

[INFO|2025-03-19 16:58:32] configuration_utils.py:746 >> Model config LlamaConfig {
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": [
    128001,
    128008,
    128009
  ],
  "head_dim": 128,
  "hidden_act": "silu",
  "hidden_size": 3072,
  "initializer_range": 0.02,
  "intermediate_size": 8192,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 24,
  "num_hidden_layers": 28,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 32.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": true,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.46.1",
  "use_cache": true,
  "vocab_size": 128256
}


[INFO|2025-03-19 16:58:32] tokenization_utils_base.py:2646 >> tokenizer config file saved in saves/Llama-3.2-3B-Instruct/lora/train_llama_3_2_v3/checkpoint-1100/tokenizer_config.json

[INFO|2025-03-19 16:58:32] tokenization_utils_base.py:2655 >> Special tokens file saved in saves/Llama-3.2-3B-Instruct/lora/train_llama_3_2_v3/checkpoint-1100/special_tokens_map.json

[INFO|2025-03-19 16:58:45] logging.py:157 >> {'loss': 2.0655, 'learning_rate': 4.8032e-05, 'epoch': 0.38}

[INFO|2025-03-19 16:59:00] logging.py:157 >> {'loss': 2.3026, 'learning_rate': 4.8014e-05, 'epoch': 0.38}

[INFO|2025-03-19 16:59:14] logging.py:157 >> {'loss': 2.1253, 'learning_rate': 4.7997e-05, 'epoch': 0.38}

[INFO|2025-03-19 16:59:28] logging.py:157 >> {'loss': 2.0622, 'learning_rate': 4.7979e-05, 'epoch': 0.39}

[INFO|2025-03-19 16:59:45] logging.py:157 >> {'loss': 2.1748, 'learning_rate': 4.7961e-05, 'epoch': 0.39}

[INFO|2025-03-19 16:59:59] logging.py:157 >> {'loss': 2.0795, 'learning_rate': 4.7943e-05, 'epoch': 0.39}

[INFO|2025-03-19 17:00:14] logging.py:157 >> {'loss': 1.8628, 'learning_rate': 4.7925e-05, 'epoch': 0.39}

[INFO|2025-03-19 17:00:29] logging.py:157 >> {'loss': 1.9739, 'learning_rate': 4.7907e-05, 'epoch': 0.39}

[INFO|2025-03-19 17:00:44] logging.py:157 >> {'loss': 2.1656, 'learning_rate': 4.7889e-05, 'epoch': 0.40}

[INFO|2025-03-19 17:01:00] logging.py:157 >> {'loss': 1.9276, 'learning_rate': 4.7871e-05, 'epoch': 0.40}

[INFO|2025-03-19 17:01:15] logging.py:157 >> {'loss': 1.9992, 'learning_rate': 4.7853e-05, 'epoch': 0.40}

[INFO|2025-03-19 17:01:30] logging.py:157 >> {'loss': 2.0083, 'learning_rate': 4.7834e-05, 'epoch': 0.40}

[INFO|2025-03-19 17:01:44] logging.py:157 >> {'loss': 1.9970, 'learning_rate': 4.7816e-05, 'epoch': 0.40}

[INFO|2025-03-19 17:01:59] logging.py:157 >> {'loss': 2.1092, 'learning_rate': 4.7797e-05, 'epoch': 0.40}

[INFO|2025-03-19 17:02:14] logging.py:157 >> {'loss': 2.1241, 'learning_rate': 4.7779e-05, 'epoch': 0.41}

[INFO|2025-03-19 17:02:29] logging.py:157 >> {'loss': 2.0598, 'learning_rate': 4.7760e-05, 'epoch': 0.41}

[INFO|2025-03-19 17:02:46] logging.py:157 >> {'loss': 2.2654, 'learning_rate': 4.7741e-05, 'epoch': 0.41}

[INFO|2025-03-19 17:03:00] logging.py:157 >> {'loss': 2.0275, 'learning_rate': 4.7723e-05, 'epoch': 0.41}

[INFO|2025-03-19 17:03:14] logging.py:157 >> {'loss': 2.1198, 'learning_rate': 4.7704e-05, 'epoch': 0.41}

[INFO|2025-03-19 17:03:28] logging.py:157 >> {'loss': 2.0813, 'learning_rate': 4.7685e-05, 'epoch': 0.41}

[INFO|2025-03-19 17:03:28] trainer.py:4117 >> 
***** Running Evaluation *****

[INFO|2025-03-19 17:03:28] trainer.py:4119 >>   Num examples = 5152

[INFO|2025-03-19 17:03:28] trainer.py:4122 >>   Batch size = 2

[INFO|2025-03-19 17:08:26] trainer.py:3801 >> Saving model checkpoint to saves/Llama-3.2-3B-Instruct/lora/train_llama_3_2_v3/checkpoint-1200

[INFO|2025-03-19 17:08:30] configuration_utils.py:679 >> loading configuration file config.json from cache at /home/quest/.cache/huggingface/hub/models--meta-llama--Llama-3.2-3B-Instruct/snapshots/0cb88a4f764b7a12671c53f0838cd831a0843b95/config.json

[INFO|2025-03-19 17:08:30] configuration_utils.py:746 >> Model config LlamaConfig {
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": [
    128001,
    128008,
    128009
  ],
  "head_dim": 128,
  "hidden_act": "silu",
  "hidden_size": 3072,
  "initializer_range": 0.02,
  "intermediate_size": 8192,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 24,
  "num_hidden_layers": 28,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 32.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": true,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.46.1",
  "use_cache": true,
  "vocab_size": 128256
}


[INFO|2025-03-19 17:08:30] tokenization_utils_base.py:2646 >> tokenizer config file saved in saves/Llama-3.2-3B-Instruct/lora/train_llama_3_2_v3/checkpoint-1200/tokenizer_config.json

[INFO|2025-03-19 17:08:30] tokenization_utils_base.py:2655 >> Special tokens file saved in saves/Llama-3.2-3B-Instruct/lora/train_llama_3_2_v3/checkpoint-1200/special_tokens_map.json

[INFO|2025-03-19 17:08:45] logging.py:157 >> {'loss': 2.0323, 'learning_rate': 4.7666e-05, 'epoch': 0.42}

[INFO|2025-03-19 17:08:58] logging.py:157 >> {'loss': 2.0312, 'learning_rate': 4.7647e-05, 'epoch': 0.42}

[INFO|2025-03-19 17:09:13] logging.py:157 >> {'loss': 1.9332, 'learning_rate': 4.7627e-05, 'epoch': 0.42}

[INFO|2025-03-19 17:09:26] logging.py:157 >> {'loss': 2.1288, 'learning_rate': 4.7608e-05, 'epoch': 0.42}

[INFO|2025-03-19 17:09:42] logging.py:157 >> {'loss': 2.1283, 'learning_rate': 4.7589e-05, 'epoch': 0.42}

[INFO|2025-03-19 17:09:55] logging.py:157 >> {'loss': 2.1865, 'learning_rate': 4.7569e-05, 'epoch': 0.42}

[INFO|2025-03-19 17:10:10] logging.py:157 >> {'loss': 2.0240, 'learning_rate': 4.7550e-05, 'epoch': 0.43}

[INFO|2025-03-19 17:10:24] logging.py:157 >> {'loss': 2.1885, 'learning_rate': 4.7530e-05, 'epoch': 0.43}

[INFO|2025-03-19 17:10:40] logging.py:157 >> {'loss': 2.1115, 'learning_rate': 4.7511e-05, 'epoch': 0.43}

[INFO|2025-03-19 17:10:54] logging.py:157 >> {'loss': 1.8673, 'learning_rate': 4.7491e-05, 'epoch': 0.43}

[INFO|2025-03-19 17:11:08] logging.py:157 >> {'loss': 1.9817, 'learning_rate': 4.7471e-05, 'epoch': 0.43}

[INFO|2025-03-19 17:11:22] logging.py:157 >> {'loss': 2.0835, 'learning_rate': 4.7451e-05, 'epoch': 0.43}

[INFO|2025-03-19 17:11:36] logging.py:157 >> {'loss': 2.0635, 'learning_rate': 4.7432e-05, 'epoch': 0.44}

[INFO|2025-03-19 17:11:50] logging.py:157 >> {'loss': 1.8495, 'learning_rate': 4.7412e-05, 'epoch': 0.44}

[INFO|2025-03-19 17:12:05] logging.py:157 >> {'loss': 1.9912, 'learning_rate': 4.7392e-05, 'epoch': 0.44}

[INFO|2025-03-19 17:12:19] logging.py:157 >> {'loss': 2.1167, 'learning_rate': 4.7371e-05, 'epoch': 0.44}

[INFO|2025-03-19 17:12:34] logging.py:157 >> {'loss': 2.0697, 'learning_rate': 4.7351e-05, 'epoch': 0.44}

[INFO|2025-03-19 17:12:48] logging.py:157 >> {'loss': 2.1627, 'learning_rate': 4.7331e-05, 'epoch': 0.45}

[INFO|2025-03-19 17:13:02] logging.py:157 >> {'loss': 2.0382, 'learning_rate': 4.7311e-05, 'epoch': 0.45}

[INFO|2025-03-19 17:13:17] logging.py:157 >> {'loss': 2.0522, 'learning_rate': 4.7290e-05, 'epoch': 0.45}

[INFO|2025-03-19 17:13:17] trainer.py:4117 >> 
***** Running Evaluation *****

[INFO|2025-03-19 17:13:17] trainer.py:4119 >>   Num examples = 5152

[INFO|2025-03-19 17:13:17] trainer.py:4122 >>   Batch size = 2

[INFO|2025-03-19 17:18:15] trainer.py:3801 >> Saving model checkpoint to saves/Llama-3.2-3B-Instruct/lora/train_llama_3_2_v3/checkpoint-1300

[INFO|2025-03-19 17:18:16] configuration_utils.py:679 >> loading configuration file config.json from cache at /home/quest/.cache/huggingface/hub/models--meta-llama--Llama-3.2-3B-Instruct/snapshots/0cb88a4f764b7a12671c53f0838cd831a0843b95/config.json

[INFO|2025-03-19 17:18:16] configuration_utils.py:746 >> Model config LlamaConfig {
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": [
    128001,
    128008,
    128009
  ],
  "head_dim": 128,
  "hidden_act": "silu",
  "hidden_size": 3072,
  "initializer_range": 0.02,
  "intermediate_size": 8192,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 24,
  "num_hidden_layers": 28,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 32.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": true,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.46.1",
  "use_cache": true,
  "vocab_size": 128256
}


[INFO|2025-03-19 17:18:16] tokenization_utils_base.py:2646 >> tokenizer config file saved in saves/Llama-3.2-3B-Instruct/lora/train_llama_3_2_v3/checkpoint-1300/tokenizer_config.json

[INFO|2025-03-19 17:18:16] tokenization_utils_base.py:2655 >> Special tokens file saved in saves/Llama-3.2-3B-Instruct/lora/train_llama_3_2_v3/checkpoint-1300/special_tokens_map.json

[INFO|2025-03-19 17:18:30] logging.py:157 >> {'loss': 2.0693, 'learning_rate': 4.7270e-05, 'epoch': 0.45}

[INFO|2025-03-19 17:18:45] logging.py:157 >> {'loss': 1.9574, 'learning_rate': 4.7249e-05, 'epoch': 0.45}

[INFO|2025-03-19 17:19:00] logging.py:157 >> {'loss': 1.8897, 'learning_rate': 4.7228e-05, 'epoch': 0.45}

[INFO|2025-03-19 17:19:14] logging.py:157 >> {'loss': 1.9002, 'learning_rate': 4.7208e-05, 'epoch': 0.46}

[INFO|2025-03-19 17:19:28] logging.py:157 >> {'loss': 2.0336, 'learning_rate': 4.7187e-05, 'epoch': 0.46}

[INFO|2025-03-19 17:19:42] logging.py:157 >> {'loss': 2.0218, 'learning_rate': 4.7166e-05, 'epoch': 0.46}

[INFO|2025-03-19 17:19:56] logging.py:157 >> {'loss': 2.1213, 'learning_rate': 4.7145e-05, 'epoch': 0.46}

[INFO|2025-03-19 17:20:12] logging.py:157 >> {'loss': 2.1139, 'learning_rate': 4.7124e-05, 'epoch': 0.46}

[INFO|2025-03-19 17:20:26] logging.py:157 >> {'loss': 2.0214, 'learning_rate': 4.7103e-05, 'epoch': 0.46}

[INFO|2025-03-19 17:20:40] logging.py:157 >> {'loss': 2.2374, 'learning_rate': 4.7082e-05, 'epoch': 0.47}

[INFO|2025-03-19 17:20:56] logging.py:157 >> {'loss': 1.9283, 'learning_rate': 4.7061e-05, 'epoch': 0.47}

[INFO|2025-03-19 17:21:11] logging.py:157 >> {'loss': 2.0128, 'learning_rate': 4.7039e-05, 'epoch': 0.47}

[INFO|2025-03-19 17:21:25] logging.py:157 >> {'loss': 2.1867, 'learning_rate': 4.7018e-05, 'epoch': 0.47}

[INFO|2025-03-19 17:21:40] logging.py:157 >> {'loss': 2.0296, 'learning_rate': 4.6997e-05, 'epoch': 0.47}

[INFO|2025-03-19 17:21:53] logging.py:157 >> {'loss': 2.2181, 'learning_rate': 4.6975e-05, 'epoch': 0.47}

[INFO|2025-03-19 17:22:07] logging.py:157 >> {'loss': 2.2256, 'learning_rate': 4.6953e-05, 'epoch': 0.48}

[INFO|2025-03-19 17:22:21] logging.py:157 >> {'loss': 1.9509, 'learning_rate': 4.6932e-05, 'epoch': 0.48}

[INFO|2025-03-19 17:22:37] logging.py:157 >> {'loss': 1.8911, 'learning_rate': 4.6910e-05, 'epoch': 0.48}

[INFO|2025-03-19 17:22:52] logging.py:157 >> {'loss': 1.9871, 'learning_rate': 4.6888e-05, 'epoch': 0.48}

[INFO|2025-03-19 17:23:06] logging.py:157 >> {'loss': 2.0214, 'learning_rate': 4.6866e-05, 'epoch': 0.48}

[INFO|2025-03-19 17:23:06] trainer.py:4117 >> 
***** Running Evaluation *****

[INFO|2025-03-19 17:23:06] trainer.py:4119 >>   Num examples = 5152

[INFO|2025-03-19 17:23:06] trainer.py:4122 >>   Batch size = 2

[INFO|2025-03-19 17:28:05] trainer.py:3801 >> Saving model checkpoint to saves/Llama-3.2-3B-Instruct/lora/train_llama_3_2_v3/checkpoint-1400

[INFO|2025-03-19 17:28:06] configuration_utils.py:679 >> loading configuration file config.json from cache at /home/quest/.cache/huggingface/hub/models--meta-llama--Llama-3.2-3B-Instruct/snapshots/0cb88a4f764b7a12671c53f0838cd831a0843b95/config.json

[INFO|2025-03-19 17:28:06] configuration_utils.py:746 >> Model config LlamaConfig {
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": [
    128001,
    128008,
    128009
  ],
  "head_dim": 128,
  "hidden_act": "silu",
  "hidden_size": 3072,
  "initializer_range": 0.02,
  "intermediate_size": 8192,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 24,
  "num_hidden_layers": 28,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 32.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": true,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.46.1",
  "use_cache": true,
  "vocab_size": 128256
}


[INFO|2025-03-19 17:28:06] tokenization_utils_base.py:2646 >> tokenizer config file saved in saves/Llama-3.2-3B-Instruct/lora/train_llama_3_2_v3/checkpoint-1400/tokenizer_config.json

[INFO|2025-03-19 17:28:06] tokenization_utils_base.py:2655 >> Special tokens file saved in saves/Llama-3.2-3B-Instruct/lora/train_llama_3_2_v3/checkpoint-1400/special_tokens_map.json

[INFO|2025-03-19 17:28:20] logging.py:157 >> {'loss': 2.0550, 'learning_rate': 4.6845e-05, 'epoch': 0.48}

[INFO|2025-03-19 17:28:35] logging.py:157 >> {'loss': 2.1833, 'learning_rate': 4.6822e-05, 'epoch': 0.49}

[INFO|2025-03-19 17:28:50] logging.py:157 >> {'loss': 2.0331, 'learning_rate': 4.6800e-05, 'epoch': 0.49}

[INFO|2025-03-19 17:29:05] logging.py:157 >> {'loss': 2.1866, 'learning_rate': 4.6778e-05, 'epoch': 0.49}

[INFO|2025-03-19 17:29:20] logging.py:157 >> {'loss': 2.0931, 'learning_rate': 4.6756e-05, 'epoch': 0.49}

[INFO|2025-03-19 17:29:35] logging.py:157 >> {'loss': 2.0541, 'learning_rate': 4.6734e-05, 'epoch': 0.49}

[INFO|2025-03-19 17:29:48] logging.py:157 >> {'loss': 2.0277, 'learning_rate': 4.6711e-05, 'epoch': 0.50}

[INFO|2025-03-19 17:30:03] logging.py:157 >> {'loss': 1.9609, 'learning_rate': 4.6689e-05, 'epoch': 0.50}

[INFO|2025-03-19 17:30:18] logging.py:157 >> {'loss': 2.0248, 'learning_rate': 4.6666e-05, 'epoch': 0.50}

[INFO|2025-03-19 17:30:33] logging.py:157 >> {'loss': 1.9029, 'learning_rate': 4.6644e-05, 'epoch': 0.50}

[INFO|2025-03-19 17:30:48] logging.py:157 >> {'loss': 1.9818, 'learning_rate': 4.6621e-05, 'epoch': 0.50}

[INFO|2025-03-19 17:31:02] logging.py:157 >> {'loss': 1.9109, 'learning_rate': 4.6598e-05, 'epoch': 0.50}

[INFO|2025-03-19 17:31:15] logging.py:157 >> {'loss': 2.0952, 'learning_rate': 4.6576e-05, 'epoch': 0.51}

[INFO|2025-03-19 17:31:29] logging.py:157 >> {'loss': 1.8731, 'learning_rate': 4.6553e-05, 'epoch': 0.51}

[INFO|2025-03-19 17:31:43] logging.py:157 >> {'loss': 1.9922, 'learning_rate': 4.6530e-05, 'epoch': 0.51}

[INFO|2025-03-19 17:31:56] logging.py:157 >> {'loss': 2.0678, 'learning_rate': 4.6507e-05, 'epoch': 0.51}

[INFO|2025-03-19 17:32:11] logging.py:157 >> {'loss': 1.9920, 'learning_rate': 4.6484e-05, 'epoch': 0.51}

[INFO|2025-03-19 17:32:25] logging.py:157 >> {'loss': 1.9752, 'learning_rate': 4.6461e-05, 'epoch': 0.51}

[INFO|2025-03-19 17:32:40] logging.py:157 >> {'loss': 1.9302, 'learning_rate': 4.6437e-05, 'epoch': 0.52}

[INFO|2025-03-19 17:32:56] logging.py:157 >> {'loss': 2.0858, 'learning_rate': 4.6414e-05, 'epoch': 0.52}

[INFO|2025-03-19 17:32:56] trainer.py:4117 >> 
***** Running Evaluation *****

[INFO|2025-03-19 17:32:56] trainer.py:4119 >>   Num examples = 5152

[INFO|2025-03-19 17:32:56] trainer.py:4122 >>   Batch size = 2

[INFO|2025-03-19 17:37:56] trainer.py:3801 >> Saving model checkpoint to saves/Llama-3.2-3B-Instruct/lora/train_llama_3_2_v3/checkpoint-1500

[INFO|2025-03-19 17:37:58] configuration_utils.py:679 >> loading configuration file config.json from cache at /home/quest/.cache/huggingface/hub/models--meta-llama--Llama-3.2-3B-Instruct/snapshots/0cb88a4f764b7a12671c53f0838cd831a0843b95/config.json

[INFO|2025-03-19 17:37:58] configuration_utils.py:746 >> Model config LlamaConfig {
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": [
    128001,
    128008,
    128009
  ],
  "head_dim": 128,
  "hidden_act": "silu",
  "hidden_size": 3072,
  "initializer_range": 0.02,
  "intermediate_size": 8192,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 24,
  "num_hidden_layers": 28,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 32.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": true,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.46.1",
  "use_cache": true,
  "vocab_size": 128256
}


[INFO|2025-03-19 17:37:58] tokenization_utils_base.py:2646 >> tokenizer config file saved in saves/Llama-3.2-3B-Instruct/lora/train_llama_3_2_v3/checkpoint-1500/tokenizer_config.json

[INFO|2025-03-19 17:37:58] tokenization_utils_base.py:2655 >> Special tokens file saved in saves/Llama-3.2-3B-Instruct/lora/train_llama_3_2_v3/checkpoint-1500/special_tokens_map.json

[INFO|2025-03-19 17:38:13] logging.py:157 >> {'loss': 2.0094, 'learning_rate': 4.6391e-05, 'epoch': 0.52}

[INFO|2025-03-19 17:38:26] logging.py:157 >> {'loss': 2.1470, 'learning_rate': 4.6367e-05, 'epoch': 0.52}

[INFO|2025-03-19 17:38:40] logging.py:157 >> {'loss': 1.9092, 'learning_rate': 4.6344e-05, 'epoch': 0.52}

[INFO|2025-03-19 17:38:54] logging.py:157 >> {'loss': 2.0713, 'learning_rate': 4.6320e-05, 'epoch': 0.52}

[INFO|2025-03-19 17:39:08] logging.py:157 >> {'loss': 2.1138, 'learning_rate': 4.6297e-05, 'epoch': 0.53}

[INFO|2025-03-19 17:39:24] logging.py:157 >> {'loss': 2.0708, 'learning_rate': 4.6273e-05, 'epoch': 0.53}

[INFO|2025-03-19 17:39:40] logging.py:157 >> {'loss': 2.1973, 'learning_rate': 4.6249e-05, 'epoch': 0.53}

[INFO|2025-03-19 17:39:56] logging.py:157 >> {'loss': 2.0903, 'learning_rate': 4.6225e-05, 'epoch': 0.53}

[INFO|2025-03-19 17:40:11] logging.py:157 >> {'loss': 1.9515, 'learning_rate': 4.6202e-05, 'epoch': 0.53}

[INFO|2025-03-19 17:40:27] logging.py:157 >> {'loss': 1.9562, 'learning_rate': 4.6178e-05, 'epoch': 0.53}

[INFO|2025-03-19 17:40:41] logging.py:157 >> {'loss': 2.0227, 'learning_rate': 4.6153e-05, 'epoch': 0.54}

[INFO|2025-03-19 17:40:56] logging.py:157 >> {'loss': 2.0206, 'learning_rate': 4.6129e-05, 'epoch': 0.54}

[INFO|2025-03-19 17:41:12] logging.py:157 >> {'loss': 2.1611, 'learning_rate': 4.6105e-05, 'epoch': 0.54}

[INFO|2025-03-19 17:41:26] logging.py:157 >> {'loss': 2.0698, 'learning_rate': 4.6081e-05, 'epoch': 0.54}

[INFO|2025-03-19 17:41:40] logging.py:157 >> {'loss': 1.9853, 'learning_rate': 4.6057e-05, 'epoch': 0.54}

[INFO|2025-03-19 17:41:55] logging.py:157 >> {'loss': 1.9160, 'learning_rate': 4.6032e-05, 'epoch': 0.55}

[INFO|2025-03-19 17:42:10] logging.py:157 >> {'loss': 2.0816, 'learning_rate': 4.6008e-05, 'epoch': 0.55}

[INFO|2025-03-19 17:42:23] logging.py:157 >> {'loss': 1.9823, 'learning_rate': 4.5983e-05, 'epoch': 0.55}

[INFO|2025-03-19 17:42:37] logging.py:157 >> {'loss': 1.9839, 'learning_rate': 4.5959e-05, 'epoch': 0.55}

[INFO|2025-03-19 17:42:53] logging.py:157 >> {'loss': 1.9037, 'learning_rate': 4.5934e-05, 'epoch': 0.55}

[INFO|2025-03-19 17:42:53] trainer.py:4117 >> 
***** Running Evaluation *****

[INFO|2025-03-19 17:42:53] trainer.py:4119 >>   Num examples = 5152

[INFO|2025-03-19 17:42:53] trainer.py:4122 >>   Batch size = 2

[INFO|2025-03-19 17:47:53] trainer.py:3801 >> Saving model checkpoint to saves/Llama-3.2-3B-Instruct/lora/train_llama_3_2_v3/checkpoint-1600

[INFO|2025-03-19 17:48:03] tokenization_utils_base.py:2646 >> tokenizer config file saved in saves/Llama-3.2-3B-Instruct/lora/train_llama_3_2_v3/checkpoint-1600/tokenizer_config.json

[INFO|2025-03-19 17:48:03] tokenization_utils_base.py:2655 >> Special tokens file saved in saves/Llama-3.2-3B-Instruct/lora/train_llama_3_2_v3/checkpoint-1600/special_tokens_map.json

[INFO|2025-03-19 17:48:17] logging.py:157 >> {'loss': 2.0711, 'learning_rate': 4.5909e-05, 'epoch': 0.55}

[INFO|2025-03-19 17:48:31] logging.py:157 >> {'loss': 2.1041, 'learning_rate': 4.5884e-05, 'epoch': 0.56}

[INFO|2025-03-19 17:48:46] logging.py:157 >> {'loss': 2.0720, 'learning_rate': 4.5860e-05, 'epoch': 0.56}

[INFO|2025-03-19 17:48:59] logging.py:157 >> {'loss': 2.0973, 'learning_rate': 4.5835e-05, 'epoch': 0.56}

[INFO|2025-03-19 17:49:13] logging.py:157 >> {'loss': 2.1166, 'learning_rate': 4.5810e-05, 'epoch': 0.56}

[INFO|2025-03-19 17:49:29] logging.py:157 >> {'loss': 1.9936, 'learning_rate': 4.5785e-05, 'epoch': 0.56}

[INFO|2025-03-19 17:49:45] logging.py:157 >> {'loss': 1.9613, 'learning_rate': 4.5759e-05, 'epoch': 0.56}

[INFO|2025-03-19 17:49:58] logging.py:157 >> {'loss': 2.0208, 'learning_rate': 4.5734e-05, 'epoch': 0.57}

[INFO|2025-03-19 17:50:12] logging.py:157 >> {'loss': 1.9969, 'learning_rate': 4.5709e-05, 'epoch': 0.57}

[INFO|2025-03-19 17:50:27] logging.py:157 >> {'loss': 2.0009, 'learning_rate': 4.5684e-05, 'epoch': 0.57}

[INFO|2025-03-19 17:50:41] logging.py:157 >> {'loss': 1.8788, 'learning_rate': 4.5658e-05, 'epoch': 0.57}

[INFO|2025-03-19 17:50:56] logging.py:157 >> {'loss': 2.0793, 'learning_rate': 4.5633e-05, 'epoch': 0.57}

[INFO|2025-03-19 17:51:11] logging.py:157 >> {'loss': 1.9349, 'learning_rate': 4.5607e-05, 'epoch': 0.57}

[INFO|2025-03-19 17:51:25] logging.py:157 >> {'loss': 2.0543, 'learning_rate': 4.5582e-05, 'epoch': 0.58}

[INFO|2025-03-19 17:51:40] logging.py:157 >> {'loss': 1.9572, 'learning_rate': 4.5556e-05, 'epoch': 0.58}

[INFO|2025-03-19 17:51:54] logging.py:157 >> {'loss': 2.0726, 'learning_rate': 4.5530e-05, 'epoch': 0.58}

[INFO|2025-03-19 17:52:09] logging.py:157 >> {'loss': 1.9222, 'learning_rate': 4.5504e-05, 'epoch': 0.58}

[INFO|2025-03-19 17:52:23] logging.py:157 >> {'loss': 2.0171, 'learning_rate': 4.5478e-05, 'epoch': 0.58}

[INFO|2025-03-19 17:52:37] logging.py:157 >> {'loss': 2.1007, 'learning_rate': 4.5452e-05, 'epoch': 0.58}

[INFO|2025-03-19 17:52:51] logging.py:157 >> {'loss': 1.9161, 'learning_rate': 4.5426e-05, 'epoch': 0.59}

[INFO|2025-03-19 17:52:51] trainer.py:4117 >> 
***** Running Evaluation *****

[INFO|2025-03-19 17:52:51] trainer.py:4119 >>   Num examples = 5152

[INFO|2025-03-19 17:52:51] trainer.py:4122 >>   Batch size = 2

[INFO|2025-03-19 17:57:52] trainer.py:3801 >> Saving model checkpoint to saves/Llama-3.2-3B-Instruct/lora/train_llama_3_2_v3/checkpoint-1700

[INFO|2025-03-19 17:57:54] configuration_utils.py:679 >> loading configuration file config.json from cache at /home/quest/.cache/huggingface/hub/models--meta-llama--Llama-3.2-3B-Instruct/snapshots/0cb88a4f764b7a12671c53f0838cd831a0843b95/config.json

[INFO|2025-03-19 17:57:54] configuration_utils.py:746 >> Model config LlamaConfig {
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": [
    128001,
    128008,
    128009
  ],
  "head_dim": 128,
  "hidden_act": "silu",
  "hidden_size": 3072,
  "initializer_range": 0.02,
  "intermediate_size": 8192,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 24,
  "num_hidden_layers": 28,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 32.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": true,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.46.1",
  "use_cache": true,
  "vocab_size": 128256
}


[INFO|2025-03-19 17:57:54] tokenization_utils_base.py:2646 >> tokenizer config file saved in saves/Llama-3.2-3B-Instruct/lora/train_llama_3_2_v3/checkpoint-1700/tokenizer_config.json

[INFO|2025-03-19 17:57:54] tokenization_utils_base.py:2655 >> Special tokens file saved in saves/Llama-3.2-3B-Instruct/lora/train_llama_3_2_v3/checkpoint-1700/special_tokens_map.json

[INFO|2025-03-19 17:58:08] logging.py:157 >> {'loss': 1.9774, 'learning_rate': 4.5400e-05, 'epoch': 0.59}

[INFO|2025-03-19 17:58:22] logging.py:157 >> {'loss': 1.9594, 'learning_rate': 4.5374e-05, 'epoch': 0.59}

[INFO|2025-03-19 17:58:37] logging.py:157 >> {'loss': 2.0426, 'learning_rate': 4.5348e-05, 'epoch': 0.59}

[INFO|2025-03-19 17:58:52] logging.py:157 >> {'loss': 2.1053, 'learning_rate': 4.5322e-05, 'epoch': 0.59}

[INFO|2025-03-19 17:59:05] logging.py:157 >> {'loss': 1.9926, 'learning_rate': 4.5295e-05, 'epoch': 0.60}

[INFO|2025-03-19 17:59:19] logging.py:157 >> {'loss': 1.9232, 'learning_rate': 4.5269e-05, 'epoch': 0.60}

[INFO|2025-03-19 17:59:33] logging.py:157 >> {'loss': 2.0811, 'learning_rate': 4.5242e-05, 'epoch': 0.60}

[INFO|2025-03-19 17:59:48] logging.py:157 >> {'loss': 1.9687, 'learning_rate': 4.5216e-05, 'epoch': 0.60}

[INFO|2025-03-19 18:00:01] logging.py:157 >> {'loss': 2.1240, 'learning_rate': 4.5189e-05, 'epoch': 0.60}

[INFO|2025-03-19 18:00:16] logging.py:157 >> {'loss': 2.0424, 'learning_rate': 4.5163e-05, 'epoch': 0.60}

[INFO|2025-03-19 18:00:31] logging.py:157 >> {'loss': 1.9513, 'learning_rate': 4.5136e-05, 'epoch': 0.61}

[INFO|2025-03-19 18:00:46] logging.py:157 >> {'loss': 1.7127, 'learning_rate': 4.5109e-05, 'epoch': 0.61}

[INFO|2025-03-19 18:01:01] logging.py:157 >> {'loss': 2.0056, 'learning_rate': 4.5082e-05, 'epoch': 0.61}

[INFO|2025-03-19 18:01:16] logging.py:157 >> {'loss': 2.1124, 'learning_rate': 4.5055e-05, 'epoch': 0.61}

[INFO|2025-03-19 18:01:31] logging.py:157 >> {'loss': 1.9526, 'learning_rate': 4.5028e-05, 'epoch': 0.61}

[INFO|2025-03-19 18:01:46] logging.py:157 >> {'loss': 2.0324, 'learning_rate': 4.5001e-05, 'epoch': 0.61}

[INFO|2025-03-19 18:02:00] logging.py:157 >> {'loss': 1.9035, 'learning_rate': 4.4974e-05, 'epoch': 0.62}

[INFO|2025-03-19 18:02:15] logging.py:157 >> {'loss': 2.0047, 'learning_rate': 4.4947e-05, 'epoch': 0.62}

[INFO|2025-03-19 18:02:32] logging.py:157 >> {'loss': 2.1412, 'learning_rate': 4.4919e-05, 'epoch': 0.62}

[INFO|2025-03-19 18:02:46] logging.py:157 >> {'loss': 2.0706, 'learning_rate': 4.4892e-05, 'epoch': 0.62}

[INFO|2025-03-19 18:02:46] trainer.py:4117 >> 
***** Running Evaluation *****

[INFO|2025-03-19 18:02:46] trainer.py:4119 >>   Num examples = 5152

[INFO|2025-03-19 18:02:46] trainer.py:4122 >>   Batch size = 2

[INFO|2025-03-19 18:07:48] trainer.py:3801 >> Saving model checkpoint to saves/Llama-3.2-3B-Instruct/lora/train_llama_3_2_v3/checkpoint-1800

[INFO|2025-03-19 18:07:50] configuration_utils.py:679 >> loading configuration file config.json from cache at /home/quest/.cache/huggingface/hub/models--meta-llama--Llama-3.2-3B-Instruct/snapshots/0cb88a4f764b7a12671c53f0838cd831a0843b95/config.json

[INFO|2025-03-19 18:07:50] configuration_utils.py:746 >> Model config LlamaConfig {
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": [
    128001,
    128008,
    128009
  ],
  "head_dim": 128,
  "hidden_act": "silu",
  "hidden_size": 3072,
  "initializer_range": 0.02,
  "intermediate_size": 8192,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 24,
  "num_hidden_layers": 28,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 32.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": true,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.46.1",
  "use_cache": true,
  "vocab_size": 128256
}


[INFO|2025-03-19 18:07:50] tokenization_utils_base.py:2646 >> tokenizer config file saved in saves/Llama-3.2-3B-Instruct/lora/train_llama_3_2_v3/checkpoint-1800/tokenizer_config.json

[INFO|2025-03-19 18:07:50] tokenization_utils_base.py:2655 >> Special tokens file saved in saves/Llama-3.2-3B-Instruct/lora/train_llama_3_2_v3/checkpoint-1800/special_tokens_map.json

[INFO|2025-03-19 18:08:05] logging.py:157 >> {'loss': 2.0059, 'learning_rate': 4.4865e-05, 'epoch': 0.62}

[INFO|2025-03-19 18:08:22] logging.py:157 >> {'loss': 1.8939, 'learning_rate': 4.4837e-05, 'epoch': 0.62}

[INFO|2025-03-19 18:08:37] logging.py:157 >> {'loss': 2.1363, 'learning_rate': 4.4810e-05, 'epoch': 0.63}

[INFO|2025-03-19 18:08:53] logging.py:157 >> {'loss': 2.0059, 'learning_rate': 4.4782e-05, 'epoch': 0.63}

[INFO|2025-03-19 18:09:07] logging.py:157 >> {'loss': 1.8838, 'learning_rate': 4.4754e-05, 'epoch': 0.63}

[INFO|2025-03-19 18:09:22] logging.py:157 >> {'loss': 2.0848, 'learning_rate': 4.4727e-05, 'epoch': 0.63}

[INFO|2025-03-19 18:09:37] logging.py:157 >> {'loss': 1.8603, 'learning_rate': 4.4699e-05, 'epoch': 0.63}

[INFO|2025-03-19 18:09:51] logging.py:157 >> {'loss': 1.8979, 'learning_rate': 4.4671e-05, 'epoch': 0.64}

[INFO|2025-03-19 18:10:06] logging.py:157 >> {'loss': 1.9555, 'learning_rate': 4.4643e-05, 'epoch': 0.64}

[INFO|2025-03-19 18:10:21] logging.py:157 >> {'loss': 1.8661, 'learning_rate': 4.4615e-05, 'epoch': 0.64}

[INFO|2025-03-19 18:10:37] logging.py:157 >> {'loss': 1.9480, 'learning_rate': 4.4587e-05, 'epoch': 0.64}

[INFO|2025-03-19 18:10:51] logging.py:157 >> {'loss': 2.0742, 'learning_rate': 4.4559e-05, 'epoch': 0.64}

[INFO|2025-03-19 18:11:06] logging.py:157 >> {'loss': 1.9246, 'learning_rate': 4.4531e-05, 'epoch': 0.64}

[INFO|2025-03-19 18:11:20] logging.py:157 >> {'loss': 1.8086, 'learning_rate': 4.4503e-05, 'epoch': 0.65}

[INFO|2025-03-19 18:11:36] logging.py:157 >> {'loss': 1.8276, 'learning_rate': 4.4474e-05, 'epoch': 0.65}

[INFO|2025-03-19 18:11:50] logging.py:157 >> {'loss': 1.8928, 'learning_rate': 4.4446e-05, 'epoch': 0.65}

[INFO|2025-03-19 18:12:06] logging.py:157 >> {'loss': 2.0232, 'learning_rate': 4.4418e-05, 'epoch': 0.65}

[INFO|2025-03-19 18:12:21] logging.py:157 >> {'loss': 1.9489, 'learning_rate': 4.4389e-05, 'epoch': 0.65}

[INFO|2025-03-19 18:12:35] logging.py:157 >> {'loss': 2.0592, 'learning_rate': 4.4361e-05, 'epoch': 0.65}

[INFO|2025-03-19 18:12:49] logging.py:157 >> {'loss': 2.0887, 'learning_rate': 4.4332e-05, 'epoch': 0.66}

[INFO|2025-03-19 18:12:49] trainer.py:4117 >> 
***** Running Evaluation *****

[INFO|2025-03-19 18:12:49] trainer.py:4119 >>   Num examples = 5152

[INFO|2025-03-19 18:12:49] trainer.py:4122 >>   Batch size = 2

[INFO|2025-03-19 18:17:53] trainer.py:3801 >> Saving model checkpoint to saves/Llama-3.2-3B-Instruct/lora/train_llama_3_2_v3/checkpoint-1900

[INFO|2025-03-19 18:17:56] configuration_utils.py:679 >> loading configuration file config.json from cache at /home/quest/.cache/huggingface/hub/models--meta-llama--Llama-3.2-3B-Instruct/snapshots/0cb88a4f764b7a12671c53f0838cd831a0843b95/config.json

[INFO|2025-03-19 18:17:56] configuration_utils.py:746 >> Model config LlamaConfig {
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": [
    128001,
    128008,
    128009
  ],
  "head_dim": 128,
  "hidden_act": "silu",
  "hidden_size": 3072,
  "initializer_range": 0.02,
  "intermediate_size": 8192,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 24,
  "num_hidden_layers": 28,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 32.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": true,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.46.1",
  "use_cache": true,
  "vocab_size": 128256
}


[INFO|2025-03-19 18:17:56] tokenization_utils_base.py:2646 >> tokenizer config file saved in saves/Llama-3.2-3B-Instruct/lora/train_llama_3_2_v3/checkpoint-1900/tokenizer_config.json

[INFO|2025-03-19 18:17:56] tokenization_utils_base.py:2655 >> Special tokens file saved in saves/Llama-3.2-3B-Instruct/lora/train_llama_3_2_v3/checkpoint-1900/special_tokens_map.json

[INFO|2025-03-19 18:18:11] logging.py:157 >> {'loss': 1.9851, 'learning_rate': 4.4303e-05, 'epoch': 0.66}

[INFO|2025-03-19 18:18:25] logging.py:157 >> {'loss': 2.0314, 'learning_rate': 4.4274e-05, 'epoch': 0.66}

[INFO|2025-03-19 18:18:39] logging.py:157 >> {'loss': 1.9171, 'learning_rate': 4.4246e-05, 'epoch': 0.66}

[INFO|2025-03-19 18:18:53] logging.py:157 >> {'loss': 1.9374, 'learning_rate': 4.4217e-05, 'epoch': 0.66}

[INFO|2025-03-19 18:19:07] logging.py:157 >> {'loss': 1.8045, 'learning_rate': 4.4188e-05, 'epoch': 0.66}

[INFO|2025-03-19 18:19:22] logging.py:157 >> {'loss': 1.8496, 'learning_rate': 4.4159e-05, 'epoch': 0.67}

[INFO|2025-03-19 18:19:36] logging.py:157 >> {'loss': 2.0473, 'learning_rate': 4.4130e-05, 'epoch': 0.67}

[INFO|2025-03-19 18:19:51] logging.py:157 >> {'loss': 1.9551, 'learning_rate': 4.4101e-05, 'epoch': 0.67}

[INFO|2025-03-19 18:20:06] logging.py:157 >> {'loss': 1.7924, 'learning_rate': 4.4071e-05, 'epoch': 0.67}

[INFO|2025-03-19 18:20:20] logging.py:157 >> {'loss': 1.8151, 'learning_rate': 4.4042e-05, 'epoch': 0.67}

[INFO|2025-03-19 18:20:36] logging.py:157 >> {'loss': 2.0678, 'learning_rate': 4.4013e-05, 'epoch': 0.67}

[INFO|2025-03-19 18:20:49] logging.py:157 >> {'loss': 1.9672, 'learning_rate': 4.3984e-05, 'epoch': 0.68}

[INFO|2025-03-19 18:21:03] logging.py:157 >> {'loss': 2.0697, 'learning_rate': 4.3954e-05, 'epoch': 0.68}

[INFO|2025-03-19 18:21:17] logging.py:157 >> {'loss': 1.9999, 'learning_rate': 4.3925e-05, 'epoch': 0.68}

[INFO|2025-03-19 18:21:32] logging.py:157 >> {'loss': 1.8463, 'learning_rate': 4.3895e-05, 'epoch': 0.68}

[INFO|2025-03-19 18:21:47] logging.py:157 >> {'loss': 1.9961, 'learning_rate': 4.3865e-05, 'epoch': 0.68}

[INFO|2025-03-19 18:22:02] logging.py:157 >> {'loss': 1.7512, 'learning_rate': 4.3836e-05, 'epoch': 0.69}

[INFO|2025-03-19 18:22:15] logging.py:157 >> {'loss': 1.9542, 'learning_rate': 4.3806e-05, 'epoch': 0.69}

[INFO|2025-03-19 18:22:31] logging.py:157 >> {'loss': 2.0422, 'learning_rate': 4.3776e-05, 'epoch': 0.69}

[INFO|2025-03-19 18:22:44] logging.py:157 >> {'loss': 1.6599, 'learning_rate': 4.3746e-05, 'epoch': 0.69}

[INFO|2025-03-19 18:22:44] trainer.py:4117 >> 
***** Running Evaluation *****

[INFO|2025-03-19 18:22:44] trainer.py:4119 >>   Num examples = 5152

[INFO|2025-03-19 18:22:44] trainer.py:4122 >>   Batch size = 2

[INFO|2025-03-19 18:28:50] trainer.py:3801 >> Saving model checkpoint to saves/Llama-3.2-3B-Instruct/lora/train_llama_3_2_v3/checkpoint-2000

[INFO|2025-03-19 18:28:54] configuration_utils.py:679 >> loading configuration file config.json from cache at /home/quest/.cache/huggingface/hub/models--meta-llama--Llama-3.2-3B-Instruct/snapshots/0cb88a4f764b7a12671c53f0838cd831a0843b95/config.json

[INFO|2025-03-19 18:28:54] configuration_utils.py:746 >> Model config LlamaConfig {
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": [
    128001,
    128008,
    128009
  ],
  "head_dim": 128,
  "hidden_act": "silu",
  "hidden_size": 3072,
  "initializer_range": 0.02,
  "intermediate_size": 8192,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 24,
  "num_hidden_layers": 28,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 32.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": true,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.46.1",
  "use_cache": true,
  "vocab_size": 128256
}


[INFO|2025-03-19 18:28:54] tokenization_utils_base.py:2646 >> tokenizer config file saved in saves/Llama-3.2-3B-Instruct/lora/train_llama_3_2_v3/checkpoint-2000/tokenizer_config.json

[INFO|2025-03-19 18:28:54] tokenization_utils_base.py:2655 >> Special tokens file saved in saves/Llama-3.2-3B-Instruct/lora/train_llama_3_2_v3/checkpoint-2000/special_tokens_map.json

[INFO|2025-03-19 18:29:10] logging.py:157 >> {'loss': 1.9360, 'learning_rate': 4.3716e-05, 'epoch': 0.69}

[INFO|2025-03-19 18:29:26] logging.py:157 >> {'loss': 2.0227, 'learning_rate': 4.3686e-05, 'epoch': 0.69}

[INFO|2025-03-19 18:29:40] logging.py:157 >> {'loss': 2.2042, 'learning_rate': 4.3656e-05, 'epoch': 0.70}

[INFO|2025-03-19 18:29:55] logging.py:157 >> {'loss': 2.0298, 'learning_rate': 4.3626e-05, 'epoch': 0.70}

[INFO|2025-03-19 18:30:10] logging.py:157 >> {'loss': 2.0525, 'learning_rate': 4.3596e-05, 'epoch': 0.70}

[INFO|2025-03-19 18:30:24] logging.py:157 >> {'loss': 2.0098, 'learning_rate': 4.3566e-05, 'epoch': 0.70}

[INFO|2025-03-19 18:30:38] logging.py:157 >> {'loss': 1.9084, 'learning_rate': 4.3536e-05, 'epoch': 0.70}

[INFO|2025-03-19 18:30:55] logging.py:157 >> {'loss': 1.9375, 'learning_rate': 4.3505e-05, 'epoch': 0.70}

[INFO|2025-03-19 18:31:10] logging.py:157 >> {'loss': 1.9967, 'learning_rate': 4.3475e-05, 'epoch': 0.71}

[INFO|2025-03-19 18:31:26] logging.py:157 >> {'loss': 1.9215, 'learning_rate': 4.3444e-05, 'epoch': 0.71}

[INFO|2025-03-19 18:31:41] logging.py:157 >> {'loss': 2.1129, 'learning_rate': 4.3414e-05, 'epoch': 0.71}

[INFO|2025-03-19 18:32:07] logging.py:157 >> {'loss': 1.9731, 'learning_rate': 4.3383e-05, 'epoch': 0.71}

[INFO|2025-03-19 18:32:32] logging.py:157 >> {'loss': 2.0439, 'learning_rate': 4.3353e-05, 'epoch': 0.71}

[INFO|2025-03-19 18:32:48] logging.py:157 >> {'loss': 1.7532, 'learning_rate': 4.3322e-05, 'epoch': 0.71}

[INFO|2025-03-19 18:33:02] logging.py:157 >> {'loss': 1.9934, 'learning_rate': 4.3291e-05, 'epoch': 0.72}

[INFO|2025-03-19 18:33:18] logging.py:157 >> {'loss': 1.7800, 'learning_rate': 4.3260e-05, 'epoch': 0.72}

[INFO|2025-03-19 18:33:32] logging.py:157 >> {'loss': 1.8294, 'learning_rate': 4.3229e-05, 'epoch': 0.72}

[INFO|2025-03-19 18:33:46] logging.py:157 >> {'loss': 1.8816, 'learning_rate': 4.3198e-05, 'epoch': 0.72}

[INFO|2025-03-19 18:34:00] logging.py:157 >> {'loss': 1.7207, 'learning_rate': 4.3167e-05, 'epoch': 0.72}

[INFO|2025-03-19 18:34:14] logging.py:157 >> {'loss': 1.8803, 'learning_rate': 4.3136e-05, 'epoch': 0.72}

[INFO|2025-03-19 18:34:14] trainer.py:4117 >> 
***** Running Evaluation *****

[INFO|2025-03-19 18:34:14] trainer.py:4119 >>   Num examples = 5152

[INFO|2025-03-19 18:34:14] trainer.py:4122 >>   Batch size = 2

[INFO|2025-03-19 18:39:16] trainer.py:3801 >> Saving model checkpoint to saves/Llama-3.2-3B-Instruct/lora/train_llama_3_2_v3/checkpoint-2100

[INFO|2025-03-19 18:39:20] configuration_utils.py:679 >> loading configuration file config.json from cache at /home/quest/.cache/huggingface/hub/models--meta-llama--Llama-3.2-3B-Instruct/snapshots/0cb88a4f764b7a12671c53f0838cd831a0843b95/config.json

[INFO|2025-03-19 18:39:20] configuration_utils.py:746 >> Model config LlamaConfig {
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": [
    128001,
    128008,
    128009
  ],
  "head_dim": 128,
  "hidden_act": "silu",
  "hidden_size": 3072,
  "initializer_range": 0.02,
  "intermediate_size": 8192,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 24,
  "num_hidden_layers": 28,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 32.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": true,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.46.1",
  "use_cache": true,
  "vocab_size": 128256
}


[INFO|2025-03-19 18:39:20] tokenization_utils_base.py:2646 >> tokenizer config file saved in saves/Llama-3.2-3B-Instruct/lora/train_llama_3_2_v3/checkpoint-2100/tokenizer_config.json

[INFO|2025-03-19 18:39:20] tokenization_utils_base.py:2655 >> Special tokens file saved in saves/Llama-3.2-3B-Instruct/lora/train_llama_3_2_v3/checkpoint-2100/special_tokens_map.json

[INFO|2025-03-19 18:39:33] logging.py:157 >> {'loss': 1.9910, 'learning_rate': 4.3105e-05, 'epoch': 0.73}

[INFO|2025-03-19 18:39:49] logging.py:157 >> {'loss': 2.0103, 'learning_rate': 4.3074e-05, 'epoch': 0.73}

[INFO|2025-03-19 18:40:05] logging.py:157 >> {'loss': 1.8334, 'learning_rate': 4.3043e-05, 'epoch': 0.73}

[INFO|2025-03-19 18:40:20] logging.py:157 >> {'loss': 1.8590, 'learning_rate': 4.3012e-05, 'epoch': 0.73}

[INFO|2025-03-19 18:40:34] logging.py:157 >> {'loss': 2.0876, 'learning_rate': 4.2980e-05, 'epoch': 0.73}

[INFO|2025-03-19 18:40:49] logging.py:157 >> {'loss': 1.9810, 'learning_rate': 4.2949e-05, 'epoch': 0.74}

[INFO|2025-03-19 18:41:03] logging.py:157 >> {'loss': 1.8639, 'learning_rate': 4.2917e-05, 'epoch': 0.74}

[INFO|2025-03-19 18:41:19] logging.py:157 >> {'loss': 1.9021, 'learning_rate': 4.2886e-05, 'epoch': 0.74}

[INFO|2025-03-19 18:41:33] logging.py:157 >> {'loss': 1.8677, 'learning_rate': 4.2854e-05, 'epoch': 0.74}

[INFO|2025-03-19 18:41:49] logging.py:157 >> {'loss': 1.8776, 'learning_rate': 4.2822e-05, 'epoch': 0.74}

[INFO|2025-03-19 18:42:03] logging.py:157 >> {'loss': 1.7641, 'learning_rate': 4.2791e-05, 'epoch': 0.74}

[INFO|2025-03-19 18:42:17] logging.py:157 >> {'loss': 1.8181, 'learning_rate': 4.2759e-05, 'epoch': 0.75}

[INFO|2025-03-19 18:42:31] logging.py:157 >> {'loss': 1.9576, 'learning_rate': 4.2727e-05, 'epoch': 0.75}

[INFO|2025-03-19 18:42:46] logging.py:157 >> {'loss': 1.9412, 'learning_rate': 4.2695e-05, 'epoch': 0.75}

[INFO|2025-03-19 18:43:02] logging.py:157 >> {'loss': 1.8499, 'learning_rate': 4.2663e-05, 'epoch': 0.75}

[INFO|2025-03-19 18:43:18] logging.py:157 >> {'loss': 1.8637, 'learning_rate': 4.2631e-05, 'epoch': 0.75}

[INFO|2025-03-19 18:43:32] logging.py:157 >> {'loss': 2.1193, 'learning_rate': 4.2599e-05, 'epoch': 0.75}

[INFO|2025-03-19 18:43:48] logging.py:157 >> {'loss': 1.9514, 'learning_rate': 4.2567e-05, 'epoch': 0.76}

[INFO|2025-03-19 18:44:02] logging.py:157 >> {'loss': 1.9244, 'learning_rate': 4.2535e-05, 'epoch': 0.76}

[INFO|2025-03-19 18:44:17] logging.py:157 >> {'loss': 1.9146, 'learning_rate': 4.2503e-05, 'epoch': 0.76}

[INFO|2025-03-19 18:44:17] trainer.py:4117 >> 
***** Running Evaluation *****

[INFO|2025-03-19 18:44:17] trainer.py:4119 >>   Num examples = 5152

[INFO|2025-03-19 18:44:17] trainer.py:4122 >>   Batch size = 2

[INFO|2025-03-19 18:49:19] trainer.py:3801 >> Saving model checkpoint to saves/Llama-3.2-3B-Instruct/lora/train_llama_3_2_v3/checkpoint-2200

[INFO|2025-03-19 18:49:29] tokenization_utils_base.py:2646 >> tokenizer config file saved in saves/Llama-3.2-3B-Instruct/lora/train_llama_3_2_v3/checkpoint-2200/tokenizer_config.json

[INFO|2025-03-19 18:49:29] tokenization_utils_base.py:2655 >> Special tokens file saved in saves/Llama-3.2-3B-Instruct/lora/train_llama_3_2_v3/checkpoint-2200/special_tokens_map.json

[INFO|2025-03-19 18:49:44] logging.py:157 >> {'loss': 1.8840, 'learning_rate': 4.2470e-05, 'epoch': 0.76}

[INFO|2025-03-19 18:49:59] logging.py:157 >> {'loss': 2.0083, 'learning_rate': 4.2438e-05, 'epoch': 0.76}

[INFO|2025-03-19 18:50:14] logging.py:157 >> {'loss': 1.8736, 'learning_rate': 4.2406e-05, 'epoch': 0.76}

[INFO|2025-03-19 18:50:29] logging.py:157 >> {'loss': 2.0333, 'learning_rate': 4.2373e-05, 'epoch': 0.77}

[INFO|2025-03-19 18:50:44] logging.py:157 >> {'loss': 2.0334, 'learning_rate': 4.2341e-05, 'epoch': 0.77}

[INFO|2025-03-19 18:50:59] logging.py:157 >> {'loss': 2.0562, 'learning_rate': 4.2308e-05, 'epoch': 0.77}

[INFO|2025-03-19 18:51:13] logging.py:157 >> {'loss': 1.9342, 'learning_rate': 4.2275e-05, 'epoch': 0.77}

[INFO|2025-03-19 18:51:27] logging.py:157 >> {'loss': 2.0290, 'learning_rate': 4.2243e-05, 'epoch': 0.77}

[INFO|2025-03-19 18:51:42] logging.py:157 >> {'loss': 2.0019, 'learning_rate': 4.2210e-05, 'epoch': 0.77}

[INFO|2025-03-19 18:51:56] logging.py:157 >> {'loss': 1.8626, 'learning_rate': 4.2177e-05, 'epoch': 0.78}

[INFO|2025-03-19 18:52:11] logging.py:157 >> {'loss': 1.8814, 'learning_rate': 4.2144e-05, 'epoch': 0.78}

[INFO|2025-03-19 18:52:26] logging.py:157 >> {'loss': 1.9089, 'learning_rate': 4.2111e-05, 'epoch': 0.78}

[INFO|2025-03-19 18:52:40] logging.py:157 >> {'loss': 1.9159, 'learning_rate': 4.2078e-05, 'epoch': 0.78}

[INFO|2025-03-19 18:52:54] logging.py:157 >> {'loss': 1.9492, 'learning_rate': 4.2045e-05, 'epoch': 0.78}

[INFO|2025-03-19 18:53:09] logging.py:157 >> {'loss': 1.8727, 'learning_rate': 4.2012e-05, 'epoch': 0.79}

[INFO|2025-03-19 18:53:24] logging.py:157 >> {'loss': 2.0871, 'learning_rate': 4.1979e-05, 'epoch': 0.79}

[INFO|2025-03-19 18:53:40] logging.py:157 >> {'loss': 2.0113, 'learning_rate': 4.1946e-05, 'epoch': 0.79}

[INFO|2025-03-19 18:53:55] logging.py:157 >> {'loss': 1.7017, 'learning_rate': 4.1913e-05, 'epoch': 0.79}

[INFO|2025-03-19 18:54:10] logging.py:157 >> {'loss': 1.9600, 'learning_rate': 4.1879e-05, 'epoch': 0.79}

[INFO|2025-03-19 18:54:25] logging.py:157 >> {'loss': 1.9548, 'learning_rate': 4.1846e-05, 'epoch': 0.79}

[INFO|2025-03-19 18:54:25] trainer.py:4117 >> 
***** Running Evaluation *****

[INFO|2025-03-19 18:54:25] trainer.py:4119 >>   Num examples = 5152

[INFO|2025-03-19 18:54:25] trainer.py:4122 >>   Batch size = 2

[INFO|2025-03-19 18:59:26] trainer.py:3801 >> Saving model checkpoint to saves/Llama-3.2-3B-Instruct/lora/train_llama_3_2_v3/checkpoint-2300

[INFO|2025-03-19 18:59:26] tokenization_utils_base.py:2646 >> tokenizer config file saved in saves/Llama-3.2-3B-Instruct/lora/train_llama_3_2_v3/checkpoint-2300/tokenizer_config.json

[INFO|2025-03-19 18:59:26] tokenization_utils_base.py:2655 >> Special tokens file saved in saves/Llama-3.2-3B-Instruct/lora/train_llama_3_2_v3/checkpoint-2300/special_tokens_map.json

[INFO|2025-03-19 18:59:42] logging.py:157 >> {'loss': 1.9110, 'learning_rate': 4.1813e-05, 'epoch': 0.80}

[INFO|2025-03-19 18:59:57] logging.py:157 >> {'loss': 2.1067, 'learning_rate': 4.1779e-05, 'epoch': 0.80}

[INFO|2025-03-19 19:00:12] logging.py:157 >> {'loss': 1.8514, 'learning_rate': 4.1746e-05, 'epoch': 0.80}

[INFO|2025-03-19 19:00:26] logging.py:157 >> {'loss': 1.9824, 'learning_rate': 4.1712e-05, 'epoch': 0.80}

[INFO|2025-03-19 19:00:41] logging.py:157 >> {'loss': 1.9510, 'learning_rate': 4.1679e-05, 'epoch': 0.80}

[INFO|2025-03-19 19:00:55] logging.py:157 >> {'loss': 1.9881, 'learning_rate': 4.1645e-05, 'epoch': 0.80}

[INFO|2025-03-19 19:01:10] logging.py:157 >> {'loss': 1.8149, 'learning_rate': 4.1611e-05, 'epoch': 0.81}

[INFO|2025-03-19 19:01:25] logging.py:157 >> {'loss': 1.9500, 'learning_rate': 4.1577e-05, 'epoch': 0.81}

[INFO|2025-03-19 19:01:41] logging.py:157 >> {'loss': 1.9801, 'learning_rate': 4.1543e-05, 'epoch': 0.81}

[INFO|2025-03-19 19:01:56] logging.py:157 >> {'loss': 2.0279, 'learning_rate': 4.1510e-05, 'epoch': 0.81}

[INFO|2025-03-19 19:02:10] logging.py:157 >> {'loss': 1.9564, 'learning_rate': 4.1476e-05, 'epoch': 0.81}

[INFO|2025-03-19 19:02:24] logging.py:157 >> {'loss': 1.9600, 'learning_rate': 4.1442e-05, 'epoch': 0.81}

[INFO|2025-03-19 19:02:39] logging.py:157 >> {'loss': 1.7972, 'learning_rate': 4.1407e-05, 'epoch': 0.82}

[INFO|2025-03-19 19:02:53] logging.py:157 >> {'loss': 2.1149, 'learning_rate': 4.1373e-05, 'epoch': 0.82}

[INFO|2025-03-19 19:03:08] logging.py:157 >> {'loss': 1.8961, 'learning_rate': 4.1339e-05, 'epoch': 0.82}

[INFO|2025-03-19 19:03:22] logging.py:157 >> {'loss': 1.7649, 'learning_rate': 4.1305e-05, 'epoch': 0.82}

[INFO|2025-03-19 19:03:37] logging.py:157 >> {'loss': 1.8757, 'learning_rate': 4.1271e-05, 'epoch': 0.82}

[INFO|2025-03-19 19:03:52] logging.py:157 >> {'loss': 1.8669, 'learning_rate': 4.1236e-05, 'epoch': 0.82}

[INFO|2025-03-19 19:04:07] logging.py:157 >> {'loss': 1.9864, 'learning_rate': 4.1202e-05, 'epoch': 0.83}

[INFO|2025-03-19 19:04:21] logging.py:157 >> {'loss': 1.8417, 'learning_rate': 4.1168e-05, 'epoch': 0.83}

[INFO|2025-03-19 19:04:21] trainer.py:4117 >> 
***** Running Evaluation *****

[INFO|2025-03-19 19:04:21] trainer.py:4119 >>   Num examples = 5152

[INFO|2025-03-19 19:04:21] trainer.py:4122 >>   Batch size = 2

[INFO|2025-03-19 19:09:23] trainer.py:3801 >> Saving model checkpoint to saves/Llama-3.2-3B-Instruct/lora/train_llama_3_2_v3/checkpoint-2400

[INFO|2025-03-19 19:09:23] tokenization_utils_base.py:2646 >> tokenizer config file saved in saves/Llama-3.2-3B-Instruct/lora/train_llama_3_2_v3/checkpoint-2400/tokenizer_config.json

[INFO|2025-03-19 19:09:23] tokenization_utils_base.py:2655 >> Special tokens file saved in saves/Llama-3.2-3B-Instruct/lora/train_llama_3_2_v3/checkpoint-2400/special_tokens_map.json

[INFO|2025-03-19 19:09:38] logging.py:157 >> {'loss': 1.9729, 'learning_rate': 4.1133e-05, 'epoch': 0.83}

[INFO|2025-03-19 19:09:53] logging.py:157 >> {'loss': 1.9123, 'learning_rate': 4.1099e-05, 'epoch': 0.83}

[INFO|2025-03-19 19:10:07] logging.py:157 >> {'loss': 1.9329, 'learning_rate': 4.1064e-05, 'epoch': 0.83}

[INFO|2025-03-19 19:10:22] logging.py:157 >> {'loss': 1.9862, 'learning_rate': 4.1029e-05, 'epoch': 0.84}

[INFO|2025-03-19 19:10:36] logging.py:157 >> {'loss': 1.9119, 'learning_rate': 4.0995e-05, 'epoch': 0.84}

[INFO|2025-03-19 19:10:52] logging.py:157 >> {'loss': 2.1046, 'learning_rate': 4.0960e-05, 'epoch': 0.84}

[INFO|2025-03-19 19:11:07] logging.py:157 >> {'loss': 1.9122, 'learning_rate': 4.0925e-05, 'epoch': 0.84}

[INFO|2025-03-19 19:11:23] logging.py:157 >> {'loss': 1.9083, 'learning_rate': 4.0890e-05, 'epoch': 0.84}

[INFO|2025-03-19 19:11:37] logging.py:157 >> {'loss': 2.1172, 'learning_rate': 4.0855e-05, 'epoch': 0.84}

[INFO|2025-03-19 19:11:52] logging.py:157 >> {'loss': 1.8157, 'learning_rate': 4.0820e-05, 'epoch': 0.85}

[INFO|2025-03-19 19:12:05] logging.py:157 >> {'loss': 1.8188, 'learning_rate': 4.0785e-05, 'epoch': 0.85}

[INFO|2025-03-19 19:12:22] logging.py:157 >> {'loss': 1.8990, 'learning_rate': 4.0750e-05, 'epoch': 0.85}

[INFO|2025-03-19 19:12:38] logging.py:157 >> {'loss': 1.8865, 'learning_rate': 4.0715e-05, 'epoch': 0.85}

[INFO|2025-03-19 19:12:51] logging.py:157 >> {'loss': 2.0150, 'learning_rate': 4.0680e-05, 'epoch': 0.85}

[INFO|2025-03-19 19:13:06] logging.py:157 >> {'loss': 1.8813, 'learning_rate': 4.0645e-05, 'epoch': 0.85}

[INFO|2025-03-19 19:13:21] logging.py:157 >> {'loss': 1.9358, 'learning_rate': 4.0609e-05, 'epoch': 0.86}

[INFO|2025-03-19 19:13:37] logging.py:157 >> {'loss': 1.8427, 'learning_rate': 4.0574e-05, 'epoch': 0.86}

[INFO|2025-03-19 19:13:51] logging.py:157 >> {'loss': 1.9045, 'learning_rate': 4.0539e-05, 'epoch': 0.86}

[INFO|2025-03-19 19:14:06] logging.py:157 >> {'loss': 1.9459, 'learning_rate': 4.0503e-05, 'epoch': 0.86}

[INFO|2025-03-19 19:14:20] logging.py:157 >> {'loss': 1.8702, 'learning_rate': 4.0468e-05, 'epoch': 0.86}

[INFO|2025-03-19 19:14:20] trainer.py:4117 >> 
***** Running Evaluation *****

[INFO|2025-03-19 19:14:20] trainer.py:4119 >>   Num examples = 5152

[INFO|2025-03-19 19:14:20] trainer.py:4122 >>   Batch size = 2

[INFO|2025-03-19 19:19:22] trainer.py:3801 >> Saving model checkpoint to saves/Llama-3.2-3B-Instruct/lora/train_llama_3_2_v3/checkpoint-2500

[INFO|2025-03-19 19:19:22] tokenization_utils_base.py:2646 >> tokenizer config file saved in saves/Llama-3.2-3B-Instruct/lora/train_llama_3_2_v3/checkpoint-2500/tokenizer_config.json

[INFO|2025-03-19 19:19:22] tokenization_utils_base.py:2655 >> Special tokens file saved in saves/Llama-3.2-3B-Instruct/lora/train_llama_3_2_v3/checkpoint-2500/special_tokens_map.json

[INFO|2025-03-19 19:19:38] logging.py:157 >> {'loss': 1.8343, 'learning_rate': 4.0432e-05, 'epoch': 0.86}

[INFO|2025-03-19 19:19:52] logging.py:157 >> {'loss': 1.9740, 'learning_rate': 4.0397e-05, 'epoch': 0.87}

[INFO|2025-03-19 19:20:08] logging.py:157 >> {'loss': 1.8235, 'learning_rate': 4.0361e-05, 'epoch': 0.87}

[INFO|2025-03-19 19:20:23] logging.py:157 >> {'loss': 1.9141, 'learning_rate': 4.0325e-05, 'epoch': 0.87}

[INFO|2025-03-19 19:20:36] logging.py:157 >> {'loss': 1.7344, 'learning_rate': 4.0290e-05, 'epoch': 0.87}

[INFO|2025-03-19 19:20:50] logging.py:157 >> {'loss': 1.9614, 'learning_rate': 4.0254e-05, 'epoch': 0.87}

[INFO|2025-03-19 19:21:04] logging.py:157 >> {'loss': 1.9612, 'learning_rate': 4.0218e-05, 'epoch': 0.87}

[INFO|2025-03-19 19:21:19] logging.py:157 >> {'loss': 1.7857, 'learning_rate': 4.0182e-05, 'epoch': 0.88}

[INFO|2025-03-19 19:21:33] logging.py:157 >> {'loss': 1.8979, 'learning_rate': 4.0146e-05, 'epoch': 0.88}

[INFO|2025-03-19 19:21:47] logging.py:157 >> {'loss': 1.8758, 'learning_rate': 4.0110e-05, 'epoch': 0.88}

[INFO|2025-03-19 19:22:01] logging.py:157 >> {'loss': 1.9139, 'learning_rate': 4.0074e-05, 'epoch': 0.88}

[INFO|2025-03-19 19:22:16] logging.py:157 >> {'loss': 1.9357, 'learning_rate': 4.0038e-05, 'epoch': 0.88}

[INFO|2025-03-19 19:22:30] logging.py:157 >> {'loss': 2.0071, 'learning_rate': 4.0002e-05, 'epoch': 0.89}

[INFO|2025-03-19 19:22:44] logging.py:157 >> {'loss': 2.0054, 'learning_rate': 3.9966e-05, 'epoch': 0.89}

[INFO|2025-03-19 19:22:58] logging.py:157 >> {'loss': 1.6684, 'learning_rate': 3.9930e-05, 'epoch': 0.89}

[INFO|2025-03-19 19:23:14] logging.py:157 >> {'loss': 1.8011, 'learning_rate': 3.9894e-05, 'epoch': 0.89}

[INFO|2025-03-19 19:23:30] logging.py:157 >> {'loss': 1.8561, 'learning_rate': 3.9857e-05, 'epoch': 0.89}

[INFO|2025-03-19 19:23:44] logging.py:157 >> {'loss': 1.8710, 'learning_rate': 3.9821e-05, 'epoch': 0.89}

[INFO|2025-03-19 19:23:58] logging.py:157 >> {'loss': 1.9874, 'learning_rate': 3.9784e-05, 'epoch': 0.90}

[INFO|2025-03-19 19:24:12] logging.py:157 >> {'loss': 1.9438, 'learning_rate': 3.9748e-05, 'epoch': 0.90}

[INFO|2025-03-19 19:24:12] trainer.py:4117 >> 
***** Running Evaluation *****

[INFO|2025-03-19 19:24:12] trainer.py:4119 >>   Num examples = 5152

[INFO|2025-03-19 19:24:12] trainer.py:4122 >>   Batch size = 2

[INFO|2025-03-19 19:29:15] trainer.py:3801 >> Saving model checkpoint to saves/Llama-3.2-3B-Instruct/lora/train_llama_3_2_v3/checkpoint-2600

[INFO|2025-03-19 19:29:15] tokenization_utils_base.py:2646 >> tokenizer config file saved in saves/Llama-3.2-3B-Instruct/lora/train_llama_3_2_v3/checkpoint-2600/tokenizer_config.json

[INFO|2025-03-19 19:29:15] tokenization_utils_base.py:2655 >> Special tokens file saved in saves/Llama-3.2-3B-Instruct/lora/train_llama_3_2_v3/checkpoint-2600/special_tokens_map.json

[INFO|2025-03-19 19:29:30] logging.py:157 >> {'loss': 1.9613, 'learning_rate': 3.9711e-05, 'epoch': 0.90}

[INFO|2025-03-19 19:29:45] logging.py:157 >> {'loss': 1.8320, 'learning_rate': 3.9675e-05, 'epoch': 0.90}

[INFO|2025-03-19 19:30:00] logging.py:157 >> {'loss': 1.9109, 'learning_rate': 3.9638e-05, 'epoch': 0.90}

[INFO|2025-03-19 19:30:13] logging.py:157 >> {'loss': 1.7883, 'learning_rate': 3.9602e-05, 'epoch': 0.90}

[INFO|2025-03-19 19:30:28] logging.py:157 >> {'loss': 1.8389, 'learning_rate': 3.9565e-05, 'epoch': 0.91}

[INFO|2025-03-19 19:30:42] logging.py:157 >> {'loss': 1.8656, 'learning_rate': 3.9528e-05, 'epoch': 0.91}

[INFO|2025-03-19 19:30:57] logging.py:157 >> {'loss': 1.8329, 'learning_rate': 3.9491e-05, 'epoch': 0.91}

[INFO|2025-03-19 19:31:12] logging.py:157 >> {'loss': 1.8743, 'learning_rate': 3.9455e-05, 'epoch': 0.91}

[INFO|2025-03-19 19:31:27] logging.py:157 >> {'loss': 1.8988, 'learning_rate': 3.9418e-05, 'epoch': 0.91}

[INFO|2025-03-19 19:31:42] logging.py:157 >> {'loss': 1.9835, 'learning_rate': 3.9381e-05, 'epoch': 0.91}

[INFO|2025-03-19 19:31:56] logging.py:157 >> {'loss': 1.8392, 'learning_rate': 3.9344e-05, 'epoch': 0.92}

[INFO|2025-03-19 19:32:09] logging.py:157 >> {'loss': 1.7490, 'learning_rate': 3.9307e-05, 'epoch': 0.92}

[INFO|2025-03-19 19:32:24] logging.py:157 >> {'loss': 1.8788, 'learning_rate': 3.9270e-05, 'epoch': 0.92}

[INFO|2025-03-19 19:32:39] logging.py:157 >> {'loss': 1.8989, 'learning_rate': 3.9232e-05, 'epoch': 0.92}

[INFO|2025-03-19 19:32:54] logging.py:157 >> {'loss': 1.9059, 'learning_rate': 3.9195e-05, 'epoch': 0.92}

[INFO|2025-03-19 19:33:09] logging.py:157 >> {'loss': 1.9785, 'learning_rate': 3.9158e-05, 'epoch': 0.92}

[INFO|2025-03-19 19:33:24] logging.py:157 >> {'loss': 2.0172, 'learning_rate': 3.9121e-05, 'epoch': 0.93}

[INFO|2025-03-19 19:33:40] logging.py:157 >> {'loss': 1.8876, 'learning_rate': 3.9084e-05, 'epoch': 0.93}

[INFO|2025-03-19 19:33:55] logging.py:157 >> {'loss': 1.8251, 'learning_rate': 3.9046e-05, 'epoch': 0.93}

[INFO|2025-03-19 19:34:10] logging.py:157 >> {'loss': 1.8781, 'learning_rate': 3.9009e-05, 'epoch': 0.93}

[INFO|2025-03-19 19:34:10] trainer.py:4117 >> 
***** Running Evaluation *****

[INFO|2025-03-19 19:34:10] trainer.py:4119 >>   Num examples = 5152

[INFO|2025-03-19 19:34:10] trainer.py:4122 >>   Batch size = 2

[INFO|2025-03-19 19:39:12] trainer.py:3801 >> Saving model checkpoint to saves/Llama-3.2-3B-Instruct/lora/train_llama_3_2_v3/checkpoint-2700

[INFO|2025-03-19 19:39:12] tokenization_utils_base.py:2646 >> tokenizer config file saved in saves/Llama-3.2-3B-Instruct/lora/train_llama_3_2_v3/checkpoint-2700/tokenizer_config.json

[INFO|2025-03-19 19:39:12] tokenization_utils_base.py:2655 >> Special tokens file saved in saves/Llama-3.2-3B-Instruct/lora/train_llama_3_2_v3/checkpoint-2700/special_tokens_map.json

[INFO|2025-03-19 19:39:27] logging.py:157 >> {'loss': 1.8778, 'learning_rate': 3.8971e-05, 'epoch': 0.93}

[INFO|2025-03-19 19:39:42] logging.py:157 >> {'loss': 1.6530, 'learning_rate': 3.8934e-05, 'epoch': 0.94}

[INFO|2025-03-19 19:39:56] logging.py:157 >> {'loss': 1.8139, 'learning_rate': 3.8896e-05, 'epoch': 0.94}

[INFO|2025-03-19 19:40:11] logging.py:157 >> {'loss': 1.8168, 'learning_rate': 3.8859e-05, 'epoch': 0.94}

[INFO|2025-03-19 19:40:26] logging.py:157 >> {'loss': 1.8369, 'learning_rate': 3.8821e-05, 'epoch': 0.94}

[INFO|2025-03-19 19:40:40] logging.py:157 >> {'loss': 1.9347, 'learning_rate': 3.8783e-05, 'epoch': 0.94}

[INFO|2025-03-19 19:40:56] logging.py:157 >> {'loss': 1.8198, 'learning_rate': 3.8746e-05, 'epoch': 0.94}

[INFO|2025-03-19 19:41:13] logging.py:157 >> {'loss': 1.8764, 'learning_rate': 3.8708e-05, 'epoch': 0.95}

[INFO|2025-03-19 19:41:28] logging.py:157 >> {'loss': 1.8370, 'learning_rate': 3.8670e-05, 'epoch': 0.95}

[INFO|2025-03-19 19:41:43] logging.py:157 >> {'loss': 2.0625, 'learning_rate': 3.8632e-05, 'epoch': 0.95}

[INFO|2025-03-19 19:41:57] logging.py:157 >> {'loss': 1.8547, 'learning_rate': 3.8594e-05, 'epoch': 0.95}

[INFO|2025-03-19 19:42:11] logging.py:157 >> {'loss': 1.8551, 'learning_rate': 3.8556e-05, 'epoch': 0.95}

[INFO|2025-03-19 19:42:26] logging.py:157 >> {'loss': 1.6988, 'learning_rate': 3.8518e-05, 'epoch': 0.95}

[INFO|2025-03-19 19:42:41] logging.py:157 >> {'loss': 2.0281, 'learning_rate': 3.8480e-05, 'epoch': 0.96}

[INFO|2025-03-19 19:42:56] logging.py:157 >> {'loss': 1.8869, 'learning_rate': 3.8442e-05, 'epoch': 0.96}

[INFO|2025-03-19 19:43:10] logging.py:157 >> {'loss': 1.7047, 'learning_rate': 3.8404e-05, 'epoch': 0.96}

[INFO|2025-03-19 19:43:25] logging.py:157 >> {'loss': 1.9686, 'learning_rate': 3.8366e-05, 'epoch': 0.96}

[INFO|2025-03-19 19:43:40] logging.py:157 >> {'loss': 1.8924, 'learning_rate': 3.8328e-05, 'epoch': 0.96}

[INFO|2025-03-19 19:43:55] logging.py:157 >> {'loss': 1.7634, 'learning_rate': 3.8290e-05, 'epoch': 0.96}

[INFO|2025-03-19 19:44:10] logging.py:157 >> {'loss': 2.0422, 'learning_rate': 3.8251e-05, 'epoch': 0.97}

[INFO|2025-03-19 19:44:10] trainer.py:4117 >> 
***** Running Evaluation *****

[INFO|2025-03-19 19:44:10] trainer.py:4119 >>   Num examples = 5152

[INFO|2025-03-19 19:44:10] trainer.py:4122 >>   Batch size = 2

[INFO|2025-03-19 19:49:12] trainer.py:3801 >> Saving model checkpoint to saves/Llama-3.2-3B-Instruct/lora/train_llama_3_2_v3/checkpoint-2800

[INFO|2025-03-19 19:49:13] tokenization_utils_base.py:2646 >> tokenizer config file saved in saves/Llama-3.2-3B-Instruct/lora/train_llama_3_2_v3/checkpoint-2800/tokenizer_config.json

[INFO|2025-03-19 19:49:13] tokenization_utils_base.py:2655 >> Special tokens file saved in saves/Llama-3.2-3B-Instruct/lora/train_llama_3_2_v3/checkpoint-2800/special_tokens_map.json

[INFO|2025-03-19 19:49:28] logging.py:157 >> {'loss': 2.0257, 'learning_rate': 3.8213e-05, 'epoch': 0.97}

[INFO|2025-03-19 19:49:42] logging.py:157 >> {'loss': 1.8820, 'learning_rate': 3.8175e-05, 'epoch': 0.97}

[INFO|2025-03-19 19:49:57] logging.py:157 >> {'loss': 1.9902, 'learning_rate': 3.8136e-05, 'epoch': 0.97}

[INFO|2025-03-19 19:50:12] logging.py:157 >> {'loss': 1.7880, 'learning_rate': 3.8098e-05, 'epoch': 0.97}

[INFO|2025-03-19 19:50:28] logging.py:157 >> {'loss': 1.8787, 'learning_rate': 3.8059e-05, 'epoch': 0.97}

[INFO|2025-03-19 19:50:43] logging.py:157 >> {'loss': 2.0136, 'learning_rate': 3.8021e-05, 'epoch': 0.98}

[INFO|2025-03-19 19:50:57] logging.py:157 >> {'loss': 1.7947, 'learning_rate': 3.7982e-05, 'epoch': 0.98}

[INFO|2025-03-19 19:51:12] logging.py:157 >> {'loss': 1.8601, 'learning_rate': 3.7943e-05, 'epoch': 0.98}

[INFO|2025-03-19 19:51:27] logging.py:157 >> {'loss': 1.9215, 'learning_rate': 3.7905e-05, 'epoch': 0.98}

[INFO|2025-03-19 19:51:42] logging.py:157 >> {'loss': 2.0120, 'learning_rate': 3.7866e-05, 'epoch': 0.98}

[INFO|2025-03-19 19:51:56] logging.py:157 >> {'loss': 1.8174, 'learning_rate': 3.7827e-05, 'epoch': 0.99}

[INFO|2025-03-19 19:52:12] logging.py:157 >> {'loss': 1.8889, 'learning_rate': 3.7788e-05, 'epoch': 0.99}

[INFO|2025-03-19 19:52:25] logging.py:157 >> {'loss': 1.8359, 'learning_rate': 3.7750e-05, 'epoch': 0.99}

[INFO|2025-03-19 19:52:40] logging.py:157 >> {'loss': 1.8520, 'learning_rate': 3.7711e-05, 'epoch': 0.99}

[INFO|2025-03-19 19:52:53] logging.py:157 >> {'loss': 1.8708, 'learning_rate': 3.7672e-05, 'epoch': 0.99}

[INFO|2025-03-19 19:53:09] logging.py:157 >> {'loss': 1.9009, 'learning_rate': 3.7633e-05, 'epoch': 0.99}

[INFO|2025-03-19 19:53:24] logging.py:157 >> {'loss': 1.7552, 'learning_rate': 3.7594e-05, 'epoch': 1.00}

[INFO|2025-03-19 19:53:38] logging.py:157 >> {'loss': 1.9489, 'learning_rate': 3.7555e-05, 'epoch': 1.00}

[INFO|2025-03-19 19:53:53] logging.py:157 >> {'loss': 1.8540, 'learning_rate': 3.7516e-05, 'epoch': 1.00}

[INFO|2025-03-19 19:54:08] logging.py:157 >> {'loss': 1.9024, 'learning_rate': 3.7477e-05, 'epoch': 1.00}

[INFO|2025-03-19 19:54:08] trainer.py:4117 >> 
***** Running Evaluation *****

[INFO|2025-03-19 19:54:08] trainer.py:4119 >>   Num examples = 5152

[INFO|2025-03-19 19:54:08] trainer.py:4122 >>   Batch size = 2

[INFO|2025-03-19 19:59:10] trainer.py:3801 >> Saving model checkpoint to saves/Llama-3.2-3B-Instruct/lora/train_llama_3_2_v3/checkpoint-2900

[INFO|2025-03-19 19:59:11] tokenization_utils_base.py:2646 >> tokenizer config file saved in saves/Llama-3.2-3B-Instruct/lora/train_llama_3_2_v3/checkpoint-2900/tokenizer_config.json

[INFO|2025-03-19 19:59:11] tokenization_utils_base.py:2655 >> Special tokens file saved in saves/Llama-3.2-3B-Instruct/lora/train_llama_3_2_v3/checkpoint-2900/special_tokens_map.json

[INFO|2025-03-19 19:59:25] logging.py:157 >> {'loss': 1.5814, 'learning_rate': 3.7437e-05, 'epoch': 1.00}

[INFO|2025-03-19 19:59:39] logging.py:157 >> {'loss': 1.5944, 'learning_rate': 3.7398e-05, 'epoch': 1.00}

[INFO|2025-03-19 19:59:54] logging.py:157 >> {'loss': 1.6369, 'learning_rate': 3.7359e-05, 'epoch': 1.01}

[INFO|2025-03-19 20:00:08] logging.py:157 >> {'loss': 1.5003, 'learning_rate': 3.7320e-05, 'epoch': 1.01}

[INFO|2025-03-19 20:00:22] logging.py:157 >> {'loss': 1.6768, 'learning_rate': 3.7280e-05, 'epoch': 1.01}

[INFO|2025-03-19 20:00:37] logging.py:157 >> {'loss': 1.7238, 'learning_rate': 3.7241e-05, 'epoch': 1.01}

[INFO|2025-03-19 20:00:52] logging.py:157 >> {'loss': 1.6192, 'learning_rate': 3.7201e-05, 'epoch': 1.01}

[INFO|2025-03-19 20:01:07] logging.py:157 >> {'loss': 1.5605, 'learning_rate': 3.7162e-05, 'epoch': 1.01}

[INFO|2025-03-19 20:01:23] logging.py:157 >> {'loss': 1.4791, 'learning_rate': 3.7122e-05, 'epoch': 1.02}

[INFO|2025-03-19 20:01:38] logging.py:157 >> {'loss': 1.4930, 'learning_rate': 3.7083e-05, 'epoch': 1.02}

[INFO|2025-03-19 20:01:52] logging.py:157 >> {'loss': 1.5772, 'learning_rate': 3.7043e-05, 'epoch': 1.02}

[INFO|2025-03-19 20:02:07] logging.py:157 >> {'loss': 1.5512, 'learning_rate': 3.7004e-05, 'epoch': 1.02}

[INFO|2025-03-19 20:02:21] logging.py:157 >> {'loss': 1.7054, 'learning_rate': 3.6964e-05, 'epoch': 1.02}

[INFO|2025-03-19 20:02:36] logging.py:157 >> {'loss': 1.8071, 'learning_rate': 3.6924e-05, 'epoch': 1.03}

[INFO|2025-03-19 20:02:49] logging.py:157 >> {'loss': 1.5788, 'learning_rate': 3.6885e-05, 'epoch': 1.03}

[INFO|2025-03-19 20:03:03] logging.py:157 >> {'loss': 1.5965, 'learning_rate': 3.6845e-05, 'epoch': 1.03}

[INFO|2025-03-19 20:03:18] logging.py:157 >> {'loss': 1.6429, 'learning_rate': 3.6805e-05, 'epoch': 1.03}

[INFO|2025-03-19 20:03:33] logging.py:157 >> {'loss': 1.7127, 'learning_rate': 3.6765e-05, 'epoch': 1.03}

[INFO|2025-03-19 20:03:48] logging.py:157 >> {'loss': 1.7474, 'learning_rate': 3.6725e-05, 'epoch': 1.03}

[INFO|2025-03-19 20:04:02] logging.py:157 >> {'loss': 1.4639, 'learning_rate': 3.6685e-05, 'epoch': 1.04}

[INFO|2025-03-19 20:04:02] trainer.py:4117 >> 
***** Running Evaluation *****

[INFO|2025-03-19 20:04:02] trainer.py:4119 >>   Num examples = 5152

[INFO|2025-03-19 20:04:02] trainer.py:4122 >>   Batch size = 2

[INFO|2025-03-19 20:09:05] trainer.py:3801 >> Saving model checkpoint to saves/Llama-3.2-3B-Instruct/lora/train_llama_3_2_v3/checkpoint-3000

[INFO|2025-03-19 20:09:05] tokenization_utils_base.py:2646 >> tokenizer config file saved in saves/Llama-3.2-3B-Instruct/lora/train_llama_3_2_v3/checkpoint-3000/tokenizer_config.json

[INFO|2025-03-19 20:09:05] tokenization_utils_base.py:2655 >> Special tokens file saved in saves/Llama-3.2-3B-Instruct/lora/train_llama_3_2_v3/checkpoint-3000/special_tokens_map.json

[INFO|2025-03-19 20:09:20] logging.py:157 >> {'loss': 1.4992, 'learning_rate': 3.6645e-05, 'epoch': 1.04}

[INFO|2025-03-19 20:09:35] logging.py:157 >> {'loss': 1.6250, 'learning_rate': 3.6605e-05, 'epoch': 1.04}

[INFO|2025-03-19 20:09:49] logging.py:157 >> {'loss': 1.8569, 'learning_rate': 3.6565e-05, 'epoch': 1.04}

[INFO|2025-03-19 20:10:04] logging.py:157 >> {'loss': 1.5268, 'learning_rate': 3.6525e-05, 'epoch': 1.04}

[INFO|2025-03-19 20:10:17] logging.py:157 >> {'loss': 1.6616, 'learning_rate': 3.6485e-05, 'epoch': 1.04}

[INFO|2025-03-19 20:10:33] logging.py:157 >> {'loss': 1.6995, 'learning_rate': 3.6445e-05, 'epoch': 1.05}

[INFO|2025-03-19 20:10:49] logging.py:157 >> {'loss': 1.5241, 'learning_rate': 3.6405e-05, 'epoch': 1.05}

[INFO|2025-03-19 20:11:04] logging.py:157 >> {'loss': 1.4337, 'learning_rate': 3.6365e-05, 'epoch': 1.05}

[INFO|2025-03-19 20:11:19] logging.py:157 >> {'loss': 1.6571, 'learning_rate': 3.6324e-05, 'epoch': 1.05}

[INFO|2025-03-19 20:11:33] logging.py:157 >> {'loss': 1.8350, 'learning_rate': 3.6284e-05, 'epoch': 1.05}

[INFO|2025-03-19 20:11:48] logging.py:157 >> {'loss': 1.6627, 'learning_rate': 3.6244e-05, 'epoch': 1.05}

[INFO|2025-03-19 20:12:02] logging.py:157 >> {'loss': 1.4129, 'learning_rate': 3.6203e-05, 'epoch': 1.06}

[INFO|2025-03-19 20:12:16] logging.py:157 >> {'loss': 1.8261, 'learning_rate': 3.6163e-05, 'epoch': 1.06}

[INFO|2025-03-19 20:12:31] logging.py:157 >> {'loss': 1.7533, 'learning_rate': 3.6123e-05, 'epoch': 1.06}

[INFO|2025-03-19 20:12:46] logging.py:157 >> {'loss': 1.5281, 'learning_rate': 3.6082e-05, 'epoch': 1.06}

[INFO|2025-03-19 20:13:01] logging.py:157 >> {'loss': 1.6153, 'learning_rate': 3.6042e-05, 'epoch': 1.06}

[INFO|2025-03-19 20:13:16] logging.py:157 >> {'loss': 1.7719, 'learning_rate': 3.6001e-05, 'epoch': 1.06}

[INFO|2025-03-19 20:13:31] logging.py:157 >> {'loss': 1.8393, 'learning_rate': 3.5960e-05, 'epoch': 1.07}

[INFO|2025-03-19 20:13:47] logging.py:157 >> {'loss': 1.5416, 'learning_rate': 3.5920e-05, 'epoch': 1.07}

[INFO|2025-03-19 20:14:01] logging.py:157 >> {'loss': 1.6678, 'learning_rate': 3.5879e-05, 'epoch': 1.07}

[INFO|2025-03-19 20:14:01] trainer.py:4117 >> 
***** Running Evaluation *****

[INFO|2025-03-19 20:14:01] trainer.py:4119 >>   Num examples = 5152

[INFO|2025-03-19 20:14:01] trainer.py:4122 >>   Batch size = 2

[INFO|2025-03-19 20:19:04] trainer.py:3801 >> Saving model checkpoint to saves/Llama-3.2-3B-Instruct/lora/train_llama_3_2_v3/checkpoint-3100

[INFO|2025-03-19 20:19:04] tokenization_utils_base.py:2646 >> tokenizer config file saved in saves/Llama-3.2-3B-Instruct/lora/train_llama_3_2_v3/checkpoint-3100/tokenizer_config.json

[INFO|2025-03-19 20:19:04] tokenization_utils_base.py:2655 >> Special tokens file saved in saves/Llama-3.2-3B-Instruct/lora/train_llama_3_2_v3/checkpoint-3100/special_tokens_map.json

[INFO|2025-03-19 20:19:20] logging.py:157 >> {'loss': 1.4811, 'learning_rate': 3.5838e-05, 'epoch': 1.07}

[INFO|2025-03-19 20:19:34] logging.py:157 >> {'loss': 1.5877, 'learning_rate': 3.5798e-05, 'epoch': 1.07}

[INFO|2025-03-19 20:19:50] logging.py:157 >> {'loss': 1.6440, 'learning_rate': 3.5757e-05, 'epoch': 1.08}

[INFO|2025-03-19 20:20:07] logging.py:157 >> {'loss': 1.6025, 'learning_rate': 3.5716e-05, 'epoch': 1.08}

[INFO|2025-03-19 20:20:21] logging.py:157 >> {'loss': 1.4799, 'learning_rate': 3.5675e-05, 'epoch': 1.08}

[INFO|2025-03-19 20:20:36] logging.py:157 >> {'loss': 1.7575, 'learning_rate': 3.5634e-05, 'epoch': 1.08}

[INFO|2025-03-19 20:20:48] logging.py:157 >> {'loss': 1.7012, 'learning_rate': 3.5593e-05, 'epoch': 1.08}

[INFO|2025-03-19 20:21:03] logging.py:157 >> {'loss': 1.6610, 'learning_rate': 3.5552e-05, 'epoch': 1.08}

[INFO|2025-03-19 20:21:18] logging.py:157 >> {'loss': 1.5674, 'learning_rate': 3.5512e-05, 'epoch': 1.09}

[INFO|2025-03-19 20:21:34] logging.py:157 >> {'loss': 1.7397, 'learning_rate': 3.5470e-05, 'epoch': 1.09}

[INFO|2025-03-19 20:21:48] logging.py:157 >> {'loss': 1.6861, 'learning_rate': 3.5429e-05, 'epoch': 1.09}

[INFO|2025-03-19 20:22:02] logging.py:157 >> {'loss': 1.6830, 'learning_rate': 3.5388e-05, 'epoch': 1.09}

[INFO|2025-03-19 20:22:18] logging.py:157 >> {'loss': 1.4574, 'learning_rate': 3.5347e-05, 'epoch': 1.09}

[INFO|2025-03-19 20:22:32] logging.py:157 >> {'loss': 1.4857, 'learning_rate': 3.5306e-05, 'epoch': 1.09}

[INFO|2025-03-19 20:22:47] logging.py:157 >> {'loss': 1.5837, 'learning_rate': 3.5265e-05, 'epoch': 1.10}

[INFO|2025-03-19 20:23:02] logging.py:157 >> {'loss': 1.6527, 'learning_rate': 3.5224e-05, 'epoch': 1.10}

[INFO|2025-03-19 20:23:18] logging.py:157 >> {'loss': 1.8804, 'learning_rate': 3.5182e-05, 'epoch': 1.10}

[INFO|2025-03-19 20:23:34] logging.py:157 >> {'loss': 1.6321, 'learning_rate': 3.5141e-05, 'epoch': 1.10}

[INFO|2025-03-19 20:23:48] logging.py:157 >> {'loss': 1.7520, 'learning_rate': 3.5100e-05, 'epoch': 1.10}

[INFO|2025-03-19 20:24:02] logging.py:157 >> {'loss': 1.5044, 'learning_rate': 3.5058e-05, 'epoch': 1.10}

[INFO|2025-03-19 20:24:02] trainer.py:4117 >> 
***** Running Evaluation *****

[INFO|2025-03-19 20:24:02] trainer.py:4119 >>   Num examples = 5152

[INFO|2025-03-19 20:24:02] trainer.py:4122 >>   Batch size = 2

[INFO|2025-03-19 20:29:05] trainer.py:3801 >> Saving model checkpoint to saves/Llama-3.2-3B-Instruct/lora/train_llama_3_2_v3/checkpoint-3200

[INFO|2025-03-19 20:29:05] tokenization_utils_base.py:2646 >> tokenizer config file saved in saves/Llama-3.2-3B-Instruct/lora/train_llama_3_2_v3/checkpoint-3200/tokenizer_config.json

[INFO|2025-03-19 20:29:05] tokenization_utils_base.py:2655 >> Special tokens file saved in saves/Llama-3.2-3B-Instruct/lora/train_llama_3_2_v3/checkpoint-3200/special_tokens_map.json

[INFO|2025-03-19 20:29:19] logging.py:157 >> {'loss': 1.6299, 'learning_rate': 3.5017e-05, 'epoch': 1.11}

[INFO|2025-03-19 20:29:33] logging.py:157 >> {'loss': 1.5704, 'learning_rate': 3.4976e-05, 'epoch': 1.11}

[INFO|2025-03-19 20:29:47] logging.py:157 >> {'loss': 1.5801, 'learning_rate': 3.4934e-05, 'epoch': 1.11}

[INFO|2025-03-19 20:30:01] logging.py:157 >> {'loss': 1.7531, 'learning_rate': 3.4893e-05, 'epoch': 1.11}

[INFO|2025-03-19 20:30:16] logging.py:157 >> {'loss': 1.5373, 'learning_rate': 3.4851e-05, 'epoch': 1.11}

[INFO|2025-03-19 20:30:31] logging.py:157 >> {'loss': 1.6557, 'learning_rate': 3.4810e-05, 'epoch': 1.11}

[INFO|2025-03-19 20:30:46] logging.py:157 >> {'loss': 1.5573, 'learning_rate': 3.4768e-05, 'epoch': 1.12}

[INFO|2025-03-19 20:31:00] logging.py:157 >> {'loss': 1.5935, 'learning_rate': 3.4727e-05, 'epoch': 1.12}

[INFO|2025-03-19 20:31:15] logging.py:157 >> {'loss': 1.5243, 'learning_rate': 3.4685e-05, 'epoch': 1.12}

[INFO|2025-03-19 20:31:30] logging.py:157 >> {'loss': 1.5550, 'learning_rate': 3.4643e-05, 'epoch': 1.12}

[INFO|2025-03-19 20:31:44] logging.py:157 >> {'loss': 1.5634, 'learning_rate': 3.4602e-05, 'epoch': 1.12}

[INFO|2025-03-19 20:31:58] logging.py:157 >> {'loss': 1.4843, 'learning_rate': 3.4560e-05, 'epoch': 1.13}

[INFO|2025-03-19 20:32:12] logging.py:157 >> {'loss': 1.7854, 'learning_rate': 3.4518e-05, 'epoch': 1.13}

[INFO|2025-03-19 20:32:29] logging.py:157 >> {'loss': 1.6619, 'learning_rate': 3.4476e-05, 'epoch': 1.13}

[INFO|2025-03-19 20:32:44] logging.py:157 >> {'loss': 1.6062, 'learning_rate': 3.4434e-05, 'epoch': 1.13}

[INFO|2025-03-19 20:32:59] logging.py:157 >> {'loss': 1.6130, 'learning_rate': 3.4393e-05, 'epoch': 1.13}

[INFO|2025-03-19 20:33:14] logging.py:157 >> {'loss': 1.5600, 'learning_rate': 3.4351e-05, 'epoch': 1.13}

[INFO|2025-03-19 20:33:29] logging.py:157 >> {'loss': 1.5538, 'learning_rate': 3.4309e-05, 'epoch': 1.14}

[INFO|2025-03-19 20:33:43] logging.py:157 >> {'loss': 1.6157, 'learning_rate': 3.4267e-05, 'epoch': 1.14}

[INFO|2025-03-19 20:33:59] logging.py:157 >> {'loss': 1.7035, 'learning_rate': 3.4225e-05, 'epoch': 1.14}

[INFO|2025-03-19 20:33:59] trainer.py:4117 >> 
***** Running Evaluation *****

[INFO|2025-03-19 20:33:59] trainer.py:4119 >>   Num examples = 5152

[INFO|2025-03-19 20:33:59] trainer.py:4122 >>   Batch size = 2

[INFO|2025-03-19 20:39:02] trainer.py:3801 >> Saving model checkpoint to saves/Llama-3.2-3B-Instruct/lora/train_llama_3_2_v3/checkpoint-3300

[INFO|2025-03-19 20:39:02] tokenization_utils_base.py:2646 >> tokenizer config file saved in saves/Llama-3.2-3B-Instruct/lora/train_llama_3_2_v3/checkpoint-3300/tokenizer_config.json

[INFO|2025-03-19 20:39:02] tokenization_utils_base.py:2655 >> Special tokens file saved in saves/Llama-3.2-3B-Instruct/lora/train_llama_3_2_v3/checkpoint-3300/special_tokens_map.json

[INFO|2025-03-19 20:39:18] logging.py:157 >> {'loss': 1.7343, 'learning_rate': 3.4183e-05, 'epoch': 1.14}

[INFO|2025-03-19 20:39:32] logging.py:157 >> {'loss': 1.5216, 'learning_rate': 3.4141e-05, 'epoch': 1.14}

[INFO|2025-03-19 20:39:48] logging.py:157 >> {'loss': 1.7536, 'learning_rate': 3.4099e-05, 'epoch': 1.14}

[INFO|2025-03-19 20:40:02] logging.py:157 >> {'loss': 1.6519, 'learning_rate': 3.4057e-05, 'epoch': 1.15}

[INFO|2025-03-19 20:40:16] logging.py:157 >> {'loss': 1.5974, 'learning_rate': 3.4014e-05, 'epoch': 1.15}

[INFO|2025-03-19 20:40:31] logging.py:157 >> {'loss': 1.6622, 'learning_rate': 3.3972e-05, 'epoch': 1.15}

[INFO|2025-03-19 20:40:45] logging.py:157 >> {'loss': 1.5698, 'learning_rate': 3.3930e-05, 'epoch': 1.15}

[INFO|2025-03-19 20:41:01] logging.py:157 >> {'loss': 1.7524, 'learning_rate': 3.3888e-05, 'epoch': 1.15}

[INFO|2025-03-19 20:41:16] logging.py:157 >> {'loss': 1.5639, 'learning_rate': 3.3846e-05, 'epoch': 1.15}

[INFO|2025-03-19 20:41:30] logging.py:157 >> {'loss': 1.5012, 'learning_rate': 3.3803e-05, 'epoch': 1.16}

[INFO|2025-03-19 20:41:44] logging.py:157 >> {'loss': 1.6886, 'learning_rate': 3.3761e-05, 'epoch': 1.16}

[INFO|2025-03-19 20:42:00] logging.py:157 >> {'loss': 1.5278, 'learning_rate': 3.3719e-05, 'epoch': 1.16}

[INFO|2025-03-19 20:42:15] logging.py:157 >> {'loss': 1.7021, 'learning_rate': 3.3676e-05, 'epoch': 1.16}

[INFO|2025-03-19 20:42:29] logging.py:157 >> {'loss': 1.6171, 'learning_rate': 3.3634e-05, 'epoch': 1.16}

[INFO|2025-03-19 20:42:44] logging.py:157 >> {'loss': 1.6415, 'learning_rate': 3.3592e-05, 'epoch': 1.16}

[INFO|2025-03-19 20:42:57] logging.py:157 >> {'loss': 1.4272, 'learning_rate': 3.3549e-05, 'epoch': 1.17}

[INFO|2025-03-19 20:43:11] logging.py:157 >> {'loss': 1.5583, 'learning_rate': 3.3507e-05, 'epoch': 1.17}

[INFO|2025-03-19 20:43:26] logging.py:157 >> {'loss': 1.4458, 'learning_rate': 3.3464e-05, 'epoch': 1.17}

[INFO|2025-03-19 20:43:41] logging.py:157 >> {'loss': 1.6661, 'learning_rate': 3.3422e-05, 'epoch': 1.17}

[INFO|2025-03-19 20:43:55] logging.py:157 >> {'loss': 1.5468, 'learning_rate': 3.3379e-05, 'epoch': 1.17}

[INFO|2025-03-19 20:43:55] trainer.py:4117 >> 
***** Running Evaluation *****

[INFO|2025-03-19 20:43:55] trainer.py:4119 >>   Num examples = 5152

[INFO|2025-03-19 20:43:55] trainer.py:4122 >>   Batch size = 2

[INFO|2025-03-19 20:48:57] trainer.py:3801 >> Saving model checkpoint to saves/Llama-3.2-3B-Instruct/lora/train_llama_3_2_v3/checkpoint-3400

[INFO|2025-03-19 20:48:57] tokenization_utils_base.py:2646 >> tokenizer config file saved in saves/Llama-3.2-3B-Instruct/lora/train_llama_3_2_v3/checkpoint-3400/tokenizer_config.json

[INFO|2025-03-19 20:48:57] tokenization_utils_base.py:2655 >> Special tokens file saved in saves/Llama-3.2-3B-Instruct/lora/train_llama_3_2_v3/checkpoint-3400/special_tokens_map.json

[INFO|2025-03-19 20:49:12] logging.py:157 >> {'loss': 1.7464, 'learning_rate': 3.3336e-05, 'epoch': 1.18}

[INFO|2025-03-19 20:49:25] logging.py:157 >> {'loss': 1.6995, 'learning_rate': 3.3294e-05, 'epoch': 1.18}

[INFO|2025-03-19 20:49:39] logging.py:157 >> {'loss': 1.6856, 'learning_rate': 3.3251e-05, 'epoch': 1.18}

[INFO|2025-03-19 20:49:54] logging.py:157 >> {'loss': 1.7915, 'learning_rate': 3.3209e-05, 'epoch': 1.18}

[INFO|2025-03-19 20:50:09] logging.py:157 >> {'loss': 1.6551, 'learning_rate': 3.3166e-05, 'epoch': 1.18}

[INFO|2025-03-19 20:50:23] logging.py:157 >> {'loss': 1.5769, 'learning_rate': 3.3123e-05, 'epoch': 1.18}

[INFO|2025-03-19 20:50:37] logging.py:157 >> {'loss': 1.6655, 'learning_rate': 3.3080e-05, 'epoch': 1.19}

[INFO|2025-03-19 20:50:52] logging.py:157 >> {'loss': 1.6876, 'learning_rate': 3.3038e-05, 'epoch': 1.19}

[INFO|2025-03-19 20:51:08] logging.py:157 >> {'loss': 1.5485, 'learning_rate': 3.2995e-05, 'epoch': 1.19}

[INFO|2025-03-19 20:51:22] logging.py:157 >> {'loss': 1.5468, 'learning_rate': 3.2952e-05, 'epoch': 1.19}

[INFO|2025-03-19 20:51:37] logging.py:157 >> {'loss': 1.4810, 'learning_rate': 3.2909e-05, 'epoch': 1.19}

[INFO|2025-03-19 20:51:52] logging.py:157 >> {'loss': 1.5224, 'learning_rate': 3.2866e-05, 'epoch': 1.19}

[INFO|2025-03-19 20:52:09] logging.py:157 >> {'loss': 1.4852, 'learning_rate': 3.2823e-05, 'epoch': 1.20}

[INFO|2025-03-19 20:52:22] logging.py:157 >> {'loss': 1.7479, 'learning_rate': 3.2780e-05, 'epoch': 1.20}

[INFO|2025-03-19 20:52:36] logging.py:157 >> {'loss': 1.5145, 'learning_rate': 3.2737e-05, 'epoch': 1.20}

[INFO|2025-03-19 20:52:51] logging.py:157 >> {'loss': 1.3681, 'learning_rate': 3.2694e-05, 'epoch': 1.20}

[INFO|2025-03-19 20:53:05] logging.py:157 >> {'loss': 1.4684, 'learning_rate': 3.2651e-05, 'epoch': 1.20}

[INFO|2025-03-19 20:53:22] logging.py:157 >> {'loss': 1.6797, 'learning_rate': 3.2608e-05, 'epoch': 1.20}

[INFO|2025-03-19 20:53:37] logging.py:157 >> {'loss': 1.7841, 'learning_rate': 3.2565e-05, 'epoch': 1.21}

[INFO|2025-03-19 20:53:52] logging.py:157 >> {'loss': 1.5870, 'learning_rate': 3.2522e-05, 'epoch': 1.21}

[INFO|2025-03-19 20:53:52] trainer.py:4117 >> 
***** Running Evaluation *****

[INFO|2025-03-19 20:53:52] trainer.py:4119 >>   Num examples = 5152

[INFO|2025-03-19 20:53:52] trainer.py:4122 >>   Batch size = 2

[INFO|2025-03-19 20:58:54] trainer.py:3801 >> Saving model checkpoint to saves/Llama-3.2-3B-Instruct/lora/train_llama_3_2_v3/checkpoint-3500

[INFO|2025-03-19 20:58:54] tokenization_utils_base.py:2646 >> tokenizer config file saved in saves/Llama-3.2-3B-Instruct/lora/train_llama_3_2_v3/checkpoint-3500/tokenizer_config.json

[INFO|2025-03-19 20:58:54] tokenization_utils_base.py:2655 >> Special tokens file saved in saves/Llama-3.2-3B-Instruct/lora/train_llama_3_2_v3/checkpoint-3500/special_tokens_map.json

[INFO|2025-03-19 20:59:10] logging.py:157 >> {'loss': 1.6380, 'learning_rate': 3.2479e-05, 'epoch': 1.21}

[INFO|2025-03-19 20:59:24] logging.py:157 >> {'loss': 1.7114, 'learning_rate': 3.2436e-05, 'epoch': 1.21}

[INFO|2025-03-19 20:59:38] logging.py:157 >> {'loss': 1.4603, 'learning_rate': 3.2393e-05, 'epoch': 1.21}

[INFO|2025-03-19 20:59:52] logging.py:157 >> {'loss': 1.5752, 'learning_rate': 3.2350e-05, 'epoch': 1.21}

[INFO|2025-03-19 21:00:06] logging.py:157 >> {'loss': 1.6436, 'learning_rate': 3.2307e-05, 'epoch': 1.22}

[INFO|2025-03-19 21:00:20] logging.py:157 >> {'loss': 1.7136, 'learning_rate': 3.2263e-05, 'epoch': 1.22}

[INFO|2025-03-19 21:00:34] logging.py:157 >> {'loss': 1.5953, 'learning_rate': 3.2220e-05, 'epoch': 1.22}

[INFO|2025-03-19 21:00:48] logging.py:157 >> {'loss': 1.5091, 'learning_rate': 3.2177e-05, 'epoch': 1.22}

[INFO|2025-03-19 21:01:02] logging.py:157 >> {'loss': 1.4829, 'learning_rate': 3.2134e-05, 'epoch': 1.22}

[INFO|2025-03-19 21:01:16] logging.py:157 >> {'loss': 1.5893, 'learning_rate': 3.2090e-05, 'epoch': 1.23}

[INFO|2025-03-19 21:01:30] logging.py:157 >> {'loss': 1.6360, 'learning_rate': 3.2047e-05, 'epoch': 1.23}

[INFO|2025-03-19 21:01:45] logging.py:157 >> {'loss': 1.5176, 'learning_rate': 3.2003e-05, 'epoch': 1.23}

[INFO|2025-03-19 21:02:00] logging.py:157 >> {'loss': 1.5923, 'learning_rate': 3.1960e-05, 'epoch': 1.23}

[INFO|2025-03-19 21:02:15] logging.py:157 >> {'loss': 1.7818, 'learning_rate': 3.1917e-05, 'epoch': 1.23}

[INFO|2025-03-19 21:02:31] logging.py:157 >> {'loss': 1.6905, 'learning_rate': 3.1873e-05, 'epoch': 1.23}

[INFO|2025-03-19 21:02:46] logging.py:157 >> {'loss': 1.6911, 'learning_rate': 3.1830e-05, 'epoch': 1.24}

[INFO|2025-03-19 21:03:02] logging.py:157 >> {'loss': 1.7496, 'learning_rate': 3.1786e-05, 'epoch': 1.24}

[INFO|2025-03-19 21:03:16] logging.py:157 >> {'loss': 1.7336, 'learning_rate': 3.1743e-05, 'epoch': 1.24}

[INFO|2025-03-19 21:03:31] logging.py:157 >> {'loss': 1.4650, 'learning_rate': 3.1699e-05, 'epoch': 1.24}

[INFO|2025-03-19 21:03:45] logging.py:157 >> {'loss': 1.6104, 'learning_rate': 3.1656e-05, 'epoch': 1.24}

[INFO|2025-03-19 21:03:45] trainer.py:4117 >> 
***** Running Evaluation *****

[INFO|2025-03-19 21:03:45] trainer.py:4119 >>   Num examples = 5152

[INFO|2025-03-19 21:03:45] trainer.py:4122 >>   Batch size = 2

[INFO|2025-03-19 21:08:47] trainer.py:3801 >> Saving model checkpoint to saves/Llama-3.2-3B-Instruct/lora/train_llama_3_2_v3/checkpoint-3600

[INFO|2025-03-19 21:08:48] tokenization_utils_base.py:2646 >> tokenizer config file saved in saves/Llama-3.2-3B-Instruct/lora/train_llama_3_2_v3/checkpoint-3600/tokenizer_config.json

[INFO|2025-03-19 21:08:48] tokenization_utils_base.py:2655 >> Special tokens file saved in saves/Llama-3.2-3B-Instruct/lora/train_llama_3_2_v3/checkpoint-3600/special_tokens_map.json

[INFO|2025-03-19 21:09:03] logging.py:157 >> {'loss': 1.6578, 'learning_rate': 3.1612e-05, 'epoch': 1.24}

[INFO|2025-03-19 21:09:17] logging.py:157 >> {'loss': 1.6152, 'learning_rate': 3.1569e-05, 'epoch': 1.25}

[INFO|2025-03-19 21:09:31] logging.py:157 >> {'loss': 1.5520, 'learning_rate': 3.1525e-05, 'epoch': 1.25}

[INFO|2025-03-19 21:09:46] logging.py:157 >> {'loss': 1.5490, 'learning_rate': 3.1481e-05, 'epoch': 1.25}

[INFO|2025-03-19 21:10:00] logging.py:157 >> {'loss': 1.6619, 'learning_rate': 3.1438e-05, 'epoch': 1.25}

[INFO|2025-03-19 21:10:14] logging.py:157 >> {'loss': 1.4934, 'learning_rate': 3.1394e-05, 'epoch': 1.25}

[INFO|2025-03-19 21:10:29] logging.py:157 >> {'loss': 1.5786, 'learning_rate': 3.1350e-05, 'epoch': 1.25}

[INFO|2025-03-19 21:10:43] logging.py:157 >> {'loss': 1.6549, 'learning_rate': 3.1307e-05, 'epoch': 1.26}

[INFO|2025-03-19 21:10:58] logging.py:157 >> {'loss': 1.5668, 'learning_rate': 3.1263e-05, 'epoch': 1.26}

[INFO|2025-03-19 21:11:13] logging.py:157 >> {'loss': 1.6708, 'learning_rate': 3.1219e-05, 'epoch': 1.26}

[INFO|2025-03-19 21:11:29] logging.py:157 >> {'loss': 1.5536, 'learning_rate': 3.1175e-05, 'epoch': 1.26}

[INFO|2025-03-19 21:11:44] logging.py:157 >> {'loss': 1.7482, 'learning_rate': 3.1132e-05, 'epoch': 1.26}

[INFO|2025-03-19 21:11:59] logging.py:157 >> {'loss': 1.4481, 'learning_rate': 3.1088e-05, 'epoch': 1.26}

[INFO|2025-03-19 21:12:14] logging.py:157 >> {'loss': 1.4390, 'learning_rate': 3.1044e-05, 'epoch': 1.27}

[INFO|2025-03-19 21:12:28] logging.py:157 >> {'loss': 1.7446, 'learning_rate': 3.1000e-05, 'epoch': 1.27}

[INFO|2025-03-19 21:12:43] logging.py:157 >> {'loss': 1.7079, 'learning_rate': 3.0956e-05, 'epoch': 1.27}

[INFO|2025-03-19 21:12:59] logging.py:157 >> {'loss': 1.6566, 'learning_rate': 3.0912e-05, 'epoch': 1.27}

[INFO|2025-03-19 21:13:13] logging.py:157 >> {'loss': 1.5546, 'learning_rate': 3.0868e-05, 'epoch': 1.27}

[INFO|2025-03-19 21:13:27] logging.py:157 >> {'loss': 1.6433, 'learning_rate': 3.0824e-05, 'epoch': 1.28}

[INFO|2025-03-19 21:13:42] logging.py:157 >> {'loss': 1.7821, 'learning_rate': 3.0781e-05, 'epoch': 1.28}

[INFO|2025-03-19 21:13:42] trainer.py:4117 >> 
***** Running Evaluation *****

[INFO|2025-03-19 21:13:42] trainer.py:4119 >>   Num examples = 5152

[INFO|2025-03-19 21:13:42] trainer.py:4122 >>   Batch size = 2

[INFO|2025-03-19 21:18:44] trainer.py:3801 >> Saving model checkpoint to saves/Llama-3.2-3B-Instruct/lora/train_llama_3_2_v3/checkpoint-3700

[INFO|2025-03-19 21:18:44] tokenization_utils_base.py:2646 >> tokenizer config file saved in saves/Llama-3.2-3B-Instruct/lora/train_llama_3_2_v3/checkpoint-3700/tokenizer_config.json

[INFO|2025-03-19 21:18:44] tokenization_utils_base.py:2655 >> Special tokens file saved in saves/Llama-3.2-3B-Instruct/lora/train_llama_3_2_v3/checkpoint-3700/special_tokens_map.json

[INFO|2025-03-19 21:18:59] logging.py:157 >> {'loss': 1.5101, 'learning_rate': 3.0737e-05, 'epoch': 1.28}

[INFO|2025-03-19 21:19:14] logging.py:157 >> {'loss': 1.6443, 'learning_rate': 3.0693e-05, 'epoch': 1.28}

[INFO|2025-03-19 21:19:28] logging.py:157 >> {'loss': 1.7490, 'learning_rate': 3.0649e-05, 'epoch': 1.28}

[INFO|2025-03-19 21:19:43] logging.py:157 >> {'loss': 1.5354, 'learning_rate': 3.0605e-05, 'epoch': 1.28}

[INFO|2025-03-19 21:19:57] logging.py:157 >> {'loss': 1.6307, 'learning_rate': 3.0561e-05, 'epoch': 1.29}

[INFO|2025-03-19 21:20:12] logging.py:157 >> {'loss': 1.5355, 'learning_rate': 3.0516e-05, 'epoch': 1.29}

[INFO|2025-03-19 21:20:28] logging.py:157 >> {'loss': 1.5563, 'learning_rate': 3.0472e-05, 'epoch': 1.29}

[INFO|2025-03-19 21:20:44] logging.py:157 >> {'loss': 1.7514, 'learning_rate': 3.0428e-05, 'epoch': 1.29}

[INFO|2025-03-19 21:20:58] logging.py:157 >> {'loss': 1.7233, 'learning_rate': 3.0384e-05, 'epoch': 1.29}

[INFO|2025-03-19 21:21:13] logging.py:157 >> {'loss': 1.6305, 'learning_rate': 3.0340e-05, 'epoch': 1.29}

[INFO|2025-03-19 21:21:26] logging.py:157 >> {'loss': 1.4687, 'learning_rate': 3.0296e-05, 'epoch': 1.30}

[INFO|2025-03-19 21:21:42] logging.py:157 >> {'loss': 1.5322, 'learning_rate': 3.0252e-05, 'epoch': 1.30}

[INFO|2025-03-19 21:21:57] logging.py:157 >> {'loss': 1.5623, 'learning_rate': 3.0208e-05, 'epoch': 1.30}

[INFO|2025-03-19 21:22:12] logging.py:157 >> {'loss': 1.5408, 'learning_rate': 3.0163e-05, 'epoch': 1.30}

[INFO|2025-03-19 21:22:28] logging.py:157 >> {'loss': 1.7226, 'learning_rate': 3.0119e-05, 'epoch': 1.30}

[INFO|2025-03-19 21:22:42] logging.py:157 >> {'loss': 1.6527, 'learning_rate': 3.0075e-05, 'epoch': 1.30}

[INFO|2025-03-19 21:22:57] logging.py:157 >> {'loss': 1.3560, 'learning_rate': 3.0031e-05, 'epoch': 1.31}

[INFO|2025-03-19 21:23:13] logging.py:157 >> {'loss': 1.5758, 'learning_rate': 2.9986e-05, 'epoch': 1.31}

[INFO|2025-03-19 21:23:26] logging.py:157 >> {'loss': 1.7612, 'learning_rate': 2.9942e-05, 'epoch': 1.31}

[INFO|2025-03-19 21:23:40] logging.py:157 >> {'loss': 1.4870, 'learning_rate': 2.9898e-05, 'epoch': 1.31}

[INFO|2025-03-19 21:23:40] trainer.py:4117 >> 
***** Running Evaluation *****

[INFO|2025-03-19 21:23:40] trainer.py:4119 >>   Num examples = 5152

[INFO|2025-03-19 21:23:40] trainer.py:4122 >>   Batch size = 2

[INFO|2025-03-19 21:28:42] trainer.py:3801 >> Saving model checkpoint to saves/Llama-3.2-3B-Instruct/lora/train_llama_3_2_v3/checkpoint-3800

[INFO|2025-03-19 21:28:42] tokenization_utils_base.py:2646 >> tokenizer config file saved in saves/Llama-3.2-3B-Instruct/lora/train_llama_3_2_v3/checkpoint-3800/tokenizer_config.json

[INFO|2025-03-19 21:28:42] tokenization_utils_base.py:2655 >> Special tokens file saved in saves/Llama-3.2-3B-Instruct/lora/train_llama_3_2_v3/checkpoint-3800/special_tokens_map.json

[INFO|2025-03-19 21:28:56] logging.py:157 >> {'loss': 1.6571, 'learning_rate': 2.9853e-05, 'epoch': 1.31}

[INFO|2025-03-19 21:29:12] logging.py:157 >> {'loss': 1.7139, 'learning_rate': 2.9809e-05, 'epoch': 1.31}

[INFO|2025-03-19 21:29:28] logging.py:157 >> {'loss': 1.4831, 'learning_rate': 2.9765e-05, 'epoch': 1.32}

[INFO|2025-03-19 21:29:43] logging.py:157 >> {'loss': 1.5488, 'learning_rate': 2.9720e-05, 'epoch': 1.32}

[INFO|2025-03-19 21:29:58] logging.py:157 >> {'loss': 1.6824, 'learning_rate': 2.9676e-05, 'epoch': 1.32}

[INFO|2025-03-19 21:30:13] logging.py:157 >> {'loss': 1.4995, 'learning_rate': 2.9632e-05, 'epoch': 1.32}

[INFO|2025-03-19 21:30:29] logging.py:157 >> {'loss': 1.5666, 'learning_rate': 2.9587e-05, 'epoch': 1.32}

[INFO|2025-03-19 21:30:44] logging.py:157 >> {'loss': 1.4783, 'learning_rate': 2.9543e-05, 'epoch': 1.33}

[INFO|2025-03-19 21:30:58] logging.py:157 >> {'loss': 1.6805, 'learning_rate': 2.9498e-05, 'epoch': 1.33}

[INFO|2025-03-19 21:31:13] logging.py:157 >> {'loss': 1.4921, 'learning_rate': 2.9454e-05, 'epoch': 1.33}

[INFO|2025-03-19 21:31:27] logging.py:157 >> {'loss': 1.4782, 'learning_rate': 2.9409e-05, 'epoch': 1.33}

[INFO|2025-03-19 21:31:41] logging.py:157 >> {'loss': 1.6302, 'learning_rate': 2.9365e-05, 'epoch': 1.33}

[INFO|2025-03-19 21:31:55] logging.py:157 >> {'loss': 1.7448, 'learning_rate': 2.9320e-05, 'epoch': 1.33}

[INFO|2025-03-19 21:32:11] logging.py:157 >> {'loss': 1.7057, 'learning_rate': 2.9276e-05, 'epoch': 1.34}

[INFO|2025-03-19 21:32:27] logging.py:157 >> {'loss': 1.5604, 'learning_rate': 2.9231e-05, 'epoch': 1.34}

[INFO|2025-03-19 21:32:42] logging.py:157 >> {'loss': 1.6855, 'learning_rate': 2.9187e-05, 'epoch': 1.34}

[INFO|2025-03-19 21:32:56] logging.py:157 >> {'loss': 1.4608, 'learning_rate': 2.9142e-05, 'epoch': 1.34}

[INFO|2025-03-19 21:33:12] logging.py:157 >> {'loss': 1.5470, 'learning_rate': 2.9098e-05, 'epoch': 1.34}

[INFO|2025-03-19 21:33:27] logging.py:157 >> {'loss': 1.5400, 'learning_rate': 2.9053e-05, 'epoch': 1.34}

[INFO|2025-03-19 21:33:42] logging.py:157 >> {'loss': 1.7095, 'learning_rate': 2.9009e-05, 'epoch': 1.35}

[INFO|2025-03-19 21:33:42] trainer.py:4117 >> 
***** Running Evaluation *****

[INFO|2025-03-19 21:33:42] trainer.py:4119 >>   Num examples = 5152

[INFO|2025-03-19 21:33:42] trainer.py:4122 >>   Batch size = 2

[INFO|2025-03-19 21:38:44] trainer.py:3801 >> Saving model checkpoint to saves/Llama-3.2-3B-Instruct/lora/train_llama_3_2_v3/checkpoint-3900

[INFO|2025-03-19 21:38:44] tokenization_utils_base.py:2646 >> tokenizer config file saved in saves/Llama-3.2-3B-Instruct/lora/train_llama_3_2_v3/checkpoint-3900/tokenizer_config.json

[INFO|2025-03-19 21:38:44] tokenization_utils_base.py:2655 >> Special tokens file saved in saves/Llama-3.2-3B-Instruct/lora/train_llama_3_2_v3/checkpoint-3900/special_tokens_map.json

[INFO|2025-03-19 21:39:00] logging.py:157 >> {'loss': 1.5174, 'learning_rate': 2.8964e-05, 'epoch': 1.35}

[INFO|2025-03-19 21:39:15] logging.py:157 >> {'loss': 1.5891, 'learning_rate': 2.8919e-05, 'epoch': 1.35}

[INFO|2025-03-19 21:39:31] logging.py:157 >> {'loss': 1.5278, 'learning_rate': 2.8875e-05, 'epoch': 1.35}

[INFO|2025-03-19 21:39:45] logging.py:157 >> {'loss': 1.6493, 'learning_rate': 2.8830e-05, 'epoch': 1.35}

[INFO|2025-03-19 21:40:00] logging.py:157 >> {'loss': 1.7780, 'learning_rate': 2.8785e-05, 'epoch': 1.35}

[INFO|2025-03-19 21:40:15] logging.py:157 >> {'loss': 1.4341, 'learning_rate': 2.8741e-05, 'epoch': 1.36}

[INFO|2025-03-19 21:40:29] logging.py:157 >> {'loss': 1.5343, 'learning_rate': 2.8696e-05, 'epoch': 1.36}

[INFO|2025-03-19 21:40:44] logging.py:157 >> {'loss': 1.5372, 'learning_rate': 2.8651e-05, 'epoch': 1.36}

[INFO|2025-03-19 21:40:59] logging.py:157 >> {'loss': 1.5789, 'learning_rate': 2.8607e-05, 'epoch': 1.36}

[INFO|2025-03-19 21:41:13] logging.py:157 >> {'loss': 1.5288, 'learning_rate': 2.8562e-05, 'epoch': 1.36}

[INFO|2025-03-19 21:41:27] logging.py:157 >> {'loss': 1.6924, 'learning_rate': 2.8517e-05, 'epoch': 1.36}

[INFO|2025-03-19 21:41:43] logging.py:157 >> {'loss': 1.4513, 'learning_rate': 2.8472e-05, 'epoch': 1.37}

[INFO|2025-03-19 21:41:57] logging.py:157 >> {'loss': 1.6280, 'learning_rate': 2.8428e-05, 'epoch': 1.37}

[INFO|2025-03-19 21:42:12] logging.py:157 >> {'loss': 1.6345, 'learning_rate': 2.8383e-05, 'epoch': 1.37}

[INFO|2025-03-19 21:42:27] logging.py:157 >> {'loss': 1.6602, 'learning_rate': 2.8338e-05, 'epoch': 1.37}

[INFO|2025-03-19 21:42:42] logging.py:157 >> {'loss': 1.7069, 'learning_rate': 2.8293e-05, 'epoch': 1.37}

[INFO|2025-03-19 21:42:56] logging.py:157 >> {'loss': 1.7904, 'learning_rate': 2.8249e-05, 'epoch': 1.38}

[INFO|2025-03-19 21:43:10] logging.py:157 >> {'loss': 1.3779, 'learning_rate': 2.8204e-05, 'epoch': 1.38}

[INFO|2025-03-19 21:43:26] logging.py:157 >> {'loss': 1.4361, 'learning_rate': 2.8159e-05, 'epoch': 1.38}

[INFO|2025-03-19 21:43:41] logging.py:157 >> {'loss': 1.6268, 'learning_rate': 2.8114e-05, 'epoch': 1.38}

[INFO|2025-03-19 21:43:41] trainer.py:4117 >> 
***** Running Evaluation *****

[INFO|2025-03-19 21:43:41] trainer.py:4119 >>   Num examples = 5152

[INFO|2025-03-19 21:43:41] trainer.py:4122 >>   Batch size = 2

[INFO|2025-03-19 21:48:43] trainer.py:3801 >> Saving model checkpoint to saves/Llama-3.2-3B-Instruct/lora/train_llama_3_2_v3/checkpoint-4000

[INFO|2025-03-19 21:48:43] tokenization_utils_base.py:2646 >> tokenizer config file saved in saves/Llama-3.2-3B-Instruct/lora/train_llama_3_2_v3/checkpoint-4000/tokenizer_config.json

[INFO|2025-03-19 21:48:43] tokenization_utils_base.py:2655 >> Special tokens file saved in saves/Llama-3.2-3B-Instruct/lora/train_llama_3_2_v3/checkpoint-4000/special_tokens_map.json

[INFO|2025-03-19 21:48:59] logging.py:157 >> {'loss': 1.4895, 'learning_rate': 2.8069e-05, 'epoch': 1.38}

[INFO|2025-03-19 21:49:14] logging.py:157 >> {'loss': 1.3747, 'learning_rate': 2.8024e-05, 'epoch': 1.38}

[INFO|2025-03-19 21:49:28] logging.py:157 >> {'loss': 1.7088, 'learning_rate': 2.7980e-05, 'epoch': 1.39}

[INFO|2025-03-19 21:49:42] logging.py:157 >> {'loss': 1.5349, 'learning_rate': 2.7935e-05, 'epoch': 1.39}

[INFO|2025-03-19 21:49:56] logging.py:157 >> {'loss': 1.6052, 'learning_rate': 2.7890e-05, 'epoch': 1.39}

[INFO|2025-03-19 21:50:10] logging.py:157 >> {'loss': 1.5674, 'learning_rate': 2.7845e-05, 'epoch': 1.39}

[INFO|2025-03-19 21:50:26] logging.py:157 >> {'loss': 1.5367, 'learning_rate': 2.7800e-05, 'epoch': 1.39}

[INFO|2025-03-19 21:50:42] logging.py:157 >> {'loss': 1.6455, 'learning_rate': 2.7755e-05, 'epoch': 1.39}

[INFO|2025-03-19 21:50:56] logging.py:157 >> {'loss': 1.4673, 'learning_rate': 2.7710e-05, 'epoch': 1.40}

[INFO|2025-03-19 21:51:11] logging.py:157 >> {'loss': 1.8353, 'learning_rate': 2.7665e-05, 'epoch': 1.40}

[INFO|2025-03-19 21:51:25] logging.py:157 >> {'loss': 1.6284, 'learning_rate': 2.7620e-05, 'epoch': 1.40}

[INFO|2025-03-19 21:51:38] logging.py:157 >> {'loss': 1.4190, 'learning_rate': 2.7575e-05, 'epoch': 1.40}

[INFO|2025-03-19 21:51:53] logging.py:157 >> {'loss': 1.6019, 'learning_rate': 2.7531e-05, 'epoch': 1.40}

[INFO|2025-03-19 21:52:07] logging.py:157 >> {'loss': 1.5257, 'learning_rate': 2.7486e-05, 'epoch': 1.40}

[INFO|2025-03-19 21:52:23] logging.py:157 >> {'loss': 1.7099, 'learning_rate': 2.7441e-05, 'epoch': 1.41}

[INFO|2025-03-19 21:52:37] logging.py:157 >> {'loss': 1.5619, 'learning_rate': 2.7396e-05, 'epoch': 1.41}

[INFO|2025-03-19 21:52:52] logging.py:157 >> {'loss': 1.6232, 'learning_rate': 2.7351e-05, 'epoch': 1.41}

[INFO|2025-03-19 21:53:06] logging.py:157 >> {'loss': 1.4343, 'learning_rate': 2.7306e-05, 'epoch': 1.41}

[INFO|2025-03-19 21:53:22] logging.py:157 >> {'loss': 1.6581, 'learning_rate': 2.7261e-05, 'epoch': 1.41}

[INFO|2025-03-19 21:53:37] logging.py:157 >> {'loss': 1.5578, 'learning_rate': 2.7216e-05, 'epoch': 1.42}

[INFO|2025-03-19 21:53:37] trainer.py:4117 >> 
***** Running Evaluation *****

[INFO|2025-03-19 21:53:37] trainer.py:4119 >>   Num examples = 5152

[INFO|2025-03-19 21:53:37] trainer.py:4122 >>   Batch size = 2

[INFO|2025-03-19 21:58:40] trainer.py:3801 >> Saving model checkpoint to saves/Llama-3.2-3B-Instruct/lora/train_llama_3_2_v3/checkpoint-4100

[INFO|2025-03-19 21:58:40] tokenization_utils_base.py:2646 >> tokenizer config file saved in saves/Llama-3.2-3B-Instruct/lora/train_llama_3_2_v3/checkpoint-4100/tokenizer_config.json

[INFO|2025-03-19 21:58:40] tokenization_utils_base.py:2655 >> Special tokens file saved in saves/Llama-3.2-3B-Instruct/lora/train_llama_3_2_v3/checkpoint-4100/special_tokens_map.json

[INFO|2025-03-19 21:58:56] logging.py:157 >> {'loss': 1.4315, 'learning_rate': 2.7171e-05, 'epoch': 1.42}

[INFO|2025-03-19 21:59:11] logging.py:157 >> {'loss': 1.4681, 'learning_rate': 2.7126e-05, 'epoch': 1.42}

[INFO|2025-03-19 21:59:26] logging.py:157 >> {'loss': 1.6477, 'learning_rate': 2.7081e-05, 'epoch': 1.42}

[INFO|2025-03-19 21:59:39] logging.py:157 >> {'loss': 1.4782, 'learning_rate': 2.7036e-05, 'epoch': 1.42}

[INFO|2025-03-19 21:59:55] logging.py:157 >> {'loss': 1.5575, 'learning_rate': 2.6991e-05, 'epoch': 1.42}

[INFO|2025-03-19 22:00:10] logging.py:157 >> {'loss': 1.6112, 'learning_rate': 2.6945e-05, 'epoch': 1.43}

[INFO|2025-03-19 22:00:25] logging.py:157 >> {'loss': 1.5086, 'learning_rate': 2.6900e-05, 'epoch': 1.43}

[INFO|2025-03-19 22:00:39] logging.py:157 >> {'loss': 1.4094, 'learning_rate': 2.6855e-05, 'epoch': 1.43}

[INFO|2025-03-19 22:00:56] logging.py:157 >> {'loss': 1.8440, 'learning_rate': 2.6810e-05, 'epoch': 1.43}

[INFO|2025-03-19 22:01:09] logging.py:157 >> {'loss': 1.6021, 'learning_rate': 2.6765e-05, 'epoch': 1.43}

[INFO|2025-03-19 22:01:23] logging.py:157 >> {'loss': 1.5554, 'learning_rate': 2.6720e-05, 'epoch': 1.43}

[INFO|2025-03-19 22:01:37] logging.py:157 >> {'loss': 1.5730, 'learning_rate': 2.6675e-05, 'epoch': 1.44}

[INFO|2025-03-19 22:01:52] logging.py:157 >> {'loss': 1.6161, 'learning_rate': 2.6630e-05, 'epoch': 1.44}

[INFO|2025-03-19 22:02:07] logging.py:157 >> {'loss': 1.5982, 'learning_rate': 2.6585e-05, 'epoch': 1.44}

[INFO|2025-03-19 22:02:22] logging.py:157 >> {'loss': 1.5799, 'learning_rate': 2.6540e-05, 'epoch': 1.44}

[INFO|2025-03-19 22:02:36] logging.py:157 >> {'loss': 1.5046, 'learning_rate': 2.6495e-05, 'epoch': 1.44}

[INFO|2025-03-19 22:02:51] logging.py:157 >> {'loss': 1.4790, 'learning_rate': 2.6450e-05, 'epoch': 1.44}

[INFO|2025-03-19 22:03:08] logging.py:157 >> {'loss': 1.6851, 'learning_rate': 2.6405e-05, 'epoch': 1.45}

[INFO|2025-03-19 22:03:22] logging.py:157 >> {'loss': 1.6525, 'learning_rate': 2.6359e-05, 'epoch': 1.45}

[INFO|2025-03-19 22:03:37] logging.py:157 >> {'loss': 1.6169, 'learning_rate': 2.6314e-05, 'epoch': 1.45}

[INFO|2025-03-19 22:03:37] trainer.py:4117 >> 
***** Running Evaluation *****

[INFO|2025-03-19 22:03:37] trainer.py:4119 >>   Num examples = 5152

[INFO|2025-03-19 22:03:37] trainer.py:4122 >>   Batch size = 2

[INFO|2025-03-19 22:08:39] trainer.py:3801 >> Saving model checkpoint to saves/Llama-3.2-3B-Instruct/lora/train_llama_3_2_v3/checkpoint-4200

[INFO|2025-03-19 22:08:40] tokenization_utils_base.py:2646 >> tokenizer config file saved in saves/Llama-3.2-3B-Instruct/lora/train_llama_3_2_v3/checkpoint-4200/tokenizer_config.json

[INFO|2025-03-19 22:08:40] tokenization_utils_base.py:2655 >> Special tokens file saved in saves/Llama-3.2-3B-Instruct/lora/train_llama_3_2_v3/checkpoint-4200/special_tokens_map.json

[INFO|2025-03-19 22:08:56] logging.py:157 >> {'loss': 1.6106, 'learning_rate': 2.6269e-05, 'epoch': 1.45}

[INFO|2025-03-19 22:09:10] logging.py:157 >> {'loss': 1.6049, 'learning_rate': 2.6224e-05, 'epoch': 1.45}

[INFO|2025-03-19 22:09:24] logging.py:157 >> {'loss': 1.6913, 'learning_rate': 2.6179e-05, 'epoch': 1.45}

[INFO|2025-03-19 22:09:39] logging.py:157 >> {'loss': 1.8001, 'learning_rate': 2.6134e-05, 'epoch': 1.46}

[INFO|2025-03-19 22:09:54] logging.py:157 >> {'loss': 1.5623, 'learning_rate': 2.6089e-05, 'epoch': 1.46}

[INFO|2025-03-19 22:10:10] logging.py:157 >> {'loss': 1.5785, 'learning_rate': 2.6043e-05, 'epoch': 1.46}

[INFO|2025-03-19 22:10:24] logging.py:157 >> {'loss': 1.6677, 'learning_rate': 2.5998e-05, 'epoch': 1.46}

[INFO|2025-03-19 22:10:40] logging.py:157 >> {'loss': 1.6234, 'learning_rate': 2.5953e-05, 'epoch': 1.46}

[INFO|2025-03-19 22:10:56] logging.py:157 >> {'loss': 1.5872, 'learning_rate': 2.5908e-05, 'epoch': 1.47}

[INFO|2025-03-19 22:11:10] logging.py:157 >> {'loss': 1.4980, 'learning_rate': 2.5863e-05, 'epoch': 1.47}

[INFO|2025-03-19 22:11:25] logging.py:157 >> {'loss': 1.6297, 'learning_rate': 2.5818e-05, 'epoch': 1.47}

[INFO|2025-03-19 22:11:41] logging.py:157 >> {'loss': 1.6213, 'learning_rate': 2.5773e-05, 'epoch': 1.47}

[INFO|2025-03-19 22:11:56] logging.py:157 >> {'loss': 1.6611, 'learning_rate': 2.5727e-05, 'epoch': 1.47}

[INFO|2025-03-19 22:12:11] logging.py:157 >> {'loss': 1.6242, 'learning_rate': 2.5682e-05, 'epoch': 1.47}

[INFO|2025-03-19 22:12:26] logging.py:157 >> {'loss': 1.7035, 'learning_rate': 2.5637e-05, 'epoch': 1.48}

[INFO|2025-03-19 22:12:40] logging.py:157 >> {'loss': 1.6301, 'learning_rate': 2.5592e-05, 'epoch': 1.48}

[INFO|2025-03-19 22:12:53] logging.py:157 >> {'loss': 1.6739, 'learning_rate': 2.5547e-05, 'epoch': 1.48}

[INFO|2025-03-19 22:13:08] logging.py:157 >> {'loss': 1.5641, 'learning_rate': 2.5502e-05, 'epoch': 1.48}

[INFO|2025-03-19 22:13:23] logging.py:157 >> {'loss': 1.6560, 'learning_rate': 2.5456e-05, 'epoch': 1.48}

[INFO|2025-03-19 22:13:40] logging.py:157 >> {'loss': 1.6035, 'learning_rate': 2.5411e-05, 'epoch': 1.48}

[INFO|2025-03-19 22:13:40] trainer.py:4117 >> 
***** Running Evaluation *****

[INFO|2025-03-19 22:13:40] trainer.py:4119 >>   Num examples = 5152

[INFO|2025-03-19 22:13:40] trainer.py:4122 >>   Batch size = 2

[INFO|2025-03-19 22:18:43] trainer.py:3801 >> Saving model checkpoint to saves/Llama-3.2-3B-Instruct/lora/train_llama_3_2_v3/checkpoint-4300

[INFO|2025-03-19 22:18:43] tokenization_utils_base.py:2646 >> tokenizer config file saved in saves/Llama-3.2-3B-Instruct/lora/train_llama_3_2_v3/checkpoint-4300/tokenizer_config.json

[INFO|2025-03-19 22:18:43] tokenization_utils_base.py:2655 >> Special tokens file saved in saves/Llama-3.2-3B-Instruct/lora/train_llama_3_2_v3/checkpoint-4300/special_tokens_map.json

[INFO|2025-03-19 22:18:57] logging.py:157 >> {'loss': 1.5835, 'learning_rate': 2.5366e-05, 'epoch': 1.49}

[INFO|2025-03-19 22:19:12] logging.py:157 >> {'loss': 1.3598, 'learning_rate': 2.5321e-05, 'epoch': 1.49}

[INFO|2025-03-19 22:19:27] logging.py:157 >> {'loss': 1.4669, 'learning_rate': 2.5276e-05, 'epoch': 1.49}

[INFO|2025-03-19 22:19:42] logging.py:157 >> {'loss': 1.6554, 'learning_rate': 2.5230e-05, 'epoch': 1.49}

[INFO|2025-03-19 22:19:57] logging.py:157 >> {'loss': 1.6183, 'learning_rate': 2.5185e-05, 'epoch': 1.49}

[INFO|2025-03-19 22:20:11] logging.py:157 >> {'loss': 1.6294, 'learning_rate': 2.5140e-05, 'epoch': 1.49}

[INFO|2025-03-19 22:20:26] logging.py:157 >> {'loss': 1.6407, 'learning_rate': 2.5095e-05, 'epoch': 1.50}

[INFO|2025-03-19 22:20:41] logging.py:157 >> {'loss': 1.3848, 'learning_rate': 2.5050e-05, 'epoch': 1.50}

[INFO|2025-03-19 22:20:55] logging.py:157 >> {'loss': 1.6543, 'learning_rate': 2.5005e-05, 'epoch': 1.50}

[INFO|2025-03-19 22:21:10] logging.py:157 >> {'loss': 1.5556, 'learning_rate': 2.4959e-05, 'epoch': 1.50}

[INFO|2025-03-19 22:21:24] logging.py:157 >> {'loss': 1.6478, 'learning_rate': 2.4914e-05, 'epoch': 1.50}

[INFO|2025-03-19 22:21:40] logging.py:157 >> {'loss': 1.7343, 'learning_rate': 2.4869e-05, 'epoch': 1.50}

[INFO|2025-03-19 22:21:55] logging.py:157 >> {'loss': 1.5500, 'learning_rate': 2.4824e-05, 'epoch': 1.51}

[INFO|2025-03-19 22:22:11] logging.py:157 >> {'loss': 1.5387, 'learning_rate': 2.4779e-05, 'epoch': 1.51}

[INFO|2025-03-19 22:22:26] logging.py:157 >> {'loss': 1.6335, 'learning_rate': 2.4733e-05, 'epoch': 1.51}

[INFO|2025-03-19 22:22:39] logging.py:157 >> {'loss': 1.6190, 'learning_rate': 2.4688e-05, 'epoch': 1.51}

[INFO|2025-03-19 22:22:54] logging.py:157 >> {'loss': 1.8241, 'learning_rate': 2.4643e-05, 'epoch': 1.51}

[INFO|2025-03-19 22:23:10] logging.py:157 >> {'loss': 1.4704, 'learning_rate': 2.4598e-05, 'epoch': 1.52}

[INFO|2025-03-19 22:23:25] logging.py:157 >> {'loss': 1.7106, 'learning_rate': 2.4553e-05, 'epoch': 1.52}

[INFO|2025-03-19 22:23:38] logging.py:157 >> {'loss': 1.5906, 'learning_rate': 2.4508e-05, 'epoch': 1.52}

[INFO|2025-03-19 22:23:38] trainer.py:4117 >> 
***** Running Evaluation *****

[INFO|2025-03-19 22:23:38] trainer.py:4119 >>   Num examples = 5152

[INFO|2025-03-19 22:23:38] trainer.py:4122 >>   Batch size = 2

[INFO|2025-03-19 22:28:41] trainer.py:3801 >> Saving model checkpoint to saves/Llama-3.2-3B-Instruct/lora/train_llama_3_2_v3/checkpoint-4400

[INFO|2025-03-19 22:28:41] tokenization_utils_base.py:2646 >> tokenizer config file saved in saves/Llama-3.2-3B-Instruct/lora/train_llama_3_2_v3/checkpoint-4400/tokenizer_config.json

[INFO|2025-03-19 22:28:41] tokenization_utils_base.py:2655 >> Special tokens file saved in saves/Llama-3.2-3B-Instruct/lora/train_llama_3_2_v3/checkpoint-4400/special_tokens_map.json

[INFO|2025-03-19 22:28:56] logging.py:157 >> {'loss': 1.6952, 'learning_rate': 2.4462e-05, 'epoch': 1.52}

[INFO|2025-03-19 22:29:10] logging.py:157 >> {'loss': 1.6130, 'learning_rate': 2.4417e-05, 'epoch': 1.52}

[INFO|2025-03-19 22:29:25] logging.py:157 >> {'loss': 1.5232, 'learning_rate': 2.4372e-05, 'epoch': 1.52}

[INFO|2025-03-19 22:29:39] logging.py:157 >> {'loss': 1.6271, 'learning_rate': 2.4327e-05, 'epoch': 1.53}

[INFO|2025-03-19 22:29:54] logging.py:157 >> {'loss': 1.6823, 'learning_rate': 2.4282e-05, 'epoch': 1.53}

[INFO|2025-03-19 22:30:08] logging.py:157 >> {'loss': 1.6422, 'learning_rate': 2.4236e-05, 'epoch': 1.53}

[INFO|2025-03-19 22:30:22] logging.py:157 >> {'loss': 1.7892, 'learning_rate': 2.4191e-05, 'epoch': 1.53}

[INFO|2025-03-19 22:30:36] logging.py:157 >> {'loss': 1.6334, 'learning_rate': 2.4146e-05, 'epoch': 1.53}

[INFO|2025-03-19 22:30:52] logging.py:157 >> {'loss': 1.5241, 'learning_rate': 2.4101e-05, 'epoch': 1.53}

[INFO|2025-03-19 22:31:05] logging.py:157 >> {'loss': 1.8001, 'learning_rate': 2.4056e-05, 'epoch': 1.54}

[INFO|2025-03-19 22:31:19] logging.py:157 >> {'loss': 1.6390, 'learning_rate': 2.4011e-05, 'epoch': 1.54}

[INFO|2025-03-19 22:31:34] logging.py:157 >> {'loss': 1.4352, 'learning_rate': 2.3966e-05, 'epoch': 1.54}

[INFO|2025-03-19 22:31:48] logging.py:157 >> {'loss': 1.7281, 'learning_rate': 2.3920e-05, 'epoch': 1.54}

[INFO|2025-03-19 22:32:03] logging.py:157 >> {'loss': 1.5333, 'learning_rate': 2.3875e-05, 'epoch': 1.54}

[INFO|2025-03-19 22:32:18] logging.py:157 >> {'loss': 1.5145, 'learning_rate': 2.3830e-05, 'epoch': 1.54}

[INFO|2025-03-19 22:32:32] logging.py:157 >> {'loss': 1.5271, 'learning_rate': 2.3785e-05, 'epoch': 1.55}

[INFO|2025-03-19 22:32:48] logging.py:157 >> {'loss': 1.5743, 'learning_rate': 2.3740e-05, 'epoch': 1.55}

[INFO|2025-03-19 22:33:04] logging.py:157 >> {'loss': 1.7567, 'learning_rate': 2.3695e-05, 'epoch': 1.55}

[INFO|2025-03-19 22:33:19] logging.py:157 >> {'loss': 1.6999, 'learning_rate': 2.3650e-05, 'epoch': 1.55}

[INFO|2025-03-19 22:33:33] logging.py:157 >> {'loss': 1.6043, 'learning_rate': 2.3605e-05, 'epoch': 1.55}

[INFO|2025-03-19 22:33:33] trainer.py:4117 >> 
***** Running Evaluation *****

[INFO|2025-03-19 22:33:33] trainer.py:4119 >>   Num examples = 5152

[INFO|2025-03-19 22:33:33] trainer.py:4122 >>   Batch size = 2

[INFO|2025-03-19 22:38:36] trainer.py:3801 >> Saving model checkpoint to saves/Llama-3.2-3B-Instruct/lora/train_llama_3_2_v3/checkpoint-4500

[INFO|2025-03-19 22:38:36] tokenization_utils_base.py:2646 >> tokenizer config file saved in saves/Llama-3.2-3B-Instruct/lora/train_llama_3_2_v3/checkpoint-4500/tokenizer_config.json

[INFO|2025-03-19 22:38:36] tokenization_utils_base.py:2655 >> Special tokens file saved in saves/Llama-3.2-3B-Instruct/lora/train_llama_3_2_v3/checkpoint-4500/special_tokens_map.json

[INFO|2025-03-19 22:38:52] logging.py:157 >> {'loss': 1.7165, 'learning_rate': 2.3559e-05, 'epoch': 1.55}

[INFO|2025-03-19 22:39:08] logging.py:157 >> {'loss': 1.4922, 'learning_rate': 2.3514e-05, 'epoch': 1.56}

[INFO|2025-03-19 22:39:22] logging.py:157 >> {'loss': 1.5641, 'learning_rate': 2.3469e-05, 'epoch': 1.56}

[INFO|2025-03-19 22:39:36] logging.py:157 >> {'loss': 1.5996, 'learning_rate': 2.3424e-05, 'epoch': 1.56}

[INFO|2025-03-19 22:39:49] logging.py:157 >> {'loss': 1.5926, 'learning_rate': 2.3379e-05, 'epoch': 1.56}

[INFO|2025-03-19 22:40:04] logging.py:157 >> {'loss': 1.6674, 'learning_rate': 2.3334e-05, 'epoch': 1.56}

[INFO|2025-03-19 22:40:17] logging.py:157 >> {'loss': 1.5451, 'learning_rate': 2.3289e-05, 'epoch': 1.57}

[INFO|2025-03-19 22:40:32] logging.py:157 >> {'loss': 1.5137, 'learning_rate': 2.3244e-05, 'epoch': 1.57}

[INFO|2025-03-19 22:40:48] logging.py:157 >> {'loss': 1.5101, 'learning_rate': 2.3199e-05, 'epoch': 1.57}

[INFO|2025-03-19 22:41:02] logging.py:157 >> {'loss': 1.7445, 'learning_rate': 2.3154e-05, 'epoch': 1.57}

[INFO|2025-03-19 22:41:16] logging.py:157 >> {'loss': 1.4767, 'learning_rate': 2.3109e-05, 'epoch': 1.57}

[INFO|2025-03-19 22:41:32] logging.py:157 >> {'loss': 1.5893, 'learning_rate': 2.3064e-05, 'epoch': 1.57}

[INFO|2025-03-19 22:41:47] logging.py:157 >> {'loss': 1.6746, 'learning_rate': 2.3018e-05, 'epoch': 1.58}

[INFO|2025-03-19 22:42:02] logging.py:157 >> {'loss': 1.5350, 'learning_rate': 2.2973e-05, 'epoch': 1.58}

[INFO|2025-03-19 22:42:20] logging.py:157 >> {'loss': 1.7309, 'learning_rate': 2.2928e-05, 'epoch': 1.58}

[INFO|2025-03-19 22:42:35] logging.py:157 >> {'loss': 1.4671, 'learning_rate': 2.2883e-05, 'epoch': 1.58}

[INFO|2025-03-19 22:42:50] logging.py:157 >> {'loss': 1.5991, 'learning_rate': 2.2838e-05, 'epoch': 1.58}

[INFO|2025-03-19 22:43:04] logging.py:157 >> {'loss': 1.4322, 'learning_rate': 2.2793e-05, 'epoch': 1.58}

[INFO|2025-03-19 22:43:18] logging.py:157 >> {'loss': 1.5494, 'learning_rate': 2.2748e-05, 'epoch': 1.59}

[INFO|2025-03-19 22:43:32] logging.py:157 >> {'loss': 1.5725, 'learning_rate': 2.2703e-05, 'epoch': 1.59}

[INFO|2025-03-19 22:43:32] trainer.py:4117 >> 
***** Running Evaluation *****

[INFO|2025-03-19 22:43:32] trainer.py:4119 >>   Num examples = 5152

[INFO|2025-03-19 22:43:32] trainer.py:4122 >>   Batch size = 2

[INFO|2025-03-19 22:48:34] trainer.py:3801 >> Saving model checkpoint to saves/Llama-3.2-3B-Instruct/lora/train_llama_3_2_v3/checkpoint-4600

[INFO|2025-03-19 22:48:34] tokenization_utils_base.py:2646 >> tokenizer config file saved in saves/Llama-3.2-3B-Instruct/lora/train_llama_3_2_v3/checkpoint-4600/tokenizer_config.json

[INFO|2025-03-19 22:48:34] tokenization_utils_base.py:2655 >> Special tokens file saved in saves/Llama-3.2-3B-Instruct/lora/train_llama_3_2_v3/checkpoint-4600/special_tokens_map.json

[INFO|2025-03-19 22:48:49] logging.py:157 >> {'loss': 1.6844, 'learning_rate': 2.2658e-05, 'epoch': 1.59}

[INFO|2025-03-19 22:49:07] logging.py:157 >> {'loss': 1.5499, 'learning_rate': 2.2613e-05, 'epoch': 1.59}

[INFO|2025-03-19 22:49:22] logging.py:157 >> {'loss': 1.3190, 'learning_rate': 2.2568e-05, 'epoch': 1.59}

[INFO|2025-03-19 22:49:35] logging.py:157 >> {'loss': 1.6526, 'learning_rate': 2.2523e-05, 'epoch': 1.59}

[INFO|2025-03-19 22:49:50] logging.py:157 >> {'loss': 1.6511, 'learning_rate': 2.2478e-05, 'epoch': 1.60}

[INFO|2025-03-19 22:50:05] logging.py:157 >> {'loss': 1.4597, 'learning_rate': 2.2434e-05, 'epoch': 1.60}

[INFO|2025-03-19 22:50:21] logging.py:157 >> {'loss': 1.4277, 'learning_rate': 2.2389e-05, 'epoch': 1.60}

[INFO|2025-03-19 22:50:37] logging.py:157 >> {'loss': 1.6106, 'learning_rate': 2.2344e-05, 'epoch': 1.60}

[INFO|2025-03-19 22:50:51] logging.py:157 >> {'loss': 1.6317, 'learning_rate': 2.2299e-05, 'epoch': 1.60}

[INFO|2025-03-19 22:51:06] logging.py:157 >> {'loss': 1.7425, 'learning_rate': 2.2254e-05, 'epoch': 1.60}

[INFO|2025-03-19 22:51:21] logging.py:157 >> {'loss': 1.6648, 'learning_rate': 2.2209e-05, 'epoch': 1.61}

[INFO|2025-03-19 22:51:36] logging.py:157 >> {'loss': 1.5851, 'learning_rate': 2.2164e-05, 'epoch': 1.61}

[INFO|2025-03-19 22:51:50] logging.py:157 >> {'loss': 1.6085, 'learning_rate': 2.2119e-05, 'epoch': 1.61}

[INFO|2025-03-19 22:52:05] logging.py:157 >> {'loss': 1.5516, 'learning_rate': 2.2074e-05, 'epoch': 1.61}

[INFO|2025-03-19 22:52:21] logging.py:157 >> {'loss': 1.6560, 'learning_rate': 2.2029e-05, 'epoch': 1.61}

[INFO|2025-03-19 22:52:35] logging.py:157 >> {'loss': 1.6932, 'learning_rate': 2.1985e-05, 'epoch': 1.62}

[INFO|2025-03-19 22:52:48] logging.py:157 >> {'loss': 1.5364, 'learning_rate': 2.1940e-05, 'epoch': 1.62}

[INFO|2025-03-19 22:53:03] logging.py:157 >> {'loss': 1.6513, 'learning_rate': 2.1895e-05, 'epoch': 1.62}

[INFO|2025-03-19 22:53:17] logging.py:157 >> {'loss': 1.7621, 'learning_rate': 2.1850e-05, 'epoch': 1.62}

[INFO|2025-03-19 22:53:33] logging.py:157 >> {'loss': 1.5248, 'learning_rate': 2.1805e-05, 'epoch': 1.62}

[INFO|2025-03-19 22:53:33] trainer.py:4117 >> 
***** Running Evaluation *****

[INFO|2025-03-19 22:53:33] trainer.py:4119 >>   Num examples = 5152

[INFO|2025-03-19 22:53:33] trainer.py:4122 >>   Batch size = 2

[INFO|2025-03-19 22:58:35] trainer.py:3801 >> Saving model checkpoint to saves/Llama-3.2-3B-Instruct/lora/train_llama_3_2_v3/checkpoint-4700

[INFO|2025-03-19 22:58:35] tokenization_utils_base.py:2646 >> tokenizer config file saved in saves/Llama-3.2-3B-Instruct/lora/train_llama_3_2_v3/checkpoint-4700/tokenizer_config.json

[INFO|2025-03-19 22:58:35] tokenization_utils_base.py:2655 >> Special tokens file saved in saves/Llama-3.2-3B-Instruct/lora/train_llama_3_2_v3/checkpoint-4700/special_tokens_map.json

[INFO|2025-03-19 22:58:50] logging.py:157 >> {'loss': 1.4587, 'learning_rate': 2.1760e-05, 'epoch': 1.62}

[INFO|2025-03-19 22:59:03] logging.py:157 >> {'loss': 1.5620, 'learning_rate': 2.1716e-05, 'epoch': 1.63}

[INFO|2025-03-19 22:59:17] logging.py:157 >> {'loss': 1.6419, 'learning_rate': 2.1671e-05, 'epoch': 1.63}

[INFO|2025-03-19 22:59:31] logging.py:157 >> {'loss': 1.5150, 'learning_rate': 2.1626e-05, 'epoch': 1.63}

[INFO|2025-03-19 22:59:45] logging.py:157 >> {'loss': 1.5382, 'learning_rate': 2.1581e-05, 'epoch': 1.63}

[INFO|2025-03-19 22:59:59] logging.py:157 >> {'loss': 1.6605, 'learning_rate': 2.1536e-05, 'epoch': 1.63}

[INFO|2025-03-19 23:00:14] logging.py:157 >> {'loss': 1.5807, 'learning_rate': 2.1492e-05, 'epoch': 1.63}

[INFO|2025-03-19 23:00:29] logging.py:157 >> {'loss': 1.4693, 'learning_rate': 2.1447e-05, 'epoch': 1.64}

[INFO|2025-03-19 23:00:42] logging.py:157 >> {'loss': 1.6447, 'learning_rate': 2.1402e-05, 'epoch': 1.64}

[INFO|2025-03-19 23:00:58] logging.py:157 >> {'loss': 1.6602, 'learning_rate': 2.1358e-05, 'epoch': 1.64}

[INFO|2025-03-19 23:01:12] logging.py:157 >> {'loss': 1.6683, 'learning_rate': 2.1313e-05, 'epoch': 1.64}

[INFO|2025-03-19 23:01:28] logging.py:157 >> {'loss': 1.7128, 'learning_rate': 2.1268e-05, 'epoch': 1.64}

[INFO|2025-03-19 23:01:43] logging.py:157 >> {'loss': 1.4841, 'learning_rate': 2.1224e-05, 'epoch': 1.64}

[INFO|2025-03-19 23:01:57] logging.py:157 >> {'loss': 1.5661, 'learning_rate': 2.1179e-05, 'epoch': 1.65}

[INFO|2025-03-19 23:02:14] logging.py:157 >> {'loss': 1.6832, 'learning_rate': 2.1134e-05, 'epoch': 1.65}

[INFO|2025-03-19 23:02:29] logging.py:157 >> {'loss': 1.6255, 'learning_rate': 2.1090e-05, 'epoch': 1.65}

[INFO|2025-03-19 23:02:44] logging.py:157 >> {'loss': 1.6331, 'learning_rate': 2.1045e-05, 'epoch': 1.65}

[INFO|2025-03-19 23:03:00] logging.py:157 >> {'loss': 1.5364, 'learning_rate': 2.1000e-05, 'epoch': 1.65}

[INFO|2025-03-19 23:03:17] logging.py:157 >> {'loss': 1.5208, 'learning_rate': 2.0956e-05, 'epoch': 1.65}

[INFO|2025-03-19 23:03:31] logging.py:157 >> {'loss': 1.5631, 'learning_rate': 2.0911e-05, 'epoch': 1.66}

[INFO|2025-03-19 23:03:31] trainer.py:4117 >> 
***** Running Evaluation *****

[INFO|2025-03-19 23:03:31] trainer.py:4119 >>   Num examples = 5152

[INFO|2025-03-19 23:03:31] trainer.py:4122 >>   Batch size = 2

[INFO|2025-03-19 23:08:34] trainer.py:3801 >> Saving model checkpoint to saves/Llama-3.2-3B-Instruct/lora/train_llama_3_2_v3/checkpoint-4800

[INFO|2025-03-19 23:08:34] tokenization_utils_base.py:2646 >> tokenizer config file saved in saves/Llama-3.2-3B-Instruct/lora/train_llama_3_2_v3/checkpoint-4800/tokenizer_config.json

[INFO|2025-03-19 23:08:34] tokenization_utils_base.py:2655 >> Special tokens file saved in saves/Llama-3.2-3B-Instruct/lora/train_llama_3_2_v3/checkpoint-4800/special_tokens_map.json

[INFO|2025-03-19 23:08:48] logging.py:157 >> {'loss': 1.6779, 'learning_rate': 2.0867e-05, 'epoch': 1.66}

[INFO|2025-03-19 23:09:03] logging.py:157 >> {'loss': 1.6235, 'learning_rate': 2.0822e-05, 'epoch': 1.66}

[INFO|2025-03-19 23:09:17] logging.py:157 >> {'loss': 1.5400, 'learning_rate': 2.0778e-05, 'epoch': 1.66}

[INFO|2025-03-19 23:09:31] logging.py:157 >> {'loss': 1.5550, 'learning_rate': 2.0733e-05, 'epoch': 1.66}

[INFO|2025-03-19 23:09:45] logging.py:157 >> {'loss': 1.4143, 'learning_rate': 2.0688e-05, 'epoch': 1.67}

[INFO|2025-03-19 23:10:00] logging.py:157 >> {'loss': 1.4494, 'learning_rate': 2.0644e-05, 'epoch': 1.67}

[INFO|2025-03-19 23:10:16] logging.py:157 >> {'loss': 1.6786, 'learning_rate': 2.0599e-05, 'epoch': 1.67}

[INFO|2025-03-19 23:10:32] logging.py:157 >> {'loss': 1.5767, 'learning_rate': 2.0555e-05, 'epoch': 1.67}

[INFO|2025-03-19 23:10:47] logging.py:157 >> {'loss': 1.6063, 'learning_rate': 2.0511e-05, 'epoch': 1.67}

[INFO|2025-03-19 23:11:02] logging.py:157 >> {'loss': 1.7939, 'learning_rate': 2.0466e-05, 'epoch': 1.67}

[INFO|2025-03-19 23:11:18] logging.py:157 >> {'loss': 1.6076, 'learning_rate': 2.0422e-05, 'epoch': 1.68}

[INFO|2025-03-19 23:11:33] logging.py:157 >> {'loss': 1.6637, 'learning_rate': 2.0377e-05, 'epoch': 1.68}

[INFO|2025-03-19 23:11:49] logging.py:157 >> {'loss': 1.4911, 'learning_rate': 2.0333e-05, 'epoch': 1.68}

[INFO|2025-03-19 23:12:06] logging.py:157 >> {'loss': 1.4803, 'learning_rate': 2.0288e-05, 'epoch': 1.68}

[INFO|2025-03-19 23:12:21] logging.py:157 >> {'loss': 1.5552, 'learning_rate': 2.0244e-05, 'epoch': 1.68}

[INFO|2025-03-19 23:12:37] logging.py:157 >> {'loss': 1.5839, 'learning_rate': 2.0200e-05, 'epoch': 1.68}

[INFO|2025-03-19 23:12:52] logging.py:157 >> {'loss': 1.5595, 'learning_rate': 2.0155e-05, 'epoch': 1.69}

[INFO|2025-03-19 23:13:06] logging.py:157 >> {'loss': 1.4406, 'learning_rate': 2.0111e-05, 'epoch': 1.69}

[INFO|2025-03-19 23:13:20] logging.py:157 >> {'loss': 1.4581, 'learning_rate': 2.0067e-05, 'epoch': 1.69}

[INFO|2025-03-19 23:13:35] logging.py:157 >> {'loss': 1.3422, 'learning_rate': 2.0023e-05, 'epoch': 1.69}

[INFO|2025-03-19 23:13:35] trainer.py:4117 >> 
***** Running Evaluation *****

[INFO|2025-03-19 23:13:35] trainer.py:4119 >>   Num examples = 5152

[INFO|2025-03-19 23:13:35] trainer.py:4122 >>   Batch size = 2

[INFO|2025-03-19 23:18:38] trainer.py:3801 >> Saving model checkpoint to saves/Llama-3.2-3B-Instruct/lora/train_llama_3_2_v3/checkpoint-4900

[INFO|2025-03-19 23:18:38] tokenization_utils_base.py:2646 >> tokenizer config file saved in saves/Llama-3.2-3B-Instruct/lora/train_llama_3_2_v3/checkpoint-4900/tokenizer_config.json

[INFO|2025-03-19 23:18:38] tokenization_utils_base.py:2655 >> Special tokens file saved in saves/Llama-3.2-3B-Instruct/lora/train_llama_3_2_v3/checkpoint-4900/special_tokens_map.json

[INFO|2025-03-19 23:18:53] logging.py:157 >> {'loss': 1.6199, 'learning_rate': 1.9978e-05, 'epoch': 1.69}

[INFO|2025-03-19 23:19:09] logging.py:157 >> {'loss': 1.4618, 'learning_rate': 1.9934e-05, 'epoch': 1.69}

[INFO|2025-03-19 23:19:23] logging.py:157 >> {'loss': 1.4210, 'learning_rate': 1.9890e-05, 'epoch': 1.70}

[INFO|2025-03-19 23:19:38] logging.py:157 >> {'loss': 1.5439, 'learning_rate': 1.9846e-05, 'epoch': 1.70}

[INFO|2025-03-19 23:19:53] logging.py:157 >> {'loss': 1.5895, 'learning_rate': 1.9801e-05, 'epoch': 1.70}

[INFO|2025-03-19 23:20:08] logging.py:157 >> {'loss': 1.5493, 'learning_rate': 1.9757e-05, 'epoch': 1.70}

[INFO|2025-03-19 23:20:23] logging.py:157 >> {'loss': 1.4848, 'learning_rate': 1.9713e-05, 'epoch': 1.70}

[INFO|2025-03-19 23:20:38] logging.py:157 >> {'loss': 1.6864, 'learning_rate': 1.9669e-05, 'epoch': 1.70}

[INFO|2025-03-19 23:20:53] logging.py:157 >> {'loss': 1.5397, 'learning_rate': 1.9625e-05, 'epoch': 1.71}

[INFO|2025-03-19 23:21:07] logging.py:157 >> {'loss': 1.5402, 'learning_rate': 1.9581e-05, 'epoch': 1.71}

[INFO|2025-03-19 23:21:21] logging.py:157 >> {'loss': 1.5474, 'learning_rate': 1.9536e-05, 'epoch': 1.71}

[INFO|2025-03-19 23:21:37] logging.py:157 >> {'loss': 1.7291, 'learning_rate': 1.9492e-05, 'epoch': 1.71}

[INFO|2025-03-19 23:21:53] logging.py:157 >> {'loss': 1.6061, 'learning_rate': 1.9448e-05, 'epoch': 1.71}

[INFO|2025-03-19 23:22:09] logging.py:157 >> {'loss': 1.5232, 'learning_rate': 1.9404e-05, 'epoch': 1.72}

[INFO|2025-03-19 23:22:22] logging.py:157 >> {'loss': 1.4085, 'learning_rate': 1.9360e-05, 'epoch': 1.72}

[INFO|2025-03-19 23:22:37] logging.py:157 >> {'loss': 1.6112, 'learning_rate': 1.9316e-05, 'epoch': 1.72}

[INFO|2025-03-19 23:22:51] logging.py:157 >> {'loss': 1.4714, 'learning_rate': 1.9272e-05, 'epoch': 1.72}

[INFO|2025-03-19 23:23:05] logging.py:157 >> {'loss': 1.5794, 'learning_rate': 1.9228e-05, 'epoch': 1.72}

[INFO|2025-03-19 23:23:21] logging.py:157 >> {'loss': 1.3401, 'learning_rate': 1.9184e-05, 'epoch': 1.72}

[INFO|2025-03-19 23:23:36] logging.py:157 >> {'loss': 1.6494, 'learning_rate': 1.9140e-05, 'epoch': 1.73}

[INFO|2025-03-19 23:23:36] trainer.py:4117 >> 
***** Running Evaluation *****

[INFO|2025-03-19 23:23:36] trainer.py:4119 >>   Num examples = 5152

[INFO|2025-03-19 23:23:36] trainer.py:4122 >>   Batch size = 2

[INFO|2025-03-19 23:28:39] trainer.py:3801 >> Saving model checkpoint to saves/Llama-3.2-3B-Instruct/lora/train_llama_3_2_v3/checkpoint-5000

[INFO|2025-03-19 23:28:39] tokenization_utils_base.py:2646 >> tokenizer config file saved in saves/Llama-3.2-3B-Instruct/lora/train_llama_3_2_v3/checkpoint-5000/tokenizer_config.json

[INFO|2025-03-19 23:28:39] tokenization_utils_base.py:2655 >> Special tokens file saved in saves/Llama-3.2-3B-Instruct/lora/train_llama_3_2_v3/checkpoint-5000/special_tokens_map.json

[INFO|2025-03-19 23:28:54] logging.py:157 >> {'loss': 1.5537, 'learning_rate': 1.9096e-05, 'epoch': 1.73}

[INFO|2025-03-19 23:29:09] logging.py:157 >> {'loss': 1.5513, 'learning_rate': 1.9053e-05, 'epoch': 1.73}

[INFO|2025-03-19 23:29:22] logging.py:157 >> {'loss': 1.5017, 'learning_rate': 1.9009e-05, 'epoch': 1.73}

[INFO|2025-03-19 23:29:39] logging.py:157 >> {'loss': 1.5809, 'learning_rate': 1.8965e-05, 'epoch': 1.73}

[INFO|2025-03-19 23:29:55] logging.py:157 >> {'loss': 1.5080, 'learning_rate': 1.8921e-05, 'epoch': 1.73}

[INFO|2025-03-19 23:30:09] logging.py:157 >> {'loss': 1.5438, 'learning_rate': 1.8877e-05, 'epoch': 1.74}

[INFO|2025-03-19 23:30:24] logging.py:157 >> {'loss': 1.5712, 'learning_rate': 1.8833e-05, 'epoch': 1.74}

[INFO|2025-03-19 23:30:39] logging.py:157 >> {'loss': 1.5853, 'learning_rate': 1.8790e-05, 'epoch': 1.74}

[INFO|2025-03-19 23:30:53] logging.py:157 >> {'loss': 1.5206, 'learning_rate': 1.8746e-05, 'epoch': 1.74}

[INFO|2025-03-19 23:31:08] logging.py:157 >> {'loss': 1.5501, 'learning_rate': 1.8702e-05, 'epoch': 1.74}

[INFO|2025-03-19 23:31:24] logging.py:157 >> {'loss': 1.5398, 'learning_rate': 1.8658e-05, 'epoch': 1.74}

[INFO|2025-03-19 23:31:39] logging.py:157 >> {'loss': 1.6691, 'learning_rate': 1.8615e-05, 'epoch': 1.75}

[INFO|2025-03-19 23:31:53] logging.py:157 >> {'loss': 1.5563, 'learning_rate': 1.8571e-05, 'epoch': 1.75}

[INFO|2025-03-19 23:32:08] logging.py:157 >> {'loss': 1.5760, 'learning_rate': 1.8527e-05, 'epoch': 1.75}

[INFO|2025-03-19 23:32:24] logging.py:157 >> {'loss': 1.5960, 'learning_rate': 1.8484e-05, 'epoch': 1.75}

[INFO|2025-03-19 23:32:40] logging.py:157 >> {'loss': 1.6654, 'learning_rate': 1.8440e-05, 'epoch': 1.75}

[INFO|2025-03-19 23:32:54] logging.py:157 >> {'loss': 1.6658, 'learning_rate': 1.8397e-05, 'epoch': 1.75}

[INFO|2025-03-19 23:33:09] logging.py:157 >> {'loss': 1.6899, 'learning_rate': 1.8353e-05, 'epoch': 1.76}

[INFO|2025-03-19 23:33:24] logging.py:157 >> {'loss': 1.5537, 'learning_rate': 1.8309e-05, 'epoch': 1.76}

[INFO|2025-03-19 23:33:39] logging.py:157 >> {'loss': 1.7179, 'learning_rate': 1.8266e-05, 'epoch': 1.76}

[INFO|2025-03-19 23:33:39] trainer.py:4117 >> 
***** Running Evaluation *****

[INFO|2025-03-19 23:33:39] trainer.py:4119 >>   Num examples = 5152

[INFO|2025-03-19 23:33:39] trainer.py:4122 >>   Batch size = 2

[INFO|2025-03-19 23:38:41] trainer.py:3801 >> Saving model checkpoint to saves/Llama-3.2-3B-Instruct/lora/train_llama_3_2_v3/checkpoint-5100

[INFO|2025-03-19 23:38:42] tokenization_utils_base.py:2646 >> tokenizer config file saved in saves/Llama-3.2-3B-Instruct/lora/train_llama_3_2_v3/checkpoint-5100/tokenizer_config.json

[INFO|2025-03-19 23:38:42] tokenization_utils_base.py:2655 >> Special tokens file saved in saves/Llama-3.2-3B-Instruct/lora/train_llama_3_2_v3/checkpoint-5100/special_tokens_map.json

[INFO|2025-03-19 23:38:57] logging.py:157 >> {'loss': 1.5693, 'learning_rate': 1.8222e-05, 'epoch': 1.76}

[INFO|2025-03-19 23:39:10] logging.py:157 >> {'loss': 1.6393, 'learning_rate': 1.8179e-05, 'epoch': 1.76}

[INFO|2025-03-19 23:39:25] logging.py:157 >> {'loss': 1.5266, 'learning_rate': 1.8135e-05, 'epoch': 1.77}

[INFO|2025-03-19 23:39:40] logging.py:157 >> {'loss': 1.6732, 'learning_rate': 1.8092e-05, 'epoch': 1.77}

[INFO|2025-03-19 23:39:55] logging.py:157 >> {'loss': 1.6770, 'learning_rate': 1.8049e-05, 'epoch': 1.77}

[INFO|2025-03-19 23:40:10] logging.py:157 >> {'loss': 1.6006, 'learning_rate': 1.8005e-05, 'epoch': 1.77}

[INFO|2025-03-19 23:40:25] logging.py:157 >> {'loss': 1.7213, 'learning_rate': 1.7962e-05, 'epoch': 1.77}

[INFO|2025-03-19 23:40:40] logging.py:157 >> {'loss': 1.4524, 'learning_rate': 1.7918e-05, 'epoch': 1.77}

[INFO|2025-03-19 23:40:55] logging.py:157 >> {'loss': 1.7883, 'learning_rate': 1.7875e-05, 'epoch': 1.78}

[INFO|2025-03-19 23:41:09] logging.py:157 >> {'loss': 1.6021, 'learning_rate': 1.7832e-05, 'epoch': 1.78}

[INFO|2025-03-19 23:41:23] logging.py:157 >> {'loss': 1.6690, 'learning_rate': 1.7789e-05, 'epoch': 1.78}

[INFO|2025-03-19 23:41:38] logging.py:157 >> {'loss': 1.6747, 'learning_rate': 1.7745e-05, 'epoch': 1.78}

[INFO|2025-03-19 23:41:51] logging.py:157 >> {'loss': 1.6800, 'learning_rate': 1.7702e-05, 'epoch': 1.78}

[INFO|2025-03-19 23:42:05] logging.py:157 >> {'loss': 1.4474, 'learning_rate': 1.7659e-05, 'epoch': 1.78}

[INFO|2025-03-19 23:42:18] logging.py:157 >> {'loss': 1.5827, 'learning_rate': 1.7616e-05, 'epoch': 1.79}

[INFO|2025-03-19 23:42:33] logging.py:157 >> {'loss': 1.7162, 'learning_rate': 1.7573e-05, 'epoch': 1.79}

[INFO|2025-03-19 23:42:48] logging.py:157 >> {'loss': 1.7603, 'learning_rate': 1.7529e-05, 'epoch': 1.79}

[INFO|2025-03-19 23:43:02] logging.py:157 >> {'loss': 1.7398, 'learning_rate': 1.7486e-05, 'epoch': 1.79}

[INFO|2025-03-19 23:43:17] logging.py:157 >> {'loss': 1.6979, 'learning_rate': 1.7443e-05, 'epoch': 1.79}

[INFO|2025-03-19 23:43:32] logging.py:157 >> {'loss': 1.6765, 'learning_rate': 1.7400e-05, 'epoch': 1.79}

[INFO|2025-03-19 23:43:32] trainer.py:4117 >> 
***** Running Evaluation *****

[INFO|2025-03-19 23:43:32] trainer.py:4119 >>   Num examples = 5152

[INFO|2025-03-19 23:43:32] trainer.py:4122 >>   Batch size = 2

[INFO|2025-03-19 23:48:34] trainer.py:3801 >> Saving model checkpoint to saves/Llama-3.2-3B-Instruct/lora/train_llama_3_2_v3/checkpoint-5200

[INFO|2025-03-19 23:48:34] tokenization_utils_base.py:2646 >> tokenizer config file saved in saves/Llama-3.2-3B-Instruct/lora/train_llama_3_2_v3/checkpoint-5200/tokenizer_config.json

[INFO|2025-03-19 23:48:34] tokenization_utils_base.py:2655 >> Special tokens file saved in saves/Llama-3.2-3B-Instruct/lora/train_llama_3_2_v3/checkpoint-5200/special_tokens_map.json

[INFO|2025-03-19 23:48:52] logging.py:157 >> {'loss': 1.7017, 'learning_rate': 1.7357e-05, 'epoch': 1.80}

[INFO|2025-03-19 23:49:06] logging.py:157 >> {'loss': 1.6367, 'learning_rate': 1.7314e-05, 'epoch': 1.80}

[INFO|2025-03-19 23:49:22] logging.py:157 >> {'loss': 1.4940, 'learning_rate': 1.7271e-05, 'epoch': 1.80}

[INFO|2025-03-19 23:49:37] logging.py:157 >> {'loss': 1.4853, 'learning_rate': 1.7228e-05, 'epoch': 1.80}

[INFO|2025-03-19 23:49:52] logging.py:157 >> {'loss': 1.5978, 'learning_rate': 1.7185e-05, 'epoch': 1.80}

[INFO|2025-03-19 23:50:08] logging.py:157 >> {'loss': 1.4449, 'learning_rate': 1.7142e-05, 'epoch': 1.81}

[INFO|2025-03-19 23:50:21] logging.py:157 >> {'loss': 1.5634, 'learning_rate': 1.7099e-05, 'epoch': 1.81}

[INFO|2025-03-19 23:50:36] logging.py:157 >> {'loss': 1.7108, 'learning_rate': 1.7057e-05, 'epoch': 1.81}

[INFO|2025-03-19 23:50:51] logging.py:157 >> {'loss': 1.5851, 'learning_rate': 1.7014e-05, 'epoch': 1.81}

[INFO|2025-03-19 23:51:07] logging.py:157 >> {'loss': 1.5847, 'learning_rate': 1.6971e-05, 'epoch': 1.81}

[INFO|2025-03-19 23:51:22] logging.py:157 >> {'loss': 1.6970, 'learning_rate': 1.6928e-05, 'epoch': 1.81}

[INFO|2025-03-19 23:51:37] logging.py:157 >> {'loss': 1.4408, 'learning_rate': 1.6885e-05, 'epoch': 1.82}

[INFO|2025-03-19 23:51:51] logging.py:157 >> {'loss': 1.4580, 'learning_rate': 1.6843e-05, 'epoch': 1.82}

[INFO|2025-03-19 23:52:06] logging.py:157 >> {'loss': 1.6636, 'learning_rate': 1.6800e-05, 'epoch': 1.82}

[INFO|2025-03-19 23:52:21] logging.py:157 >> {'loss': 1.4486, 'learning_rate': 1.6757e-05, 'epoch': 1.82}

[INFO|2025-03-19 23:52:36] logging.py:157 >> {'loss': 1.5637, 'learning_rate': 1.6715e-05, 'epoch': 1.82}

[INFO|2025-03-19 23:52:50] logging.py:157 >> {'loss': 1.5229, 'learning_rate': 1.6672e-05, 'epoch': 1.82}

[INFO|2025-03-19 23:53:05] logging.py:157 >> {'loss': 1.4714, 'learning_rate': 1.6629e-05, 'epoch': 1.83}

[INFO|2025-03-19 23:53:20] logging.py:157 >> {'loss': 1.6004, 'learning_rate': 1.6587e-05, 'epoch': 1.83}

[INFO|2025-03-19 23:53:35] logging.py:157 >> {'loss': 1.6113, 'learning_rate': 1.6544e-05, 'epoch': 1.83}

[INFO|2025-03-19 23:53:35] trainer.py:4117 >> 
***** Running Evaluation *****

[INFO|2025-03-19 23:53:35] trainer.py:4119 >>   Num examples = 5152

[INFO|2025-03-19 23:53:35] trainer.py:4122 >>   Batch size = 2

[INFO|2025-03-19 23:58:37] trainer.py:3801 >> Saving model checkpoint to saves/Llama-3.2-3B-Instruct/lora/train_llama_3_2_v3/checkpoint-5300

[INFO|2025-03-19 23:58:37] tokenization_utils_base.py:2646 >> tokenizer config file saved in saves/Llama-3.2-3B-Instruct/lora/train_llama_3_2_v3/checkpoint-5300/tokenizer_config.json

[INFO|2025-03-19 23:58:37] tokenization_utils_base.py:2655 >> Special tokens file saved in saves/Llama-3.2-3B-Instruct/lora/train_llama_3_2_v3/checkpoint-5300/special_tokens_map.json

[INFO|2025-03-19 23:58:52] logging.py:157 >> {'loss': 1.6502, 'learning_rate': 1.6502e-05, 'epoch': 1.83}

[INFO|2025-03-19 23:59:07] logging.py:157 >> {'loss': 1.4004, 'learning_rate': 1.6459e-05, 'epoch': 1.83}

[INFO|2025-03-19 23:59:23] logging.py:157 >> {'loss': 1.4761, 'learning_rate': 1.6417e-05, 'epoch': 1.83}

[INFO|2025-03-19 23:59:37] logging.py:157 >> {'loss': 1.3688, 'learning_rate': 1.6375e-05, 'epoch': 1.84}

[INFO|2025-03-19 23:59:51] logging.py:157 >> {'loss': 1.6384, 'learning_rate': 1.6332e-05, 'epoch': 1.84}

[INFO|2025-03-20 00:00:06] logging.py:157 >> {'loss': 1.4626, 'learning_rate': 1.6290e-05, 'epoch': 1.84}

[INFO|2025-03-20 00:00:20] logging.py:157 >> {'loss': 1.6656, 'learning_rate': 1.6247e-05, 'epoch': 1.84}

[INFO|2025-03-20 00:00:35] logging.py:157 >> {'loss': 1.5867, 'learning_rate': 1.6205e-05, 'epoch': 1.84}

[INFO|2025-03-20 00:00:50] logging.py:157 >> {'loss': 1.6817, 'learning_rate': 1.6163e-05, 'epoch': 1.84}

[INFO|2025-03-20 00:01:05] logging.py:157 >> {'loss': 1.4657, 'learning_rate': 1.6121e-05, 'epoch': 1.85}

[INFO|2025-03-20 00:01:21] logging.py:157 >> {'loss': 1.5683, 'learning_rate': 1.6078e-05, 'epoch': 1.85}

[INFO|2025-03-20 00:01:35] logging.py:157 >> {'loss': 1.4067, 'learning_rate': 1.6036e-05, 'epoch': 1.85}

[INFO|2025-03-20 00:01:50] logging.py:157 >> {'loss': 1.5182, 'learning_rate': 1.5994e-05, 'epoch': 1.85}

[INFO|2025-03-20 00:02:05] logging.py:157 >> {'loss': 1.3746, 'learning_rate': 1.5952e-05, 'epoch': 1.85}

[INFO|2025-03-20 00:02:20] logging.py:157 >> {'loss': 1.6049, 'learning_rate': 1.5910e-05, 'epoch': 1.86}

[INFO|2025-03-20 00:02:35] logging.py:157 >> {'loss': 1.6796, 'learning_rate': 1.5868e-05, 'epoch': 1.86}

[INFO|2025-03-20 00:02:50] logging.py:157 >> {'loss': 1.5805, 'learning_rate': 1.5826e-05, 'epoch': 1.86}

[INFO|2025-03-20 00:03:04] logging.py:157 >> {'loss': 1.7086, 'learning_rate': 1.5784e-05, 'epoch': 1.86}

[INFO|2025-03-20 00:03:19] logging.py:157 >> {'loss': 1.5143, 'learning_rate': 1.5742e-05, 'epoch': 1.86}

[INFO|2025-03-20 00:03:33] logging.py:157 >> {'loss': 1.6190, 'learning_rate': 1.5700e-05, 'epoch': 1.86}

[INFO|2025-03-20 00:03:33] trainer.py:4117 >> 
***** Running Evaluation *****

[INFO|2025-03-20 00:03:33] trainer.py:4119 >>   Num examples = 5152

[INFO|2025-03-20 00:03:33] trainer.py:4122 >>   Batch size = 2

[INFO|2025-03-20 00:08:36] trainer.py:3801 >> Saving model checkpoint to saves/Llama-3.2-3B-Instruct/lora/train_llama_3_2_v3/checkpoint-5400

[INFO|2025-03-20 00:08:36] tokenization_utils_base.py:2646 >> tokenizer config file saved in saves/Llama-3.2-3B-Instruct/lora/train_llama_3_2_v3/checkpoint-5400/tokenizer_config.json

[INFO|2025-03-20 00:08:36] tokenization_utils_base.py:2655 >> Special tokens file saved in saves/Llama-3.2-3B-Instruct/lora/train_llama_3_2_v3/checkpoint-5400/special_tokens_map.json

[INFO|2025-03-20 00:08:52] logging.py:157 >> {'loss': 1.5411, 'learning_rate': 1.5658e-05, 'epoch': 1.87}

[INFO|2025-03-20 00:09:10] logging.py:157 >> {'loss': 1.6282, 'learning_rate': 1.5616e-05, 'epoch': 1.87}

[INFO|2025-03-20 00:09:23] logging.py:157 >> {'loss': 1.4287, 'learning_rate': 1.5574e-05, 'epoch': 1.87}

[INFO|2025-03-20 00:09:37] logging.py:157 >> {'loss': 1.5350, 'learning_rate': 1.5532e-05, 'epoch': 1.87}

[INFO|2025-03-20 00:09:52] logging.py:157 >> {'loss': 1.6161, 'learning_rate': 1.5490e-05, 'epoch': 1.87}

[INFO|2025-03-20 00:10:08] logging.py:157 >> {'loss': 1.5055, 'learning_rate': 1.5449e-05, 'epoch': 1.87}

[INFO|2025-03-20 00:10:22] logging.py:157 >> {'loss': 1.4418, 'learning_rate': 1.5407e-05, 'epoch': 1.88}

[INFO|2025-03-20 00:10:37] logging.py:157 >> {'loss': 1.4942, 'learning_rate': 1.5365e-05, 'epoch': 1.88}

[INFO|2025-03-20 00:10:51] logging.py:157 >> {'loss': 1.4630, 'learning_rate': 1.5323e-05, 'epoch': 1.88}

[INFO|2025-03-20 00:11:07] logging.py:157 >> {'loss': 1.5791, 'learning_rate': 1.5282e-05, 'epoch': 1.88}

[INFO|2025-03-20 00:11:20] logging.py:157 >> {'loss': 1.4234, 'learning_rate': 1.5240e-05, 'epoch': 1.88}

[INFO|2025-03-20 00:11:36] logging.py:157 >> {'loss': 1.5630, 'learning_rate': 1.5199e-05, 'epoch': 1.88}

[INFO|2025-03-20 00:11:50] logging.py:157 >> {'loss': 1.7527, 'learning_rate': 1.5157e-05, 'epoch': 1.89}

[INFO|2025-03-20 00:12:04] logging.py:157 >> {'loss': 1.7170, 'learning_rate': 1.5116e-05, 'epoch': 1.89}

[INFO|2025-03-20 00:12:19] logging.py:157 >> {'loss': 1.4937, 'learning_rate': 1.5074e-05, 'epoch': 1.89}

[INFO|2025-03-20 00:12:34] logging.py:157 >> {'loss': 1.4229, 'learning_rate': 1.5033e-05, 'epoch': 1.89}

[INFO|2025-03-20 00:12:49] logging.py:157 >> {'loss': 1.6266, 'learning_rate': 1.4991e-05, 'epoch': 1.89}

[INFO|2025-03-20 00:13:04] logging.py:157 >> {'loss': 1.6235, 'learning_rate': 1.4950e-05, 'epoch': 1.89}

[INFO|2025-03-20 00:13:19] logging.py:157 >> {'loss': 1.5106, 'learning_rate': 1.4908e-05, 'epoch': 1.90}

[INFO|2025-03-20 00:13:33] logging.py:157 >> {'loss': 1.5246, 'learning_rate': 1.4867e-05, 'epoch': 1.90}

[INFO|2025-03-20 00:13:33] trainer.py:4117 >> 
***** Running Evaluation *****

[INFO|2025-03-20 00:13:33] trainer.py:4119 >>   Num examples = 5152

[INFO|2025-03-20 00:13:33] trainer.py:4122 >>   Batch size = 2

[INFO|2025-03-20 00:18:38] trainer.py:3801 >> Saving model checkpoint to saves/Llama-3.2-3B-Instruct/lora/train_llama_3_2_v3/checkpoint-5500

[INFO|2025-03-20 00:18:38] tokenization_utils_base.py:2646 >> tokenizer config file saved in saves/Llama-3.2-3B-Instruct/lora/train_llama_3_2_v3/checkpoint-5500/tokenizer_config.json

[INFO|2025-03-20 00:18:38] tokenization_utils_base.py:2655 >> Special tokens file saved in saves/Llama-3.2-3B-Instruct/lora/train_llama_3_2_v3/checkpoint-5500/special_tokens_map.json

[INFO|2025-03-20 00:18:55] logging.py:157 >> {'loss': 1.5513, 'learning_rate': 1.4826e-05, 'epoch': 1.90}

[INFO|2025-03-20 00:19:10] logging.py:157 >> {'loss': 1.5631, 'learning_rate': 1.4785e-05, 'epoch': 1.90}

[INFO|2025-03-20 00:19:25] logging.py:157 >> {'loss': 1.4545, 'learning_rate': 1.4743e-05, 'epoch': 1.90}

[INFO|2025-03-20 00:19:38] logging.py:157 >> {'loss': 1.6733, 'learning_rate': 1.4702e-05, 'epoch': 1.91}

[INFO|2025-03-20 00:19:53] logging.py:157 >> {'loss': 1.6161, 'learning_rate': 1.4661e-05, 'epoch': 1.91}

[INFO|2025-03-20 00:20:07] logging.py:157 >> {'loss': 1.5435, 'learning_rate': 1.4620e-05, 'epoch': 1.91}

[INFO|2025-03-20 00:20:21] logging.py:157 >> {'loss': 1.5690, 'learning_rate': 1.4579e-05, 'epoch': 1.91}

[INFO|2025-03-20 00:20:36] logging.py:157 >> {'loss': 1.5386, 'learning_rate': 1.4538e-05, 'epoch': 1.91}

[INFO|2025-03-20 00:20:51] logging.py:157 >> {'loss': 1.6246, 'learning_rate': 1.4497e-05, 'epoch': 1.91}

[INFO|2025-03-20 00:21:06] logging.py:157 >> {'loss': 1.7033, 'learning_rate': 1.4456e-05, 'epoch': 1.92}

[INFO|2025-03-20 00:21:20] logging.py:157 >> {'loss': 1.5987, 'learning_rate': 1.4415e-05, 'epoch': 1.92}

[INFO|2025-03-20 00:21:37] logging.py:157 >> {'loss': 1.6121, 'learning_rate': 1.4374e-05, 'epoch': 1.92}

[INFO|2025-03-20 00:21:50] logging.py:157 >> {'loss': 1.5625, 'learning_rate': 1.4333e-05, 'epoch': 1.92}

[INFO|2025-03-20 00:22:05] logging.py:157 >> {'loss': 1.5696, 'learning_rate': 1.4292e-05, 'epoch': 1.92}

[INFO|2025-03-20 00:22:20] logging.py:157 >> {'loss': 1.5375, 'learning_rate': 1.4251e-05, 'epoch': 1.92}

[INFO|2025-03-20 00:22:33] logging.py:157 >> {'loss': 1.4787, 'learning_rate': 1.4211e-05, 'epoch': 1.93}

[INFO|2025-03-20 00:22:48] logging.py:157 >> {'loss': 1.6336, 'learning_rate': 1.4170e-05, 'epoch': 1.93}

[INFO|2025-03-20 00:23:03] logging.py:157 >> {'loss': 1.4733, 'learning_rate': 1.4129e-05, 'epoch': 1.93}

[INFO|2025-03-20 00:23:18] logging.py:157 >> {'loss': 1.4804, 'learning_rate': 1.4088e-05, 'epoch': 1.93}

[INFO|2025-03-20 00:23:32] logging.py:157 >> {'loss': 1.4021, 'learning_rate': 1.4048e-05, 'epoch': 1.93}

[INFO|2025-03-20 00:23:32] trainer.py:4117 >> 
***** Running Evaluation *****

[INFO|2025-03-20 00:23:32] trainer.py:4119 >>   Num examples = 5152

[INFO|2025-03-20 00:23:32] trainer.py:4122 >>   Batch size = 2

[INFO|2025-03-20 00:28:35] trainer.py:3801 >> Saving model checkpoint to saves/Llama-3.2-3B-Instruct/lora/train_llama_3_2_v3/checkpoint-5600

[INFO|2025-03-20 00:28:35] tokenization_utils_base.py:2646 >> tokenizer config file saved in saves/Llama-3.2-3B-Instruct/lora/train_llama_3_2_v3/checkpoint-5600/tokenizer_config.json

[INFO|2025-03-20 00:28:35] tokenization_utils_base.py:2655 >> Special tokens file saved in saves/Llama-3.2-3B-Instruct/lora/train_llama_3_2_v3/checkpoint-5600/special_tokens_map.json

[INFO|2025-03-20 00:28:51] logging.py:157 >> {'loss': 1.5480, 'learning_rate': 1.4007e-05, 'epoch': 1.93}

[INFO|2025-03-20 00:29:05] logging.py:157 >> {'loss': 1.4105, 'learning_rate': 1.3967e-05, 'epoch': 1.94}

[INFO|2025-03-20 00:29:20] logging.py:157 >> {'loss': 1.6461, 'learning_rate': 1.3926e-05, 'epoch': 1.94}

[INFO|2025-03-20 00:29:35] logging.py:157 >> {'loss': 1.5583, 'learning_rate': 1.3886e-05, 'epoch': 1.94}

[INFO|2025-03-20 00:29:49] logging.py:157 >> {'loss': 1.5725, 'learning_rate': 1.3845e-05, 'epoch': 1.94}

[INFO|2025-03-20 00:30:03] logging.py:157 >> {'loss': 1.6508, 'learning_rate': 1.3805e-05, 'epoch': 1.94}

[INFO|2025-03-20 00:30:19] logging.py:157 >> {'loss': 1.4518, 'learning_rate': 1.3764e-05, 'epoch': 1.94}

[INFO|2025-03-20 00:30:33] logging.py:157 >> {'loss': 1.5115, 'learning_rate': 1.3724e-05, 'epoch': 1.95}

[INFO|2025-03-20 00:30:48] logging.py:157 >> {'loss': 1.4718, 'learning_rate': 1.3684e-05, 'epoch': 1.95}

[INFO|2025-03-20 00:31:04] logging.py:157 >> {'loss': 1.7192, 'learning_rate': 1.3643e-05, 'epoch': 1.95}

[INFO|2025-03-20 00:31:18] logging.py:157 >> {'loss': 1.5601, 'learning_rate': 1.3603e-05, 'epoch': 1.95}

[INFO|2025-03-20 00:31:32] logging.py:157 >> {'loss': 1.5995, 'learning_rate': 1.3563e-05, 'epoch': 1.95}

[INFO|2025-03-20 00:31:45] logging.py:157 >> {'loss': 1.5884, 'learning_rate': 1.3523e-05, 'epoch': 1.96}

[INFO|2025-03-20 00:31:59] logging.py:157 >> {'loss': 1.3273, 'learning_rate': 1.3483e-05, 'epoch': 1.96}

[INFO|2025-03-20 00:32:14] logging.py:157 >> {'loss': 1.5392, 'learning_rate': 1.3443e-05, 'epoch': 1.96}

[INFO|2025-03-20 00:32:28] logging.py:157 >> {'loss': 1.7643, 'learning_rate': 1.3403e-05, 'epoch': 1.96}

[INFO|2025-03-20 00:32:41] logging.py:157 >> {'loss': 1.3339, 'learning_rate': 1.3363e-05, 'epoch': 1.96}

[INFO|2025-03-20 00:32:57] logging.py:157 >> {'loss': 1.6180, 'learning_rate': 1.3323e-05, 'epoch': 1.96}

[INFO|2025-03-20 00:33:10] logging.py:157 >> {'loss': 1.4599, 'learning_rate': 1.3283e-05, 'epoch': 1.97}

[INFO|2025-03-20 00:33:26] logging.py:157 >> {'loss': 1.6958, 'learning_rate': 1.3243e-05, 'epoch': 1.97}

[INFO|2025-03-20 00:33:26] trainer.py:4117 >> 
***** Running Evaluation *****

[INFO|2025-03-20 00:33:26] trainer.py:4119 >>   Num examples = 5152

[INFO|2025-03-20 00:33:26] trainer.py:4122 >>   Batch size = 2

[INFO|2025-03-20 00:38:29] trainer.py:3801 >> Saving model checkpoint to saves/Llama-3.2-3B-Instruct/lora/train_llama_3_2_v3/checkpoint-5700

[INFO|2025-03-20 00:38:29] tokenization_utils_base.py:2646 >> tokenizer config file saved in saves/Llama-3.2-3B-Instruct/lora/train_llama_3_2_v3/checkpoint-5700/tokenizer_config.json

[INFO|2025-03-20 00:38:29] tokenization_utils_base.py:2655 >> Special tokens file saved in saves/Llama-3.2-3B-Instruct/lora/train_llama_3_2_v3/checkpoint-5700/special_tokens_map.json

[INFO|2025-03-20 00:38:44] logging.py:157 >> {'loss': 1.6825, 'learning_rate': 1.3203e-05, 'epoch': 1.97}

[INFO|2025-03-20 00:38:58] logging.py:157 >> {'loss': 1.4653, 'learning_rate': 1.3163e-05, 'epoch': 1.97}

[INFO|2025-03-20 00:39:12] logging.py:157 >> {'loss': 1.4238, 'learning_rate': 1.3123e-05, 'epoch': 1.97}

[INFO|2025-03-20 00:39:26] logging.py:157 >> {'loss': 1.6132, 'learning_rate': 1.3084e-05, 'epoch': 1.97}

[INFO|2025-03-20 00:39:40] logging.py:157 >> {'loss': 1.6232, 'learning_rate': 1.3044e-05, 'epoch': 1.98}

[INFO|2025-03-20 00:39:56] logging.py:157 >> {'loss': 1.3970, 'learning_rate': 1.3004e-05, 'epoch': 1.98}

[INFO|2025-03-20 00:40:12] logging.py:157 >> {'loss': 1.5828, 'learning_rate': 1.2965e-05, 'epoch': 1.98}

[INFO|2025-03-20 00:40:26] logging.py:157 >> {'loss': 1.3892, 'learning_rate': 1.2925e-05, 'epoch': 1.98}

[INFO|2025-03-20 00:40:40] logging.py:157 >> {'loss': 1.6072, 'learning_rate': 1.2885e-05, 'epoch': 1.98}

[INFO|2025-03-20 00:40:54] logging.py:157 >> {'loss': 1.6093, 'learning_rate': 1.2846e-05, 'epoch': 1.98}

[INFO|2025-03-20 00:41:09] logging.py:157 >> {'loss': 1.6073, 'learning_rate': 1.2806e-05, 'epoch': 1.99}

[INFO|2025-03-20 00:41:22] logging.py:157 >> {'loss': 1.6056, 'learning_rate': 1.2767e-05, 'epoch': 1.99}

[INFO|2025-03-20 00:41:37] logging.py:157 >> {'loss': 1.5603, 'learning_rate': 1.2728e-05, 'epoch': 1.99}

[INFO|2025-03-20 00:41:52] logging.py:157 >> {'loss': 1.4533, 'learning_rate': 1.2688e-05, 'epoch': 1.99}

[INFO|2025-03-20 00:42:07] logging.py:157 >> {'loss': 1.5024, 'learning_rate': 1.2649e-05, 'epoch': 1.99}

[INFO|2025-03-20 00:42:23] logging.py:157 >> {'loss': 1.5502, 'learning_rate': 1.2610e-05, 'epoch': 1.99}

[INFO|2025-03-20 00:42:38] logging.py:157 >> {'loss': 1.4939, 'learning_rate': 1.2571e-05, 'epoch': 2.00}

[INFO|2025-03-20 00:42:53] logging.py:157 >> {'loss': 1.5699, 'learning_rate': 1.2531e-05, 'epoch': 2.00}

[INFO|2025-03-20 00:43:07] logging.py:157 >> {'loss': 1.6030, 'learning_rate': 1.2492e-05, 'epoch': 2.00}

[INFO|2025-03-20 00:43:21] logging.py:157 >> {'loss': 1.2991, 'learning_rate': 1.2453e-05, 'epoch': 2.00}

[INFO|2025-03-20 00:43:21] trainer.py:4117 >> 
***** Running Evaluation *****

[INFO|2025-03-20 00:43:21] trainer.py:4119 >>   Num examples = 5152

[INFO|2025-03-20 00:43:21] trainer.py:4122 >>   Batch size = 2

[INFO|2025-03-20 00:48:24] trainer.py:3801 >> Saving model checkpoint to saves/Llama-3.2-3B-Instruct/lora/train_llama_3_2_v3/checkpoint-5800

[INFO|2025-03-20 00:48:24] tokenization_utils_base.py:2646 >> tokenizer config file saved in saves/Llama-3.2-3B-Instruct/lora/train_llama_3_2_v3/checkpoint-5800/tokenizer_config.json

[INFO|2025-03-20 00:48:24] tokenization_utils_base.py:2655 >> Special tokens file saved in saves/Llama-3.2-3B-Instruct/lora/train_llama_3_2_v3/checkpoint-5800/special_tokens_map.json

[INFO|2025-03-20 00:48:39] logging.py:157 >> {'loss': 1.2056, 'learning_rate': 1.2414e-05, 'epoch': 2.00}

[INFO|2025-03-20 00:48:53] logging.py:157 >> {'loss': 1.3082, 'learning_rate': 1.2375e-05, 'epoch': 2.01}

[INFO|2025-03-20 00:49:08] logging.py:157 >> {'loss': 1.3180, 'learning_rate': 1.2336e-05, 'epoch': 2.01}

[INFO|2025-03-20 00:49:23] logging.py:157 >> {'loss': 1.2964, 'learning_rate': 1.2297e-05, 'epoch': 2.01}

[INFO|2025-03-20 00:49:37] logging.py:157 >> {'loss': 1.2769, 'learning_rate': 1.2258e-05, 'epoch': 2.01}

[INFO|2025-03-20 00:49:52] logging.py:157 >> {'loss': 1.2394, 'learning_rate': 1.2219e-05, 'epoch': 2.01}

[INFO|2025-03-20 00:50:07] logging.py:157 >> {'loss': 1.4214, 'learning_rate': 1.2181e-05, 'epoch': 2.01}

[INFO|2025-03-20 00:50:21] logging.py:157 >> {'loss': 1.3354, 'learning_rate': 1.2142e-05, 'epoch': 2.02}

[INFO|2025-03-20 00:50:36] logging.py:157 >> {'loss': 1.2902, 'learning_rate': 1.2103e-05, 'epoch': 2.02}

[INFO|2025-03-20 00:50:50] logging.py:157 >> {'loss': 1.4419, 'learning_rate': 1.2064e-05, 'epoch': 2.02}

[INFO|2025-03-20 00:51:06] logging.py:157 >> {'loss': 1.4533, 'learning_rate': 1.2026e-05, 'epoch': 2.02}

[INFO|2025-03-20 00:51:21] logging.py:157 >> {'loss': 1.3776, 'learning_rate': 1.1987e-05, 'epoch': 2.02}

[INFO|2025-03-20 00:51:36] logging.py:157 >> {'loss': 1.3226, 'learning_rate': 1.1949e-05, 'epoch': 2.02}

[INFO|2025-03-20 00:51:50] logging.py:157 >> {'loss': 1.3716, 'learning_rate': 1.1910e-05, 'epoch': 2.03}

[INFO|2025-03-20 00:52:06] logging.py:157 >> {'loss': 1.3514, 'learning_rate': 1.1872e-05, 'epoch': 2.03}

[INFO|2025-03-20 00:52:20] logging.py:157 >> {'loss': 1.3363, 'learning_rate': 1.1833e-05, 'epoch': 2.03}

[INFO|2025-03-20 00:52:35] logging.py:157 >> {'loss': 1.4586, 'learning_rate': 1.1795e-05, 'epoch': 2.03}

[INFO|2025-03-20 00:52:51] logging.py:157 >> {'loss': 1.2840, 'learning_rate': 1.1756e-05, 'epoch': 2.03}

[INFO|2025-03-20 00:53:06] logging.py:157 >> {'loss': 1.1747, 'learning_rate': 1.1718e-05, 'epoch': 2.03}

[INFO|2025-03-20 00:53:19] logging.py:157 >> {'loss': 1.3191, 'learning_rate': 1.1680e-05, 'epoch': 2.04}

[INFO|2025-03-20 00:53:19] trainer.py:4117 >> 
***** Running Evaluation *****

[INFO|2025-03-20 00:53:19] trainer.py:4119 >>   Num examples = 5152

[INFO|2025-03-20 00:53:19] trainer.py:4122 >>   Batch size = 2

[INFO|2025-03-20 00:58:22] trainer.py:3801 >> Saving model checkpoint to saves/Llama-3.2-3B-Instruct/lora/train_llama_3_2_v3/checkpoint-5900

[INFO|2025-03-20 00:58:22] tokenization_utils_base.py:2646 >> tokenizer config file saved in saves/Llama-3.2-3B-Instruct/lora/train_llama_3_2_v3/checkpoint-5900/tokenizer_config.json

[INFO|2025-03-20 00:58:22] tokenization_utils_base.py:2655 >> Special tokens file saved in saves/Llama-3.2-3B-Instruct/lora/train_llama_3_2_v3/checkpoint-5900/special_tokens_map.json

[INFO|2025-03-20 00:58:38] logging.py:157 >> {'loss': 1.3192, 'learning_rate': 1.1642e-05, 'epoch': 2.04}

[INFO|2025-03-20 00:58:54] logging.py:157 >> {'loss': 1.2545, 'learning_rate': 1.1603e-05, 'epoch': 2.04}

[INFO|2025-03-20 00:59:08] logging.py:157 >> {'loss': 1.2510, 'learning_rate': 1.1565e-05, 'epoch': 2.04}

[INFO|2025-03-20 00:59:23] logging.py:157 >> {'loss': 1.2729, 'learning_rate': 1.1527e-05, 'epoch': 2.04}

[INFO|2025-03-20 00:59:37] logging.py:157 >> {'loss': 1.3133, 'learning_rate': 1.1489e-05, 'epoch': 2.04}

[INFO|2025-03-20 00:59:53] logging.py:157 >> {'loss': 1.1557, 'learning_rate': 1.1451e-05, 'epoch': 2.05}

[INFO|2025-03-20 01:00:06] logging.py:157 >> {'loss': 1.2544, 'learning_rate': 1.1413e-05, 'epoch': 2.05}

[INFO|2025-03-20 01:00:23] logging.py:157 >> {'loss': 1.1892, 'learning_rate': 1.1375e-05, 'epoch': 2.05}

[INFO|2025-03-20 01:00:37] logging.py:157 >> {'loss': 1.2846, 'learning_rate': 1.1337e-05, 'epoch': 2.05}

[INFO|2025-03-20 01:00:53] logging.py:157 >> {'loss': 1.2655, 'learning_rate': 1.1300e-05, 'epoch': 2.05}

[INFO|2025-03-20 01:01:08] logging.py:157 >> {'loss': 1.4232, 'learning_rate': 1.1262e-05, 'epoch': 2.06}

[INFO|2025-03-20 01:01:23] logging.py:157 >> {'loss': 1.3452, 'learning_rate': 1.1224e-05, 'epoch': 2.06}

[INFO|2025-03-20 01:01:39] logging.py:157 >> {'loss': 1.1950, 'learning_rate': 1.1186e-05, 'epoch': 2.06}

[INFO|2025-03-20 01:01:53] logging.py:157 >> {'loss': 1.2425, 'learning_rate': 1.1149e-05, 'epoch': 2.06}

[INFO|2025-03-20 01:02:08] logging.py:157 >> {'loss': 1.2935, 'learning_rate': 1.1111e-05, 'epoch': 2.06}

[INFO|2025-03-20 01:02:23] logging.py:157 >> {'loss': 1.3557, 'learning_rate': 1.1074e-05, 'epoch': 2.06}

[INFO|2025-03-20 01:02:37] logging.py:157 >> {'loss': 1.3213, 'learning_rate': 1.1036e-05, 'epoch': 2.07}

[INFO|2025-03-20 01:02:51] logging.py:157 >> {'loss': 1.1403, 'learning_rate': 1.0999e-05, 'epoch': 2.07}

[INFO|2025-03-20 01:03:06] logging.py:157 >> {'loss': 1.2678, 'learning_rate': 1.0961e-05, 'epoch': 2.07}

[INFO|2025-03-20 01:03:20] logging.py:157 >> {'loss': 1.4244, 'learning_rate': 1.0924e-05, 'epoch': 2.07}

[INFO|2025-03-20 01:03:20] trainer.py:4117 >> 
***** Running Evaluation *****

[INFO|2025-03-20 01:03:20] trainer.py:4119 >>   Num examples = 5152

[INFO|2025-03-20 01:03:20] trainer.py:4122 >>   Batch size = 2

[INFO|2025-03-20 01:08:22] trainer.py:3801 >> Saving model checkpoint to saves/Llama-3.2-3B-Instruct/lora/train_llama_3_2_v3/checkpoint-6000

[INFO|2025-03-20 01:08:23] tokenization_utils_base.py:2646 >> tokenizer config file saved in saves/Llama-3.2-3B-Instruct/lora/train_llama_3_2_v3/checkpoint-6000/tokenizer_config.json

[INFO|2025-03-20 01:08:23] tokenization_utils_base.py:2655 >> Special tokens file saved in saves/Llama-3.2-3B-Instruct/lora/train_llama_3_2_v3/checkpoint-6000/special_tokens_map.json

[INFO|2025-03-20 01:08:37] logging.py:157 >> {'loss': 1.2736, 'learning_rate': 1.0887e-05, 'epoch': 2.07}

[INFO|2025-03-20 01:08:51] logging.py:157 >> {'loss': 1.2989, 'learning_rate': 1.0849e-05, 'epoch': 2.07}

[INFO|2025-03-20 01:09:06] logging.py:157 >> {'loss': 1.1559, 'learning_rate': 1.0812e-05, 'epoch': 2.08}

[INFO|2025-03-20 01:09:22] logging.py:157 >> {'loss': 1.3089, 'learning_rate': 1.0775e-05, 'epoch': 2.08}

[INFO|2025-03-20 01:09:38] logging.py:157 >> {'loss': 1.2807, 'learning_rate': 1.0738e-05, 'epoch': 2.08}

[INFO|2025-03-20 01:09:54] logging.py:157 >> {'loss': 1.3651, 'learning_rate': 1.0701e-05, 'epoch': 2.08}

[INFO|2025-03-20 01:10:11] logging.py:157 >> {'loss': 1.3162, 'learning_rate': 1.0664e-05, 'epoch': 2.08}

[INFO|2025-03-20 01:10:26] logging.py:157 >> {'loss': 1.3553, 'learning_rate': 1.0627e-05, 'epoch': 2.08}

[INFO|2025-03-20 01:10:40] logging.py:157 >> {'loss': 1.2501, 'learning_rate': 1.0590e-05, 'epoch': 2.09}

[INFO|2025-03-20 01:10:55] logging.py:157 >> {'loss': 1.0742, 'learning_rate': 1.0553e-05, 'epoch': 2.09}

[INFO|2025-03-20 01:11:10] logging.py:157 >> {'loss': 1.3905, 'learning_rate': 1.0516e-05, 'epoch': 2.09}

[INFO|2025-03-20 01:11:25] logging.py:157 >> {'loss': 1.4442, 'learning_rate': 1.0479e-05, 'epoch': 2.09}

[INFO|2025-03-20 01:11:37] logging.py:157 >> {'loss': 1.2988, 'learning_rate': 1.0442e-05, 'epoch': 2.09}

[INFO|2025-03-20 01:11:52] logging.py:157 >> {'loss': 1.3158, 'learning_rate': 1.0406e-05, 'epoch': 2.09}

[INFO|2025-03-20 01:12:08] logging.py:157 >> {'loss': 1.3660, 'learning_rate': 1.0369e-05, 'epoch': 2.10}

[INFO|2025-03-20 01:12:23] logging.py:157 >> {'loss': 1.4161, 'learning_rate': 1.0332e-05, 'epoch': 2.10}

[INFO|2025-03-20 01:12:38] logging.py:157 >> {'loss': 1.3209, 'learning_rate': 1.0296e-05, 'epoch': 2.10}

[INFO|2025-03-20 01:12:52] logging.py:157 >> {'loss': 1.2491, 'learning_rate': 1.0259e-05, 'epoch': 2.10}

[INFO|2025-03-20 01:13:06] logging.py:157 >> {'loss': 1.3132, 'learning_rate': 1.0223e-05, 'epoch': 2.10}

[INFO|2025-03-20 01:13:22] logging.py:157 >> {'loss': 1.2649, 'learning_rate': 1.0186e-05, 'epoch': 2.11}

[INFO|2025-03-20 01:13:22] trainer.py:4117 >> 
***** Running Evaluation *****

[INFO|2025-03-20 01:13:22] trainer.py:4119 >>   Num examples = 5152

[INFO|2025-03-20 01:13:22] trainer.py:4122 >>   Batch size = 2

[INFO|2025-03-20 01:18:25] trainer.py:3801 >> Saving model checkpoint to saves/Llama-3.2-3B-Instruct/lora/train_llama_3_2_v3/checkpoint-6100

[INFO|2025-03-20 01:18:25] tokenization_utils_base.py:2646 >> tokenizer config file saved in saves/Llama-3.2-3B-Instruct/lora/train_llama_3_2_v3/checkpoint-6100/tokenizer_config.json

[INFO|2025-03-20 01:18:25] tokenization_utils_base.py:2655 >> Special tokens file saved in saves/Llama-3.2-3B-Instruct/lora/train_llama_3_2_v3/checkpoint-6100/special_tokens_map.json

[INFO|2025-03-20 01:18:41] logging.py:157 >> {'loss': 1.3358, 'learning_rate': 1.0150e-05, 'epoch': 2.11}

[INFO|2025-03-20 01:18:56] logging.py:157 >> {'loss': 1.4710, 'learning_rate': 1.0114e-05, 'epoch': 2.11}

[INFO|2025-03-20 01:19:12] logging.py:157 >> {'loss': 1.4750, 'learning_rate': 1.0077e-05, 'epoch': 2.11}

[INFO|2025-03-20 01:19:27] logging.py:157 >> {'loss': 1.3558, 'learning_rate': 1.0041e-05, 'epoch': 2.11}

[INFO|2025-03-20 01:19:42] logging.py:157 >> {'loss': 1.2795, 'learning_rate': 1.0005e-05, 'epoch': 2.11}

[INFO|2025-03-20 01:19:57] logging.py:157 >> {'loss': 1.3555, 'learning_rate': 9.9689e-06, 'epoch': 2.12}

[INFO|2025-03-20 01:20:12] logging.py:157 >> {'loss': 1.2671, 'learning_rate': 9.9329e-06, 'epoch': 2.12}

[INFO|2025-03-20 01:20:26] logging.py:157 >> {'loss': 1.3280, 'learning_rate': 9.8968e-06, 'epoch': 2.12}

[INFO|2025-03-20 01:20:41] logging.py:157 >> {'loss': 1.3330, 'learning_rate': 9.8608e-06, 'epoch': 2.12}

[INFO|2025-03-20 01:20:56] logging.py:157 >> {'loss': 1.3015, 'learning_rate': 9.8249e-06, 'epoch': 2.12}

[INFO|2025-03-20 01:21:10] logging.py:157 >> {'loss': 1.4338, 'learning_rate': 9.7890e-06, 'epoch': 2.12}

[INFO|2025-03-20 01:21:24] logging.py:157 >> {'loss': 1.3127, 'learning_rate': 9.7532e-06, 'epoch': 2.13}

[INFO|2025-03-20 01:21:40] logging.py:157 >> {'loss': 1.2205, 'learning_rate': 9.7174e-06, 'epoch': 2.13}

[INFO|2025-03-20 01:21:55] logging.py:157 >> {'loss': 1.2192, 'learning_rate': 9.6817e-06, 'epoch': 2.13}

[INFO|2025-03-20 01:22:11] logging.py:157 >> {'loss': 1.2920, 'learning_rate': 9.6460e-06, 'epoch': 2.13}

[INFO|2025-03-20 01:22:25] logging.py:157 >> {'loss': 1.3676, 'learning_rate': 9.6104e-06, 'epoch': 2.13}

[INFO|2025-03-20 01:22:40] logging.py:157 >> {'loss': 1.2668, 'learning_rate': 9.5748e-06, 'epoch': 2.13}

[INFO|2025-03-20 01:22:54] logging.py:157 >> {'loss': 1.2746, 'learning_rate': 9.5392e-06, 'epoch': 2.14}

[INFO|2025-03-20 01:23:08] logging.py:157 >> {'loss': 1.3675, 'learning_rate': 9.5038e-06, 'epoch': 2.14}

[INFO|2025-03-20 01:23:23] logging.py:157 >> {'loss': 1.4013, 'learning_rate': 9.4683e-06, 'epoch': 2.14}

[INFO|2025-03-20 01:23:23] trainer.py:4117 >> 
***** Running Evaluation *****

[INFO|2025-03-20 01:23:23] trainer.py:4119 >>   Num examples = 5152

[INFO|2025-03-20 01:23:23] trainer.py:4122 >>   Batch size = 2

[INFO|2025-03-20 01:28:25] trainer.py:3801 >> Saving model checkpoint to saves/Llama-3.2-3B-Instruct/lora/train_llama_3_2_v3/checkpoint-6200

[INFO|2025-03-20 01:28:25] tokenization_utils_base.py:2646 >> tokenizer config file saved in saves/Llama-3.2-3B-Instruct/lora/train_llama_3_2_v3/checkpoint-6200/tokenizer_config.json

[INFO|2025-03-20 01:28:25] tokenization_utils_base.py:2655 >> Special tokens file saved in saves/Llama-3.2-3B-Instruct/lora/train_llama_3_2_v3/checkpoint-6200/special_tokens_map.json

[INFO|2025-03-20 01:28:42] logging.py:157 >> {'loss': 1.1520, 'learning_rate': 9.4329e-06, 'epoch': 2.14}

[INFO|2025-03-20 01:28:58] logging.py:157 >> {'loss': 1.2438, 'learning_rate': 9.3976e-06, 'epoch': 2.14}

[INFO|2025-03-20 01:29:13] logging.py:157 >> {'loss': 1.3907, 'learning_rate': 9.3623e-06, 'epoch': 2.14}

[INFO|2025-03-20 01:29:28] logging.py:157 >> {'loss': 1.3239, 'learning_rate': 9.3271e-06, 'epoch': 2.15}

[INFO|2025-03-20 01:29:44] logging.py:157 >> {'loss': 1.2518, 'learning_rate': 9.2919e-06, 'epoch': 2.15}

[INFO|2025-03-20 01:29:57] logging.py:157 >> {'loss': 1.4043, 'learning_rate': 9.2568e-06, 'epoch': 2.15}

[INFO|2025-03-20 01:30:11] logging.py:157 >> {'loss': 1.4238, 'learning_rate': 9.2217e-06, 'epoch': 2.15}

[INFO|2025-03-20 01:30:27] logging.py:157 >> {'loss': 1.1780, 'learning_rate': 9.1867e-06, 'epoch': 2.15}

[INFO|2025-03-20 01:30:43] logging.py:157 >> {'loss': 1.2295, 'learning_rate': 9.1517e-06, 'epoch': 2.16}

[INFO|2025-03-20 01:30:57] logging.py:157 >> {'loss': 1.3589, 'learning_rate': 9.1168e-06, 'epoch': 2.16}

[INFO|2025-03-20 01:31:10] logging.py:157 >> {'loss': 1.1478, 'learning_rate': 9.0820e-06, 'epoch': 2.16}

[INFO|2025-03-20 01:31:26] logging.py:157 >> {'loss': 1.2757, 'learning_rate': 9.0471e-06, 'epoch': 2.16}

[INFO|2025-03-20 01:31:40] logging.py:157 >> {'loss': 1.1934, 'learning_rate': 9.0124e-06, 'epoch': 2.16}

[INFO|2025-03-20 01:31:55] logging.py:157 >> {'loss': 1.3136, 'learning_rate': 8.9777e-06, 'epoch': 2.16}

[INFO|2025-03-20 01:32:10] logging.py:157 >> {'loss': 1.3098, 'learning_rate': 8.9430e-06, 'epoch': 2.17}

[INFO|2025-03-20 01:32:24] logging.py:157 >> {'loss': 1.3978, 'learning_rate': 8.9084e-06, 'epoch': 2.17}

[INFO|2025-03-20 01:32:39] logging.py:157 >> {'loss': 1.2554, 'learning_rate': 8.8738e-06, 'epoch': 2.17}

[INFO|2025-03-20 01:32:54] logging.py:157 >> {'loss': 1.2677, 'learning_rate': 8.8393e-06, 'epoch': 2.17}

[INFO|2025-03-20 01:33:09] logging.py:157 >> {'loss': 1.2121, 'learning_rate': 8.8049e-06, 'epoch': 2.17}

[INFO|2025-03-20 01:33:25] logging.py:157 >> {'loss': 1.1802, 'learning_rate': 8.7705e-06, 'epoch': 2.17}

[INFO|2025-03-20 01:33:25] trainer.py:4117 >> 
***** Running Evaluation *****

[INFO|2025-03-20 01:33:25] trainer.py:4119 >>   Num examples = 5152

[INFO|2025-03-20 01:33:25] trainer.py:4122 >>   Batch size = 2

[INFO|2025-03-20 01:38:28] trainer.py:3801 >> Saving model checkpoint to saves/Llama-3.2-3B-Instruct/lora/train_llama_3_2_v3/checkpoint-6300

[INFO|2025-03-20 01:38:28] tokenization_utils_base.py:2646 >> tokenizer config file saved in saves/Llama-3.2-3B-Instruct/lora/train_llama_3_2_v3/checkpoint-6300/tokenizer_config.json

[INFO|2025-03-20 01:38:28] tokenization_utils_base.py:2655 >> Special tokens file saved in saves/Llama-3.2-3B-Instruct/lora/train_llama_3_2_v3/checkpoint-6300/special_tokens_map.json

[INFO|2025-03-20 01:38:42] logging.py:157 >> {'loss': 1.3355, 'learning_rate': 8.7362e-06, 'epoch': 2.18}

[INFO|2025-03-20 01:38:58] logging.py:157 >> {'loss': 1.2421, 'learning_rate': 8.7019e-06, 'epoch': 2.18}

[INFO|2025-03-20 01:39:12] logging.py:157 >> {'loss': 1.2504, 'learning_rate': 8.6676e-06, 'epoch': 2.18}

[INFO|2025-03-20 01:39:28] logging.py:157 >> {'loss': 1.3452, 'learning_rate': 8.6334e-06, 'epoch': 2.18}

[INFO|2025-03-20 01:39:43] logging.py:157 >> {'loss': 1.4128, 'learning_rate': 8.5993e-06, 'epoch': 2.18}

[INFO|2025-03-20 01:39:57] logging.py:157 >> {'loss': 1.2509, 'learning_rate': 8.5652e-06, 'epoch': 2.18}

[INFO|2025-03-20 01:40:13] logging.py:157 >> {'loss': 1.2036, 'learning_rate': 8.5312e-06, 'epoch': 2.19}

[INFO|2025-03-20 01:40:27] logging.py:157 >> {'loss': 1.3682, 'learning_rate': 8.4973e-06, 'epoch': 2.19}

[INFO|2025-03-20 01:40:43] logging.py:157 >> {'loss': 1.4754, 'learning_rate': 8.4633e-06, 'epoch': 2.19}

[INFO|2025-03-20 01:40:58] logging.py:157 >> {'loss': 1.2636, 'learning_rate': 8.4295e-06, 'epoch': 2.19}

[INFO|2025-03-20 01:41:12] logging.py:157 >> {'loss': 1.2938, 'learning_rate': 8.3957e-06, 'epoch': 2.19}

[INFO|2025-03-20 01:41:25] logging.py:157 >> {'loss': 1.2731, 'learning_rate': 8.3619e-06, 'epoch': 2.19}

[INFO|2025-03-20 01:41:40] logging.py:157 >> {'loss': 1.2967, 'learning_rate': 8.3282e-06, 'epoch': 2.20}

[INFO|2025-03-20 01:41:55] logging.py:157 >> {'loss': 1.2685, 'learning_rate': 8.2946e-06, 'epoch': 2.20}

[INFO|2025-03-20 01:42:11] logging.py:157 >> {'loss': 1.2530, 'learning_rate': 8.2610e-06, 'epoch': 2.20}

[INFO|2025-03-20 01:42:25] logging.py:157 >> {'loss': 1.3864, 'learning_rate': 8.2275e-06, 'epoch': 2.20}

[INFO|2025-03-20 01:42:40] logging.py:157 >> {'loss': 1.3964, 'learning_rate': 8.1940e-06, 'epoch': 2.20}

[INFO|2025-03-20 01:42:54] logging.py:157 >> {'loss': 1.2919, 'learning_rate': 8.1606e-06, 'epoch': 2.21}

[INFO|2025-03-20 01:43:08] logging.py:157 >> {'loss': 1.3272, 'learning_rate': 8.1272e-06, 'epoch': 2.21}

[INFO|2025-03-20 01:43:23] logging.py:157 >> {'loss': 1.3795, 'learning_rate': 8.0939e-06, 'epoch': 2.21}

[INFO|2025-03-20 01:43:23] trainer.py:4117 >> 
***** Running Evaluation *****

[INFO|2025-03-20 01:43:23] trainer.py:4119 >>   Num examples = 5152

[INFO|2025-03-20 01:43:23] trainer.py:4122 >>   Batch size = 2

[INFO|2025-03-20 01:48:26] trainer.py:3801 >> Saving model checkpoint to saves/Llama-3.2-3B-Instruct/lora/train_llama_3_2_v3/checkpoint-6400

[INFO|2025-03-20 01:48:26] tokenization_utils_base.py:2646 >> tokenizer config file saved in saves/Llama-3.2-3B-Instruct/lora/train_llama_3_2_v3/checkpoint-6400/tokenizer_config.json

[INFO|2025-03-20 01:48:26] tokenization_utils_base.py:2655 >> Special tokens file saved in saves/Llama-3.2-3B-Instruct/lora/train_llama_3_2_v3/checkpoint-6400/special_tokens_map.json

[INFO|2025-03-20 01:48:41] logging.py:157 >> {'loss': 1.3146, 'learning_rate': 8.0606e-06, 'epoch': 2.21}

[INFO|2025-03-20 01:48:56] logging.py:157 >> {'loss': 1.4449, 'learning_rate': 8.0274e-06, 'epoch': 2.21}

[INFO|2025-03-20 01:49:10] logging.py:157 >> {'loss': 1.4397, 'learning_rate': 7.9943e-06, 'epoch': 2.21}

[INFO|2025-03-20 01:49:25] logging.py:157 >> {'loss': 1.4975, 'learning_rate': 7.9612e-06, 'epoch': 2.22}

[INFO|2025-03-20 01:49:41] logging.py:157 >> {'loss': 1.3811, 'learning_rate': 7.9281e-06, 'epoch': 2.22}

[INFO|2025-03-20 01:49:55] logging.py:157 >> {'loss': 1.3034, 'learning_rate': 7.8952e-06, 'epoch': 2.22}

[INFO|2025-03-20 01:50:11] logging.py:157 >> {'loss': 1.4248, 'learning_rate': 7.8622e-06, 'epoch': 2.22}

[INFO|2025-03-20 01:50:26] logging.py:157 >> {'loss': 1.2489, 'learning_rate': 7.8294e-06, 'epoch': 2.22}

[INFO|2025-03-20 01:50:41] logging.py:157 >> {'loss': 1.2108, 'learning_rate': 7.7965e-06, 'epoch': 2.22}

[INFO|2025-03-20 01:50:55] logging.py:157 >> {'loss': 1.3631, 'learning_rate': 7.7638e-06, 'epoch': 2.23}

[INFO|2025-03-20 01:51:10] logging.py:157 >> {'loss': 1.2872, 'learning_rate': 7.7311e-06, 'epoch': 2.23}

[INFO|2025-03-20 01:51:25] logging.py:157 >> {'loss': 1.2874, 'learning_rate': 7.6984e-06, 'epoch': 2.23}

[INFO|2025-03-20 01:51:39] logging.py:157 >> {'loss': 1.3524, 'learning_rate': 7.6659e-06, 'epoch': 2.23}

[INFO|2025-03-20 01:51:54] logging.py:157 >> {'loss': 1.3036, 'learning_rate': 7.6333e-06, 'epoch': 2.23}

[INFO|2025-03-20 01:52:09] logging.py:157 >> {'loss': 1.2787, 'learning_rate': 7.6008e-06, 'epoch': 2.23}

[INFO|2025-03-20 01:52:23] logging.py:157 >> {'loss': 1.2690, 'learning_rate': 7.5684e-06, 'epoch': 2.24}

[INFO|2025-03-20 01:52:37] logging.py:157 >> {'loss': 1.2828, 'learning_rate': 7.5361e-06, 'epoch': 2.24}

[INFO|2025-03-20 01:52:50] logging.py:157 >> {'loss': 1.2132, 'learning_rate': 7.5038e-06, 'epoch': 2.24}

[INFO|2025-03-20 01:53:05] logging.py:157 >> {'loss': 1.3082, 'learning_rate': 7.4715e-06, 'epoch': 2.24}

[INFO|2025-03-20 01:53:20] logging.py:157 >> {'loss': 1.4736, 'learning_rate': 7.4393e-06, 'epoch': 2.24}

[INFO|2025-03-20 01:53:20] trainer.py:4117 >> 
***** Running Evaluation *****

[INFO|2025-03-20 01:53:20] trainer.py:4119 >>   Num examples = 5152

[INFO|2025-03-20 01:53:20] trainer.py:4122 >>   Batch size = 2

[INFO|2025-03-20 01:58:23] trainer.py:3801 >> Saving model checkpoint to saves/Llama-3.2-3B-Instruct/lora/train_llama_3_2_v3/checkpoint-6500

[INFO|2025-03-20 01:58:23] tokenization_utils_base.py:2646 >> tokenizer config file saved in saves/Llama-3.2-3B-Instruct/lora/train_llama_3_2_v3/checkpoint-6500/tokenizer_config.json

[INFO|2025-03-20 01:58:23] tokenization_utils_base.py:2655 >> Special tokens file saved in saves/Llama-3.2-3B-Instruct/lora/train_llama_3_2_v3/checkpoint-6500/special_tokens_map.json

[INFO|2025-03-20 01:58:39] logging.py:157 >> {'loss': 1.3964, 'learning_rate': 7.4072e-06, 'epoch': 2.25}

[INFO|2025-03-20 01:58:54] logging.py:157 >> {'loss': 1.4189, 'learning_rate': 7.3751e-06, 'epoch': 2.25}

[INFO|2025-03-20 01:59:10] logging.py:157 >> {'loss': 1.3561, 'learning_rate': 7.3431e-06, 'epoch': 2.25}

[INFO|2025-03-20 01:59:23] logging.py:157 >> {'loss': 1.2093, 'learning_rate': 7.3112e-06, 'epoch': 2.25}

[INFO|2025-03-20 01:59:38] logging.py:157 >> {'loss': 1.3462, 'learning_rate': 7.2793e-06, 'epoch': 2.25}

[INFO|2025-03-20 01:59:54] logging.py:157 >> {'loss': 1.2976, 'learning_rate': 7.2474e-06, 'epoch': 2.25}

[INFO|2025-03-20 02:00:07] logging.py:157 >> {'loss': 1.3102, 'learning_rate': 7.2156e-06, 'epoch': 2.26}

[INFO|2025-03-20 02:00:21] logging.py:157 >> {'loss': 1.4505, 'learning_rate': 7.1839e-06, 'epoch': 2.26}

[INFO|2025-03-20 02:00:34] logging.py:157 >> {'loss': 1.4601, 'learning_rate': 7.1522e-06, 'epoch': 2.26}

[INFO|2025-03-20 02:00:48] logging.py:157 >> {'loss': 1.3867, 'learning_rate': 7.1206e-06, 'epoch': 2.26}

[INFO|2025-03-20 02:01:01] logging.py:157 >> {'loss': 1.3330, 'learning_rate': 7.0891e-06, 'epoch': 2.26}

[INFO|2025-03-20 02:01:16] logging.py:157 >> {'loss': 1.2876, 'learning_rate': 7.0576e-06, 'epoch': 2.26}

[INFO|2025-03-20 02:01:31] logging.py:157 >> {'loss': 1.3404, 'learning_rate': 7.0261e-06, 'epoch': 2.27}

[INFO|2025-03-20 02:01:47] logging.py:157 >> {'loss': 1.1199, 'learning_rate': 6.9948e-06, 'epoch': 2.27}

[INFO|2025-03-20 02:02:02] logging.py:157 >> {'loss': 1.3772, 'learning_rate': 6.9634e-06, 'epoch': 2.27}

[INFO|2025-03-20 02:02:16] logging.py:157 >> {'loss': 1.3640, 'learning_rate': 6.9322e-06, 'epoch': 2.27}

[INFO|2025-03-20 02:02:30] logging.py:157 >> {'loss': 1.2207, 'learning_rate': 6.9010e-06, 'epoch': 2.27}

[INFO|2025-03-20 02:02:45] logging.py:157 >> {'loss': 1.3443, 'learning_rate': 6.8698e-06, 'epoch': 2.27}

[INFO|2025-03-20 02:02:59] logging.py:157 >> {'loss': 1.3035, 'learning_rate': 6.8388e-06, 'epoch': 2.28}

[INFO|2025-03-20 02:03:14] logging.py:157 >> {'loss': 1.3391, 'learning_rate': 6.8077e-06, 'epoch': 2.28}

[INFO|2025-03-20 02:03:14] trainer.py:4117 >> 
***** Running Evaluation *****

[INFO|2025-03-20 02:03:14] trainer.py:4119 >>   Num examples = 5152

[INFO|2025-03-20 02:03:14] trainer.py:4122 >>   Batch size = 2

[INFO|2025-03-20 02:08:16] trainer.py:3801 >> Saving model checkpoint to saves/Llama-3.2-3B-Instruct/lora/train_llama_3_2_v3/checkpoint-6600

[INFO|2025-03-20 02:08:16] tokenization_utils_base.py:2646 >> tokenizer config file saved in saves/Llama-3.2-3B-Instruct/lora/train_llama_3_2_v3/checkpoint-6600/tokenizer_config.json

[INFO|2025-03-20 02:08:16] tokenization_utils_base.py:2655 >> Special tokens file saved in saves/Llama-3.2-3B-Instruct/lora/train_llama_3_2_v3/checkpoint-6600/special_tokens_map.json

[INFO|2025-03-20 02:08:31] logging.py:157 >> {'loss': 1.2669, 'learning_rate': 6.7768e-06, 'epoch': 2.28}

[INFO|2025-03-20 02:08:47] logging.py:157 >> {'loss': 1.2784, 'learning_rate': 6.7459e-06, 'epoch': 2.28}

[INFO|2025-03-20 02:09:02] logging.py:157 >> {'loss': 1.5057, 'learning_rate': 6.7150e-06, 'epoch': 2.28}

[INFO|2025-03-20 02:09:16] logging.py:157 >> {'loss': 1.2924, 'learning_rate': 6.6842e-06, 'epoch': 2.28}

[INFO|2025-03-20 02:09:31] logging.py:157 >> {'loss': 1.1713, 'learning_rate': 6.6535e-06, 'epoch': 2.29}

[INFO|2025-03-20 02:09:46] logging.py:157 >> {'loss': 1.3296, 'learning_rate': 6.6229e-06, 'epoch': 2.29}

[INFO|2025-03-20 02:10:02] logging.py:157 >> {'loss': 1.3248, 'learning_rate': 6.5923e-06, 'epoch': 2.29}

[INFO|2025-03-20 02:10:18] logging.py:157 >> {'loss': 1.3718, 'learning_rate': 6.5617e-06, 'epoch': 2.29}

[INFO|2025-03-20 02:10:32] logging.py:157 >> {'loss': 1.4218, 'learning_rate': 6.5312e-06, 'epoch': 2.29}

[INFO|2025-03-20 02:10:48] logging.py:157 >> {'loss': 1.1745, 'learning_rate': 6.5008e-06, 'epoch': 2.30}

[INFO|2025-03-20 02:11:03] logging.py:157 >> {'loss': 1.3761, 'learning_rate': 6.4704e-06, 'epoch': 2.30}

[INFO|2025-03-20 02:11:17] logging.py:157 >> {'loss': 1.2593, 'learning_rate': 6.4401e-06, 'epoch': 2.30}

[INFO|2025-03-20 02:11:32] logging.py:157 >> {'loss': 1.3751, 'learning_rate': 6.4099e-06, 'epoch': 2.30}

[INFO|2025-03-20 02:11:47] logging.py:157 >> {'loss': 1.1480, 'learning_rate': 6.3797e-06, 'epoch': 2.30}

[INFO|2025-03-20 02:12:00] logging.py:157 >> {'loss': 1.2846, 'learning_rate': 6.3496e-06, 'epoch': 2.30}

[INFO|2025-03-20 02:12:15] logging.py:157 >> {'loss': 1.3336, 'learning_rate': 6.3195e-06, 'epoch': 2.31}

[INFO|2025-03-20 02:12:31] logging.py:157 >> {'loss': 1.3786, 'learning_rate': 6.2895e-06, 'epoch': 2.31}

[INFO|2025-03-20 02:12:45] logging.py:157 >> {'loss': 1.2793, 'learning_rate': 6.2596e-06, 'epoch': 2.31}

[INFO|2025-03-20 02:13:01] logging.py:157 >> {'loss': 1.3445, 'learning_rate': 6.2297e-06, 'epoch': 2.31}

[INFO|2025-03-20 02:13:15] logging.py:157 >> {'loss': 1.2583, 'learning_rate': 6.1999e-06, 'epoch': 2.31}

[INFO|2025-03-20 02:13:15] trainer.py:4117 >> 
***** Running Evaluation *****

[INFO|2025-03-20 02:13:15] trainer.py:4119 >>   Num examples = 5152

[INFO|2025-03-20 02:13:15] trainer.py:4122 >>   Batch size = 2

[INFO|2025-03-20 02:18:18] trainer.py:3801 >> Saving model checkpoint to saves/Llama-3.2-3B-Instruct/lora/train_llama_3_2_v3/checkpoint-6700

[INFO|2025-03-20 02:18:18] tokenization_utils_base.py:2646 >> tokenizer config file saved in saves/Llama-3.2-3B-Instruct/lora/train_llama_3_2_v3/checkpoint-6700/tokenizer_config.json

[INFO|2025-03-20 02:18:18] tokenization_utils_base.py:2655 >> Special tokens file saved in saves/Llama-3.2-3B-Instruct/lora/train_llama_3_2_v3/checkpoint-6700/special_tokens_map.json

[INFO|2025-03-20 02:18:33] logging.py:157 >> {'loss': 1.4123, 'learning_rate': 6.1702e-06, 'epoch': 2.31}

[INFO|2025-03-20 02:18:47] logging.py:157 >> {'loss': 1.1382, 'learning_rate': 6.1405e-06, 'epoch': 2.32}

[INFO|2025-03-20 02:19:01] logging.py:157 >> {'loss': 1.2608, 'learning_rate': 6.1108e-06, 'epoch': 2.32}

[INFO|2025-03-20 02:19:16] logging.py:157 >> {'loss': 1.4207, 'learning_rate': 6.0813e-06, 'epoch': 2.32}

[INFO|2025-03-20 02:19:31] logging.py:157 >> {'loss': 1.2987, 'learning_rate': 6.0518e-06, 'epoch': 2.32}

[INFO|2025-03-20 02:19:46] logging.py:157 >> {'loss': 1.1990, 'learning_rate': 6.0223e-06, 'epoch': 2.32}

[INFO|2025-03-20 02:19:59] logging.py:157 >> {'loss': 1.2921, 'learning_rate': 5.9929e-06, 'epoch': 2.32}

[INFO|2025-03-20 02:20:14] logging.py:157 >> {'loss': 1.2968, 'learning_rate': 5.9636e-06, 'epoch': 2.33}

[INFO|2025-03-20 02:20:27] logging.py:157 >> {'loss': 1.2634, 'learning_rate': 5.9344e-06, 'epoch': 2.33}

[INFO|2025-03-20 02:20:42] logging.py:157 >> {'loss': 1.1659, 'learning_rate': 5.9052e-06, 'epoch': 2.33}

[INFO|2025-03-20 02:20:57] logging.py:157 >> {'loss': 1.3485, 'learning_rate': 5.8760e-06, 'epoch': 2.33}

[INFO|2025-03-20 02:21:10] logging.py:157 >> {'loss': 1.3643, 'learning_rate': 5.8470e-06, 'epoch': 2.33}

[INFO|2025-03-20 02:21:26] logging.py:157 >> {'loss': 1.3301, 'learning_rate': 5.8179e-06, 'epoch': 2.33}

[INFO|2025-03-20 02:21:41] logging.py:157 >> {'loss': 1.2424, 'learning_rate': 5.7890e-06, 'epoch': 2.34}

[INFO|2025-03-20 02:21:55] logging.py:157 >> {'loss': 1.3927, 'learning_rate': 5.7601e-06, 'epoch': 2.34}

[INFO|2025-03-20 02:22:09] logging.py:157 >> {'loss': 1.2235, 'learning_rate': 5.7313e-06, 'epoch': 2.34}

[INFO|2025-03-20 02:22:24] logging.py:157 >> {'loss': 1.3006, 'learning_rate': 5.7025e-06, 'epoch': 2.34}

[INFO|2025-03-20 02:22:39] logging.py:157 >> {'loss': 1.2445, 'learning_rate': 5.6738e-06, 'epoch': 2.34}

[INFO|2025-03-20 02:22:53] logging.py:157 >> {'loss': 1.3485, 'learning_rate': 5.6452e-06, 'epoch': 2.35}

[INFO|2025-03-20 02:23:08] logging.py:157 >> {'loss': 1.3043, 'learning_rate': 5.6166e-06, 'epoch': 2.35}

[INFO|2025-03-20 02:23:08] trainer.py:4117 >> 
***** Running Evaluation *****

[INFO|2025-03-20 02:23:08] trainer.py:4119 >>   Num examples = 5152

[INFO|2025-03-20 02:23:08] trainer.py:4122 >>   Batch size = 2

[INFO|2025-03-20 02:28:10] trainer.py:3801 >> Saving model checkpoint to saves/Llama-3.2-3B-Instruct/lora/train_llama_3_2_v3/checkpoint-6800

[INFO|2025-03-20 02:28:10] tokenization_utils_base.py:2646 >> tokenizer config file saved in saves/Llama-3.2-3B-Instruct/lora/train_llama_3_2_v3/checkpoint-6800/tokenizer_config.json

[INFO|2025-03-20 02:28:10] tokenization_utils_base.py:2655 >> Special tokens file saved in saves/Llama-3.2-3B-Instruct/lora/train_llama_3_2_v3/checkpoint-6800/special_tokens_map.json

[INFO|2025-03-20 02:28:25] logging.py:157 >> {'loss': 1.2405, 'learning_rate': 5.5881e-06, 'epoch': 2.35}

[INFO|2025-03-20 02:28:40] logging.py:157 >> {'loss': 1.3088, 'learning_rate': 5.5597e-06, 'epoch': 2.35}

[INFO|2025-03-20 02:28:54] logging.py:157 >> {'loss': 1.5762, 'learning_rate': 5.5313e-06, 'epoch': 2.35}

[INFO|2025-03-20 02:29:08] logging.py:157 >> {'loss': 1.2801, 'learning_rate': 5.5030e-06, 'epoch': 2.35}

[INFO|2025-03-20 02:29:23] logging.py:157 >> {'loss': 1.5134, 'learning_rate': 5.4748e-06, 'epoch': 2.36}

[INFO|2025-03-20 02:29:38] logging.py:157 >> {'loss': 1.3006, 'learning_rate': 5.4466e-06, 'epoch': 2.36}

[INFO|2025-03-20 02:29:53] logging.py:157 >> {'loss': 1.1904, 'learning_rate': 5.4184e-06, 'epoch': 2.36}

[INFO|2025-03-20 02:30:07] logging.py:157 >> {'loss': 1.3710, 'learning_rate': 5.3904e-06, 'epoch': 2.36}

[INFO|2025-03-20 02:30:22] logging.py:157 >> {'loss': 1.3286, 'learning_rate': 5.3624e-06, 'epoch': 2.36}

[INFO|2025-03-20 02:30:36] logging.py:157 >> {'loss': 1.4042, 'learning_rate': 5.3345e-06, 'epoch': 2.36}

[INFO|2025-03-20 02:30:51] logging.py:157 >> {'loss': 1.1674, 'learning_rate': 5.3066e-06, 'epoch': 2.37}

[INFO|2025-03-20 02:31:06] logging.py:157 >> {'loss': 1.3422, 'learning_rate': 5.2788e-06, 'epoch': 2.37}

[INFO|2025-03-20 02:31:22] logging.py:157 >> {'loss': 1.3124, 'learning_rate': 5.2511e-06, 'epoch': 2.37}

[INFO|2025-03-20 02:31:37] logging.py:157 >> {'loss': 1.2141, 'learning_rate': 5.2234e-06, 'epoch': 2.37}

[INFO|2025-03-20 02:31:52] logging.py:157 >> {'loss': 1.3240, 'learning_rate': 5.1958e-06, 'epoch': 2.37}

[INFO|2025-03-20 02:32:05] logging.py:157 >> {'loss': 1.2583, 'learning_rate': 5.1682e-06, 'epoch': 2.37}

[INFO|2025-03-20 02:32:20] logging.py:157 >> {'loss': 1.2732, 'learning_rate': 5.1408e-06, 'epoch': 2.38}

[INFO|2025-03-20 02:32:36] logging.py:157 >> {'loss': 1.3840, 'learning_rate': 5.1133e-06, 'epoch': 2.38}

[INFO|2025-03-20 02:32:50] logging.py:157 >> {'loss': 1.4746, 'learning_rate': 5.0860e-06, 'epoch': 2.38}

[INFO|2025-03-20 02:33:06] logging.py:157 >> {'loss': 1.4610, 'learning_rate': 5.0587e-06, 'epoch': 2.38}

[INFO|2025-03-20 02:33:06] trainer.py:4117 >> 
***** Running Evaluation *****

[INFO|2025-03-20 02:33:06] trainer.py:4119 >>   Num examples = 5152

[INFO|2025-03-20 02:33:06] trainer.py:4122 >>   Batch size = 2

[INFO|2025-03-20 02:38:09] trainer.py:3801 >> Saving model checkpoint to saves/Llama-3.2-3B-Instruct/lora/train_llama_3_2_v3/checkpoint-6900

[INFO|2025-03-20 02:38:09] tokenization_utils_base.py:2646 >> tokenizer config file saved in saves/Llama-3.2-3B-Instruct/lora/train_llama_3_2_v3/checkpoint-6900/tokenizer_config.json

[INFO|2025-03-20 02:38:09] tokenization_utils_base.py:2655 >> Special tokens file saved in saves/Llama-3.2-3B-Instruct/lora/train_llama_3_2_v3/checkpoint-6900/special_tokens_map.json

[INFO|2025-03-20 02:38:23] logging.py:157 >> {'loss': 1.3700, 'learning_rate': 5.0315e-06, 'epoch': 2.38}

[INFO|2025-03-20 02:38:39] logging.py:157 >> {'loss': 1.1984, 'learning_rate': 5.0043e-06, 'epoch': 2.38}

[INFO|2025-03-20 02:38:54] logging.py:157 >> {'loss': 1.2762, 'learning_rate': 4.9772e-06, 'epoch': 2.39}

[INFO|2025-03-20 02:39:09] logging.py:157 >> {'loss': 1.2244, 'learning_rate': 4.9502e-06, 'epoch': 2.39}

[INFO|2025-03-20 02:39:23] logging.py:157 >> {'loss': 1.4067, 'learning_rate': 4.9233e-06, 'epoch': 2.39}

[INFO|2025-03-20 02:39:38] logging.py:157 >> {'loss': 1.3016, 'learning_rate': 4.8964e-06, 'epoch': 2.39}

[INFO|2025-03-20 02:39:52] logging.py:157 >> {'loss': 1.3034, 'learning_rate': 4.8695e-06, 'epoch': 2.39}

[INFO|2025-03-20 02:40:07] logging.py:157 >> {'loss': 1.0572, 'learning_rate': 4.8428e-06, 'epoch': 2.40}

[INFO|2025-03-20 02:40:22] logging.py:157 >> {'loss': 1.2646, 'learning_rate': 4.8161e-06, 'epoch': 2.40}

[INFO|2025-03-20 02:40:36] logging.py:157 >> {'loss': 1.3061, 'learning_rate': 4.7895e-06, 'epoch': 2.40}

[INFO|2025-03-20 02:40:52] logging.py:157 >> {'loss': 1.2054, 'learning_rate': 4.7629e-06, 'epoch': 2.40}

[INFO|2025-03-20 02:41:07] logging.py:157 >> {'loss': 1.3846, 'learning_rate': 4.7364e-06, 'epoch': 2.40}

[INFO|2025-03-20 02:41:21] logging.py:157 >> {'loss': 1.2664, 'learning_rate': 4.7100e-06, 'epoch': 2.40}

[INFO|2025-03-20 02:41:36] logging.py:157 >> {'loss': 1.4469, 'learning_rate': 4.6836e-06, 'epoch': 2.41}

[INFO|2025-03-20 02:41:51] logging.py:157 >> {'loss': 1.3975, 'learning_rate': 4.6573e-06, 'epoch': 2.41}

[INFO|2025-03-20 02:42:07] logging.py:157 >> {'loss': 1.2728, 'learning_rate': 4.6311e-06, 'epoch': 2.41}

[INFO|2025-03-20 02:42:22] logging.py:157 >> {'loss': 1.3324, 'learning_rate': 4.6049e-06, 'epoch': 2.41}

[INFO|2025-03-20 02:42:38] logging.py:157 >> {'loss': 1.2067, 'learning_rate': 4.5788e-06, 'epoch': 2.41}

[INFO|2025-03-20 02:42:52] logging.py:157 >> {'loss': 1.2851, 'learning_rate': 4.5528e-06, 'epoch': 2.41}

[INFO|2025-03-20 02:43:08] logging.py:157 >> {'loss': 1.2073, 'learning_rate': 4.5268e-06, 'epoch': 2.42}

[INFO|2025-03-20 02:43:08] trainer.py:4117 >> 
***** Running Evaluation *****

[INFO|2025-03-20 02:43:08] trainer.py:4119 >>   Num examples = 5152

[INFO|2025-03-20 02:43:08] trainer.py:4122 >>   Batch size = 2

[INFO|2025-03-20 02:48:11] trainer.py:3801 >> Saving model checkpoint to saves/Llama-3.2-3B-Instruct/lora/train_llama_3_2_v3/checkpoint-7000

[INFO|2025-03-20 02:48:11] tokenization_utils_base.py:2646 >> tokenizer config file saved in saves/Llama-3.2-3B-Instruct/lora/train_llama_3_2_v3/checkpoint-7000/tokenizer_config.json

[INFO|2025-03-20 02:48:11] tokenization_utils_base.py:2655 >> Special tokens file saved in saves/Llama-3.2-3B-Instruct/lora/train_llama_3_2_v3/checkpoint-7000/special_tokens_map.json

[INFO|2025-03-20 02:48:26] logging.py:157 >> {'loss': 1.1849, 'learning_rate': 4.5009e-06, 'epoch': 2.42}

[INFO|2025-03-20 02:48:40] logging.py:157 >> {'loss': 1.2176, 'learning_rate': 4.4751e-06, 'epoch': 2.42}

[INFO|2025-03-20 02:48:55] logging.py:157 >> {'loss': 1.4074, 'learning_rate': 4.4493e-06, 'epoch': 2.42}

[INFO|2025-03-20 02:49:08] logging.py:157 >> {'loss': 1.4531, 'learning_rate': 4.4236e-06, 'epoch': 2.42}

[INFO|2025-03-20 02:49:24] logging.py:157 >> {'loss': 1.3547, 'learning_rate': 4.3980e-06, 'epoch': 2.42}

[INFO|2025-03-20 02:49:38] logging.py:157 >> {'loss': 1.2919, 'learning_rate': 4.3724e-06, 'epoch': 2.43}

[INFO|2025-03-20 02:49:52] logging.py:157 >> {'loss': 1.3315, 'learning_rate': 4.3469e-06, 'epoch': 2.43}

[INFO|2025-03-20 02:50:08] logging.py:157 >> {'loss': 1.2349, 'learning_rate': 4.3215e-06, 'epoch': 2.43}

[INFO|2025-03-20 02:50:24] logging.py:157 >> {'loss': 1.3976, 'learning_rate': 4.2962e-06, 'epoch': 2.43}

[INFO|2025-03-20 02:50:37] logging.py:157 >> {'loss': 1.4185, 'learning_rate': 4.2709e-06, 'epoch': 2.43}

[INFO|2025-03-20 02:50:52] logging.py:157 >> {'loss': 1.2540, 'learning_rate': 4.2456e-06, 'epoch': 2.43}

[INFO|2025-03-20 02:51:07] logging.py:157 >> {'loss': 1.3762, 'learning_rate': 4.2205e-06, 'epoch': 2.44}

[INFO|2025-03-20 02:51:23] logging.py:157 >> {'loss': 1.3232, 'learning_rate': 4.1954e-06, 'epoch': 2.44}

[INFO|2025-03-20 02:51:37] logging.py:157 >> {'loss': 1.3086, 'learning_rate': 4.1704e-06, 'epoch': 2.44}

[INFO|2025-03-20 02:51:51] logging.py:157 >> {'loss': 1.3966, 'learning_rate': 4.1454e-06, 'epoch': 2.44}

[INFO|2025-03-20 02:52:07] logging.py:157 >> {'loss': 1.1404, 'learning_rate': 4.1205e-06, 'epoch': 2.44}

[INFO|2025-03-20 02:52:21] logging.py:157 >> {'loss': 1.1944, 'learning_rate': 4.0957e-06, 'epoch': 2.45}

[INFO|2025-03-20 02:52:34] logging.py:157 >> {'loss': 1.3549, 'learning_rate': 4.0710e-06, 'epoch': 2.45}

[INFO|2025-03-20 02:52:49] logging.py:157 >> {'loss': 1.1670, 'learning_rate': 4.0463e-06, 'epoch': 2.45}

[INFO|2025-03-20 02:53:04] logging.py:157 >> {'loss': 1.2022, 'learning_rate': 4.0217e-06, 'epoch': 2.45}

[INFO|2025-03-20 02:53:04] trainer.py:4117 >> 
***** Running Evaluation *****

[INFO|2025-03-20 02:53:04] trainer.py:4119 >>   Num examples = 5152

[INFO|2025-03-20 02:53:04] trainer.py:4122 >>   Batch size = 2

[INFO|2025-03-20 02:58:06] trainer.py:3801 >> Saving model checkpoint to saves/Llama-3.2-3B-Instruct/lora/train_llama_3_2_v3/checkpoint-7100

[INFO|2025-03-20 02:58:06] tokenization_utils_base.py:2646 >> tokenizer config file saved in saves/Llama-3.2-3B-Instruct/lora/train_llama_3_2_v3/checkpoint-7100/tokenizer_config.json

[INFO|2025-03-20 02:58:06] tokenization_utils_base.py:2655 >> Special tokens file saved in saves/Llama-3.2-3B-Instruct/lora/train_llama_3_2_v3/checkpoint-7100/special_tokens_map.json

[INFO|2025-03-20 02:58:22] logging.py:157 >> {'loss': 1.2889, 'learning_rate': 3.9971e-06, 'epoch': 2.45}

[INFO|2025-03-20 02:58:38] logging.py:157 >> {'loss': 1.0258, 'learning_rate': 3.9727e-06, 'epoch': 2.45}

[INFO|2025-03-20 02:58:53] logging.py:157 >> {'loss': 1.3044, 'learning_rate': 3.9483e-06, 'epoch': 2.46}

[INFO|2025-03-20 02:59:09] logging.py:157 >> {'loss': 1.2853, 'learning_rate': 3.9239e-06, 'epoch': 2.46}

[INFO|2025-03-20 02:59:24] logging.py:157 >> {'loss': 1.3218, 'learning_rate': 3.8996e-06, 'epoch': 2.46}

[INFO|2025-03-20 02:59:39] logging.py:157 >> {'loss': 1.0975, 'learning_rate': 3.8755e-06, 'epoch': 2.46}

[INFO|2025-03-20 02:59:54] logging.py:157 >> {'loss': 1.2768, 'learning_rate': 3.8513e-06, 'epoch': 2.46}

[INFO|2025-03-20 03:00:09] logging.py:157 >> {'loss': 1.3364, 'learning_rate': 3.8273e-06, 'epoch': 2.46}

[INFO|2025-03-20 03:00:23] logging.py:157 >> {'loss': 1.3947, 'learning_rate': 3.8033e-06, 'epoch': 2.47}

[INFO|2025-03-20 03:00:38] logging.py:157 >> {'loss': 1.4522, 'learning_rate': 3.7793e-06, 'epoch': 2.47}

[INFO|2025-03-20 03:00:53] logging.py:157 >> {'loss': 1.1788, 'learning_rate': 3.7555e-06, 'epoch': 2.47}

[INFO|2025-03-20 03:01:08] logging.py:157 >> {'loss': 1.2568, 'learning_rate': 3.7317e-06, 'epoch': 2.47}

[INFO|2025-03-20 03:01:24] logging.py:157 >> {'loss': 1.3532, 'learning_rate': 3.7080e-06, 'epoch': 2.47}

[INFO|2025-03-20 03:01:37] logging.py:157 >> {'loss': 1.3308, 'learning_rate': 3.6844e-06, 'epoch': 2.47}

[INFO|2025-03-20 03:01:53] logging.py:157 >> {'loss': 1.3422, 'learning_rate': 3.6608e-06, 'epoch': 2.48}

[INFO|2025-03-20 03:02:08] logging.py:157 >> {'loss': 1.2079, 'learning_rate': 3.6373e-06, 'epoch': 2.48}

[INFO|2025-03-20 03:02:22] logging.py:157 >> {'loss': 1.4211, 'learning_rate': 3.6138e-06, 'epoch': 2.48}

[INFO|2025-03-20 03:02:38] logging.py:157 >> {'loss': 1.2888, 'learning_rate': 3.5905e-06, 'epoch': 2.48}

[INFO|2025-03-20 03:02:51] logging.py:157 >> {'loss': 1.4087, 'learning_rate': 3.5672e-06, 'epoch': 2.48}

[INFO|2025-03-20 03:03:06] logging.py:157 >> {'loss': 1.5023, 'learning_rate': 3.5439e-06, 'epoch': 2.48}

[INFO|2025-03-20 03:03:06] trainer.py:4117 >> 
***** Running Evaluation *****

[INFO|2025-03-20 03:03:06] trainer.py:4119 >>   Num examples = 5152

[INFO|2025-03-20 03:03:06] trainer.py:4122 >>   Batch size = 2

[INFO|2025-03-20 03:08:09] trainer.py:3801 >> Saving model checkpoint to saves/Llama-3.2-3B-Instruct/lora/train_llama_3_2_v3/checkpoint-7200

[INFO|2025-03-20 03:08:09] tokenization_utils_base.py:2646 >> tokenizer config file saved in saves/Llama-3.2-3B-Instruct/lora/train_llama_3_2_v3/checkpoint-7200/tokenizer_config.json

[INFO|2025-03-20 03:08:09] tokenization_utils_base.py:2655 >> Special tokens file saved in saves/Llama-3.2-3B-Instruct/lora/train_llama_3_2_v3/checkpoint-7200/special_tokens_map.json

[INFO|2025-03-20 03:08:24] logging.py:157 >> {'loss': 1.3555, 'learning_rate': 3.5208e-06, 'epoch': 2.49}

[INFO|2025-03-20 03:08:39] logging.py:157 >> {'loss': 1.2951, 'learning_rate': 3.4977e-06, 'epoch': 2.49}

[INFO|2025-03-20 03:08:53] logging.py:157 >> {'loss': 1.2920, 'learning_rate': 3.4747e-06, 'epoch': 2.49}

[INFO|2025-03-20 03:09:07] logging.py:157 >> {'loss': 1.3826, 'learning_rate': 3.4517e-06, 'epoch': 2.49}

[INFO|2025-03-20 03:09:22] logging.py:157 >> {'loss': 1.3728, 'learning_rate': 3.4289e-06, 'epoch': 2.49}

[INFO|2025-03-20 03:09:37] logging.py:157 >> {'loss': 1.3133, 'learning_rate': 3.4061e-06, 'epoch': 2.50}

[INFO|2025-03-20 03:09:52] logging.py:157 >> {'loss': 1.3152, 'learning_rate': 3.3833e-06, 'epoch': 2.50}

[INFO|2025-03-20 03:10:06] logging.py:157 >> {'loss': 1.3593, 'learning_rate': 3.3607e-06, 'epoch': 2.50}

[INFO|2025-03-20 03:10:21] logging.py:157 >> {'loss': 1.3767, 'learning_rate': 3.3381e-06, 'epoch': 2.50}

[INFO|2025-03-20 03:10:36] logging.py:157 >> {'loss': 1.2439, 'learning_rate': 3.3156e-06, 'epoch': 2.50}

[INFO|2025-03-20 03:10:51] logging.py:157 >> {'loss': 1.5021, 'learning_rate': 3.2931e-06, 'epoch': 2.50}

[INFO|2025-03-20 03:11:05] logging.py:157 >> {'loss': 1.4072, 'learning_rate': 3.2707e-06, 'epoch': 2.51}

[INFO|2025-03-20 03:11:19] logging.py:157 >> {'loss': 1.2523, 'learning_rate': 3.2484e-06, 'epoch': 2.51}

[INFO|2025-03-20 03:11:34] logging.py:157 >> {'loss': 1.2754, 'learning_rate': 3.2262e-06, 'epoch': 2.51}

[INFO|2025-03-20 03:11:46] logging.py:157 >> {'loss': 1.2516, 'learning_rate': 3.2040e-06, 'epoch': 2.51}

[INFO|2025-03-20 03:12:02] logging.py:157 >> {'loss': 1.2844, 'learning_rate': 3.1819e-06, 'epoch': 2.51}

[INFO|2025-03-20 03:12:18] logging.py:157 >> {'loss': 1.3470, 'learning_rate': 3.1599e-06, 'epoch': 2.51}

[INFO|2025-03-20 03:12:32] logging.py:157 >> {'loss': 1.3510, 'learning_rate': 3.1379e-06, 'epoch': 2.52}

[INFO|2025-03-20 03:12:47] logging.py:157 >> {'loss': 1.3914, 'learning_rate': 3.1161e-06, 'epoch': 2.52}

[INFO|2025-03-20 03:13:01] logging.py:157 >> {'loss': 1.4705, 'learning_rate': 3.0942e-06, 'epoch': 2.52}

[INFO|2025-03-20 03:13:01] trainer.py:4117 >> 
***** Running Evaluation *****

[INFO|2025-03-20 03:13:01] trainer.py:4119 >>   Num examples = 5152

[INFO|2025-03-20 03:13:01] trainer.py:4122 >>   Batch size = 2

[INFO|2025-03-20 03:18:04] trainer.py:3801 >> Saving model checkpoint to saves/Llama-3.2-3B-Instruct/lora/train_llama_3_2_v3/checkpoint-7300

[INFO|2025-03-20 03:18:04] tokenization_utils_base.py:2646 >> tokenizer config file saved in saves/Llama-3.2-3B-Instruct/lora/train_llama_3_2_v3/checkpoint-7300/tokenizer_config.json

[INFO|2025-03-20 03:18:04] tokenization_utils_base.py:2655 >> Special tokens file saved in saves/Llama-3.2-3B-Instruct/lora/train_llama_3_2_v3/checkpoint-7300/special_tokens_map.json

[INFO|2025-03-20 03:18:19] logging.py:157 >> {'loss': 1.3202, 'learning_rate': 3.0725e-06, 'epoch': 2.52}

[INFO|2025-03-20 03:18:34] logging.py:157 >> {'loss': 1.3140, 'learning_rate': 3.0508e-06, 'epoch': 2.52}

[INFO|2025-03-20 03:18:48] logging.py:157 >> {'loss': 1.3771, 'learning_rate': 3.0292e-06, 'epoch': 2.52}

[INFO|2025-03-20 03:19:04] logging.py:157 >> {'loss': 1.2347, 'learning_rate': 3.0077e-06, 'epoch': 2.53}

[INFO|2025-03-20 03:19:18] logging.py:157 >> {'loss': 1.3625, 'learning_rate': 2.9863e-06, 'epoch': 2.53}

[INFO|2025-03-20 03:19:33] logging.py:157 >> {'loss': 1.3401, 'learning_rate': 2.9649e-06, 'epoch': 2.53}

[INFO|2025-03-20 03:19:48] logging.py:157 >> {'loss': 1.2435, 'learning_rate': 2.9436e-06, 'epoch': 2.53}

[INFO|2025-03-20 03:20:03] logging.py:157 >> {'loss': 1.4242, 'learning_rate': 2.9224e-06, 'epoch': 2.53}

[INFO|2025-03-20 03:20:16] logging.py:157 >> {'loss': 1.3189, 'learning_rate': 2.9012e-06, 'epoch': 2.53}

[INFO|2025-03-20 03:20:31] logging.py:157 >> {'loss': 1.2596, 'learning_rate': 2.8801e-06, 'epoch': 2.54}

[INFO|2025-03-20 03:20:46] logging.py:157 >> {'loss': 1.3655, 'learning_rate': 2.8591e-06, 'epoch': 2.54}

[INFO|2025-03-20 03:21:00] logging.py:157 >> {'loss': 1.2551, 'learning_rate': 2.8381e-06, 'epoch': 2.54}

[INFO|2025-03-20 03:21:15] logging.py:157 >> {'loss': 1.3895, 'learning_rate': 2.8173e-06, 'epoch': 2.54}

[INFO|2025-03-20 03:21:30] logging.py:157 >> {'loss': 1.3327, 'learning_rate': 2.7965e-06, 'epoch': 2.54}

[INFO|2025-03-20 03:21:45] logging.py:157 >> {'loss': 1.4037, 'learning_rate': 2.7757e-06, 'epoch': 2.55}

[INFO|2025-03-20 03:21:59] logging.py:157 >> {'loss': 1.2474, 'learning_rate': 2.7551e-06, 'epoch': 2.55}

[INFO|2025-03-20 03:22:14] logging.py:157 >> {'loss': 1.3287, 'learning_rate': 2.7345e-06, 'epoch': 2.55}

[INFO|2025-03-20 03:22:29] logging.py:157 >> {'loss': 1.2376, 'learning_rate': 2.7140e-06, 'epoch': 2.55}

[INFO|2025-03-20 03:22:44] logging.py:157 >> {'loss': 1.3481, 'learning_rate': 2.6935e-06, 'epoch': 2.55}

[INFO|2025-03-20 03:22:59] logging.py:157 >> {'loss': 1.2087, 'learning_rate': 2.6732e-06, 'epoch': 2.55}

[INFO|2025-03-20 03:22:59] trainer.py:4117 >> 
***** Running Evaluation *****

[INFO|2025-03-20 03:22:59] trainer.py:4119 >>   Num examples = 5152

[INFO|2025-03-20 03:22:59] trainer.py:4122 >>   Batch size = 2

[INFO|2025-03-20 03:28:01] trainer.py:3801 >> Saving model checkpoint to saves/Llama-3.2-3B-Instruct/lora/train_llama_3_2_v3/checkpoint-7400

[INFO|2025-03-20 03:28:01] tokenization_utils_base.py:2646 >> tokenizer config file saved in saves/Llama-3.2-3B-Instruct/lora/train_llama_3_2_v3/checkpoint-7400/tokenizer_config.json

[INFO|2025-03-20 03:28:01] tokenization_utils_base.py:2655 >> Special tokens file saved in saves/Llama-3.2-3B-Instruct/lora/train_llama_3_2_v3/checkpoint-7400/special_tokens_map.json

[INFO|2025-03-20 03:28:15] logging.py:157 >> {'loss': 1.3721, 'learning_rate': 2.6529e-06, 'epoch': 2.56}

[INFO|2025-03-20 03:28:31] logging.py:157 >> {'loss': 1.2740, 'learning_rate': 2.6327e-06, 'epoch': 2.56}

[INFO|2025-03-20 03:28:46] logging.py:157 >> {'loss': 1.5671, 'learning_rate': 2.6125e-06, 'epoch': 2.56}

[INFO|2025-03-20 03:29:01] logging.py:157 >> {'loss': 1.4589, 'learning_rate': 2.5924e-06, 'epoch': 2.56}

[INFO|2025-03-20 03:29:16] logging.py:157 >> {'loss': 1.2765, 'learning_rate': 2.5724e-06, 'epoch': 2.56}

[INFO|2025-03-20 03:29:31] logging.py:157 >> {'loss': 1.2137, 'learning_rate': 2.5525e-06, 'epoch': 2.56}

[INFO|2025-03-20 03:29:45] logging.py:157 >> {'loss': 1.3419, 'learning_rate': 2.5327e-06, 'epoch': 2.57}

[INFO|2025-03-20 03:29:59] logging.py:157 >> {'loss': 1.2770, 'learning_rate': 2.5129e-06, 'epoch': 2.57}

[INFO|2025-03-20 03:30:14] logging.py:157 >> {'loss': 1.2437, 'learning_rate': 2.4932e-06, 'epoch': 2.57}

[INFO|2025-03-20 03:30:29] logging.py:157 >> {'loss': 1.5935, 'learning_rate': 2.4735e-06, 'epoch': 2.57}

[INFO|2025-03-20 03:30:43] logging.py:157 >> {'loss': 1.3840, 'learning_rate': 2.4540e-06, 'epoch': 2.57}

[INFO|2025-03-20 03:30:58] logging.py:157 >> {'loss': 1.3144, 'learning_rate': 2.4345e-06, 'epoch': 2.57}

[INFO|2025-03-20 03:31:12] logging.py:157 >> {'loss': 1.3253, 'learning_rate': 2.4151e-06, 'epoch': 2.58}

[INFO|2025-03-20 03:31:26] logging.py:157 >> {'loss': 1.3116, 'learning_rate': 2.3957e-06, 'epoch': 2.58}

[INFO|2025-03-20 03:31:41] logging.py:157 >> {'loss': 1.2568, 'learning_rate': 2.3765e-06, 'epoch': 2.58}

[INFO|2025-03-20 03:31:55] logging.py:157 >> {'loss': 1.2414, 'learning_rate': 2.3573e-06, 'epoch': 2.58}

[INFO|2025-03-20 03:32:11] logging.py:157 >> {'loss': 1.2099, 'learning_rate': 2.3382e-06, 'epoch': 2.58}

[INFO|2025-03-20 03:32:25] logging.py:157 >> {'loss': 1.4194, 'learning_rate': 2.3191e-06, 'epoch': 2.58}

[INFO|2025-03-20 03:32:40] logging.py:157 >> {'loss': 1.3192, 'learning_rate': 2.3002e-06, 'epoch': 2.59}

[INFO|2025-03-20 03:32:55] logging.py:157 >> {'loss': 1.3631, 'learning_rate': 2.2813e-06, 'epoch': 2.59}

[INFO|2025-03-20 03:32:55] trainer.py:4117 >> 
***** Running Evaluation *****

[INFO|2025-03-20 03:32:55] trainer.py:4119 >>   Num examples = 5152

[INFO|2025-03-20 03:32:55] trainer.py:4122 >>   Batch size = 2

[INFO|2025-03-20 03:37:58] trainer.py:3801 >> Saving model checkpoint to saves/Llama-3.2-3B-Instruct/lora/train_llama_3_2_v3/checkpoint-7500

[INFO|2025-03-20 03:37:58] tokenization_utils_base.py:2646 >> tokenizer config file saved in saves/Llama-3.2-3B-Instruct/lora/train_llama_3_2_v3/checkpoint-7500/tokenizer_config.json

[INFO|2025-03-20 03:37:58] tokenization_utils_base.py:2655 >> Special tokens file saved in saves/Llama-3.2-3B-Instruct/lora/train_llama_3_2_v3/checkpoint-7500/special_tokens_map.json

[INFO|2025-03-20 03:38:12] logging.py:157 >> {'loss': 1.4558, 'learning_rate': 2.2624e-06, 'epoch': 2.59}

[INFO|2025-03-20 03:38:29] logging.py:157 >> {'loss': 1.2830, 'learning_rate': 2.2437e-06, 'epoch': 2.59}

[INFO|2025-03-20 03:38:42] logging.py:157 >> {'loss': 1.3136, 'learning_rate': 2.2250e-06, 'epoch': 2.59}

[INFO|2025-03-20 03:38:56] logging.py:157 >> {'loss': 1.2531, 'learning_rate': 2.2064e-06, 'epoch': 2.60}

[INFO|2025-03-20 03:39:11] logging.py:157 >> {'loss': 1.1869, 'learning_rate': 2.1879e-06, 'epoch': 2.60}

[INFO|2025-03-20 03:39:23] logging.py:157 >> {'loss': 1.2697, 'learning_rate': 2.1695e-06, 'epoch': 2.60}

[INFO|2025-03-20 03:39:38] logging.py:157 >> {'loss': 1.3329, 'learning_rate': 2.1511e-06, 'epoch': 2.60}

[INFO|2025-03-20 03:39:52] logging.py:157 >> {'loss': 1.3543, 'learning_rate': 2.1328e-06, 'epoch': 2.60}

[INFO|2025-03-20 03:40:06] logging.py:157 >> {'loss': 1.3322, 'learning_rate': 2.1146e-06, 'epoch': 2.60}

[INFO|2025-03-20 03:40:22] logging.py:157 >> {'loss': 1.3375, 'learning_rate': 2.0964e-06, 'epoch': 2.61}

[INFO|2025-03-20 03:40:36] logging.py:157 >> {'loss': 1.3308, 'learning_rate': 2.0783e-06, 'epoch': 2.61}

[INFO|2025-03-20 03:40:50] logging.py:157 >> {'loss': 1.3834, 'learning_rate': 2.0603e-06, 'epoch': 2.61}

[INFO|2025-03-20 03:41:04] logging.py:157 >> {'loss': 1.4052, 'learning_rate': 2.0424e-06, 'epoch': 2.61}

[INFO|2025-03-20 03:41:18] logging.py:157 >> {'loss': 1.2497, 'learning_rate': 2.0246e-06, 'epoch': 2.61}

[INFO|2025-03-20 03:41:33] logging.py:157 >> {'loss': 1.3302, 'learning_rate': 2.0068e-06, 'epoch': 2.61}

[INFO|2025-03-20 03:41:48] logging.py:157 >> {'loss': 1.1808, 'learning_rate': 1.9891e-06, 'epoch': 2.62}

[INFO|2025-03-20 03:42:03] logging.py:157 >> {'loss': 1.3244, 'learning_rate': 1.9715e-06, 'epoch': 2.62}

[INFO|2025-03-20 03:42:19] logging.py:157 >> {'loss': 1.3359, 'learning_rate': 1.9539e-06, 'epoch': 2.62}

[INFO|2025-03-20 03:42:33] logging.py:157 >> {'loss': 1.3269, 'learning_rate': 1.9364e-06, 'epoch': 2.62}

[INFO|2025-03-20 03:42:48] logging.py:157 >> {'loss': 1.3020, 'learning_rate': 1.9190e-06, 'epoch': 2.62}

[INFO|2025-03-20 03:42:48] trainer.py:4117 >> 
***** Running Evaluation *****

[INFO|2025-03-20 03:42:48] trainer.py:4119 >>   Num examples = 5152

[INFO|2025-03-20 03:42:48] trainer.py:4122 >>   Batch size = 2

[INFO|2025-03-20 03:47:50] trainer.py:3801 >> Saving model checkpoint to saves/Llama-3.2-3B-Instruct/lora/train_llama_3_2_v3/checkpoint-7600

[INFO|2025-03-20 03:47:50] tokenization_utils_base.py:2646 >> tokenizer config file saved in saves/Llama-3.2-3B-Instruct/lora/train_llama_3_2_v3/checkpoint-7600/tokenizer_config.json

[INFO|2025-03-20 03:47:50] tokenization_utils_base.py:2655 >> Special tokens file saved in saves/Llama-3.2-3B-Instruct/lora/train_llama_3_2_v3/checkpoint-7600/special_tokens_map.json

[INFO|2025-03-20 03:48:06] logging.py:157 >> {'loss': 1.2610, 'learning_rate': 1.9017e-06, 'epoch': 2.62}

[INFO|2025-03-20 03:48:21] logging.py:157 >> {'loss': 1.2665, 'learning_rate': 1.8845e-06, 'epoch': 2.63}

[INFO|2025-03-20 03:48:34] logging.py:157 >> {'loss': 1.1896, 'learning_rate': 1.8673e-06, 'epoch': 2.63}

[INFO|2025-03-20 03:48:48] logging.py:157 >> {'loss': 1.1964, 'learning_rate': 1.8502e-06, 'epoch': 2.63}

[INFO|2025-03-20 03:49:04] logging.py:157 >> {'loss': 1.2365, 'learning_rate': 1.8332e-06, 'epoch': 2.63}

[INFO|2025-03-20 03:49:18] logging.py:157 >> {'loss': 1.3401, 'learning_rate': 1.8162e-06, 'epoch': 2.63}

[INFO|2025-03-20 03:49:33] logging.py:157 >> {'loss': 1.1834, 'learning_rate': 1.7994e-06, 'epoch': 2.64}

[INFO|2025-03-20 03:49:48] logging.py:157 >> {'loss': 1.2451, 'learning_rate': 1.7826e-06, 'epoch': 2.64}

[INFO|2025-03-20 03:50:02] logging.py:157 >> {'loss': 1.3463, 'learning_rate': 1.7658e-06, 'epoch': 2.64}

[INFO|2025-03-20 03:50:18] logging.py:157 >> {'loss': 1.3697, 'learning_rate': 1.7492e-06, 'epoch': 2.64}

[INFO|2025-03-20 03:50:33] logging.py:157 >> {'loss': 1.1409, 'learning_rate': 1.7326e-06, 'epoch': 2.64}

[INFO|2025-03-20 03:50:47] logging.py:157 >> {'loss': 1.2436, 'learning_rate': 1.7161e-06, 'epoch': 2.64}

[INFO|2025-03-20 03:51:02] logging.py:157 >> {'loss': 1.2543, 'learning_rate': 1.6997e-06, 'epoch': 2.65}

[INFO|2025-03-20 03:51:16] logging.py:157 >> {'loss': 1.3228, 'learning_rate': 1.6834e-06, 'epoch': 2.65}

[INFO|2025-03-20 03:51:31] logging.py:157 >> {'loss': 1.3460, 'learning_rate': 1.6671e-06, 'epoch': 2.65}

[INFO|2025-03-20 03:51:46] logging.py:157 >> {'loss': 1.2746, 'learning_rate': 1.6509e-06, 'epoch': 2.65}

[INFO|2025-03-20 03:52:00] logging.py:157 >> {'loss': 1.4459, 'learning_rate': 1.6348e-06, 'epoch': 2.65}

[INFO|2025-03-20 03:52:13] logging.py:157 >> {'loss': 1.2391, 'learning_rate': 1.6188e-06, 'epoch': 2.65}

[INFO|2025-03-20 03:52:29] logging.py:157 >> {'loss': 1.3026, 'learning_rate': 1.6028e-06, 'epoch': 2.66}

[INFO|2025-03-20 03:52:44] logging.py:157 >> {'loss': 1.4864, 'learning_rate': 1.5870e-06, 'epoch': 2.66}

[INFO|2025-03-20 03:52:44] trainer.py:4117 >> 
***** Running Evaluation *****

[INFO|2025-03-20 03:52:44] trainer.py:4119 >>   Num examples = 5152

[INFO|2025-03-20 03:52:44] trainer.py:4122 >>   Batch size = 2

[INFO|2025-03-20 03:57:46] trainer.py:3801 >> Saving model checkpoint to saves/Llama-3.2-3B-Instruct/lora/train_llama_3_2_v3/checkpoint-7700

[INFO|2025-03-20 03:57:46] tokenization_utils_base.py:2646 >> tokenizer config file saved in saves/Llama-3.2-3B-Instruct/lora/train_llama_3_2_v3/checkpoint-7700/tokenizer_config.json

[INFO|2025-03-20 03:57:46] tokenization_utils_base.py:2655 >> Special tokens file saved in saves/Llama-3.2-3B-Instruct/lora/train_llama_3_2_v3/checkpoint-7700/special_tokens_map.json

[INFO|2025-03-20 03:58:00] logging.py:157 >> {'loss': 1.3371, 'learning_rate': 1.5712e-06, 'epoch': 2.66}

[INFO|2025-03-20 03:58:14] logging.py:157 >> {'loss': 1.2249, 'learning_rate': 1.5554e-06, 'epoch': 2.66}

[INFO|2025-03-20 03:58:29] logging.py:157 >> {'loss': 1.2075, 'learning_rate': 1.5398e-06, 'epoch': 2.66}

[INFO|2025-03-20 03:58:43] logging.py:157 >> {'loss': 1.4197, 'learning_rate': 1.5242e-06, 'epoch': 2.66}

[INFO|2025-03-20 03:58:59] logging.py:157 >> {'loss': 1.2607, 'learning_rate': 1.5087e-06, 'epoch': 2.67}

[INFO|2025-03-20 03:59:15] logging.py:157 >> {'loss': 1.3667, 'learning_rate': 1.4933e-06, 'epoch': 2.67}

[INFO|2025-03-20 03:59:29] logging.py:157 >> {'loss': 1.3041, 'learning_rate': 1.4780e-06, 'epoch': 2.67}

[INFO|2025-03-20 03:59:43] logging.py:157 >> {'loss': 1.4201, 'learning_rate': 1.4627e-06, 'epoch': 2.67}

[INFO|2025-03-20 03:59:58] logging.py:157 >> {'loss': 1.1705, 'learning_rate': 1.4475e-06, 'epoch': 2.67}

[INFO|2025-03-20 04:00:14] logging.py:157 >> {'loss': 1.4270, 'learning_rate': 1.4324e-06, 'epoch': 2.67}

[INFO|2025-03-20 04:00:28] logging.py:157 >> {'loss': 1.3734, 'learning_rate': 1.4173e-06, 'epoch': 2.68}

[INFO|2025-03-20 04:00:44] logging.py:157 >> {'loss': 1.2404, 'learning_rate': 1.4024e-06, 'epoch': 2.68}

[INFO|2025-03-20 04:00:59] logging.py:157 >> {'loss': 1.3514, 'learning_rate': 1.3875e-06, 'epoch': 2.68}

[INFO|2025-03-20 04:01:13] logging.py:157 >> {'loss': 1.2488, 'learning_rate': 1.3727e-06, 'epoch': 2.68}

[INFO|2025-03-20 04:01:28] logging.py:157 >> {'loss': 1.4747, 'learning_rate': 1.3580e-06, 'epoch': 2.68}

[INFO|2025-03-20 04:01:42] logging.py:157 >> {'loss': 1.2014, 'learning_rate': 1.3433e-06, 'epoch': 2.69}

[INFO|2025-03-20 04:01:56] logging.py:157 >> {'loss': 1.3128, 'learning_rate': 1.3287e-06, 'epoch': 2.69}

[INFO|2025-03-20 04:02:11] logging.py:157 >> {'loss': 1.3858, 'learning_rate': 1.3142e-06, 'epoch': 2.69}

[INFO|2025-03-20 04:02:24] logging.py:157 >> {'loss': 1.1819, 'learning_rate': 1.2998e-06, 'epoch': 2.69}

[INFO|2025-03-20 04:02:38] logging.py:157 >> {'loss': 1.2736, 'learning_rate': 1.2855e-06, 'epoch': 2.69}

[INFO|2025-03-20 04:02:38] trainer.py:4117 >> 
***** Running Evaluation *****

[INFO|2025-03-20 04:02:38] trainer.py:4119 >>   Num examples = 5152

[INFO|2025-03-20 04:02:38] trainer.py:4122 >>   Batch size = 2

[INFO|2025-03-20 04:07:41] trainer.py:3801 >> Saving model checkpoint to saves/Llama-3.2-3B-Instruct/lora/train_llama_3_2_v3/checkpoint-7800

[INFO|2025-03-20 04:07:41] tokenization_utils_base.py:2646 >> tokenizer config file saved in saves/Llama-3.2-3B-Instruct/lora/train_llama_3_2_v3/checkpoint-7800/tokenizer_config.json

[INFO|2025-03-20 04:07:41] tokenization_utils_base.py:2655 >> Special tokens file saved in saves/Llama-3.2-3B-Instruct/lora/train_llama_3_2_v3/checkpoint-7800/special_tokens_map.json

[INFO|2025-03-20 04:07:55] logging.py:157 >> {'loss': 1.3060, 'learning_rate': 1.2712e-06, 'epoch': 2.69}

[INFO|2025-03-20 04:08:11] logging.py:157 >> {'loss': 1.1021, 'learning_rate': 1.2570e-06, 'epoch': 2.70}

[INFO|2025-03-20 04:08:25] logging.py:157 >> {'loss': 1.3483, 'learning_rate': 1.2429e-06, 'epoch': 2.70}

[INFO|2025-03-20 04:08:41] logging.py:157 >> {'loss': 1.3160, 'learning_rate': 1.2289e-06, 'epoch': 2.70}

[INFO|2025-03-20 04:08:56] logging.py:157 >> {'loss': 1.1992, 'learning_rate': 1.2149e-06, 'epoch': 2.70}

[INFO|2025-03-20 04:09:11] logging.py:157 >> {'loss': 1.3526, 'learning_rate': 1.2011e-06, 'epoch': 2.70}

[INFO|2025-03-20 04:09:25] logging.py:157 >> {'loss': 1.3624, 'learning_rate': 1.1873e-06, 'epoch': 2.70}

[INFO|2025-03-20 04:09:39] logging.py:157 >> {'loss': 1.3922, 'learning_rate': 1.1736e-06, 'epoch': 2.71}

[INFO|2025-03-20 04:09:55] logging.py:157 >> {'loss': 1.3546, 'learning_rate': 1.1599e-06, 'epoch': 2.71}

[INFO|2025-03-20 04:10:08] logging.py:157 >> {'loss': 1.1937, 'learning_rate': 1.1463e-06, 'epoch': 2.71}

[INFO|2025-03-20 04:10:23] logging.py:157 >> {'loss': 1.4664, 'learning_rate': 1.1329e-06, 'epoch': 2.71}

[INFO|2025-03-20 04:10:37] logging.py:157 >> {'loss': 1.2318, 'learning_rate': 1.1194e-06, 'epoch': 2.71}

[INFO|2025-03-20 04:10:52] logging.py:157 >> {'loss': 1.3985, 'learning_rate': 1.1061e-06, 'epoch': 2.71}

[INFO|2025-03-20 04:11:05] logging.py:157 >> {'loss': 1.4059, 'learning_rate': 1.0929e-06, 'epoch': 2.72}

[INFO|2025-03-20 04:11:22] logging.py:157 >> {'loss': 1.4324, 'learning_rate': 1.0797e-06, 'epoch': 2.72}

[INFO|2025-03-20 04:11:35] logging.py:157 >> {'loss': 1.3970, 'learning_rate': 1.0666e-06, 'epoch': 2.72}

[INFO|2025-03-20 04:11:49] logging.py:157 >> {'loss': 1.3352, 'learning_rate': 1.0536e-06, 'epoch': 2.72}

[INFO|2025-03-20 04:12:04] logging.py:157 >> {'loss': 1.2945, 'learning_rate': 1.0406e-06, 'epoch': 2.72}

[INFO|2025-03-20 04:12:19] logging.py:157 >> {'loss': 1.3077, 'learning_rate': 1.0278e-06, 'epoch': 2.72}

[INFO|2025-03-20 04:12:33] logging.py:157 >> {'loss': 1.2744, 'learning_rate': 1.0150e-06, 'epoch': 2.73}

[INFO|2025-03-20 04:12:33] trainer.py:4117 >> 
***** Running Evaluation *****

[INFO|2025-03-20 04:12:33] trainer.py:4119 >>   Num examples = 5152

[INFO|2025-03-20 04:12:33] trainer.py:4122 >>   Batch size = 2

[INFO|2025-03-20 04:17:36] trainer.py:3801 >> Saving model checkpoint to saves/Llama-3.2-3B-Instruct/lora/train_llama_3_2_v3/checkpoint-7900

[INFO|2025-03-20 04:17:36] tokenization_utils_base.py:2646 >> tokenizer config file saved in saves/Llama-3.2-3B-Instruct/lora/train_llama_3_2_v3/checkpoint-7900/tokenizer_config.json

[INFO|2025-03-20 04:17:36] tokenization_utils_base.py:2655 >> Special tokens file saved in saves/Llama-3.2-3B-Instruct/lora/train_llama_3_2_v3/checkpoint-7900/special_tokens_map.json

[INFO|2025-03-20 04:17:52] logging.py:157 >> {'loss': 1.2358, 'learning_rate': 1.0023e-06, 'epoch': 2.73}

[INFO|2025-03-20 04:18:06] logging.py:157 >> {'loss': 1.2473, 'learning_rate': 9.8966e-07, 'epoch': 2.73}

[INFO|2025-03-20 04:18:22] logging.py:157 >> {'loss': 1.3002, 'learning_rate': 9.7711e-07, 'epoch': 2.73}

[INFO|2025-03-20 04:18:37] logging.py:157 >> {'loss': 1.3305, 'learning_rate': 9.6464e-07, 'epoch': 2.73}

[INFO|2025-03-20 04:18:53] logging.py:157 >> {'loss': 1.2782, 'learning_rate': 9.5225e-07, 'epoch': 2.74}

[INFO|2025-03-20 04:19:08] logging.py:157 >> {'loss': 1.3103, 'learning_rate': 9.3994e-07, 'epoch': 2.74}

[INFO|2025-03-20 04:19:22] logging.py:157 >> {'loss': 1.2554, 'learning_rate': 9.2770e-07, 'epoch': 2.74}

[INFO|2025-03-20 04:19:38] logging.py:157 >> {'loss': 1.4038, 'learning_rate': 9.1555e-07, 'epoch': 2.74}

[INFO|2025-03-20 04:19:53] logging.py:157 >> {'loss': 1.3537, 'learning_rate': 9.0347e-07, 'epoch': 2.74}

[INFO|2025-03-20 04:20:08] logging.py:157 >> {'loss': 1.3299, 'learning_rate': 8.9147e-07, 'epoch': 2.74}

[INFO|2025-03-20 04:20:22] logging.py:157 >> {'loss': 1.3700, 'learning_rate': 8.7955e-07, 'epoch': 2.75}

[INFO|2025-03-20 04:20:37] logging.py:157 >> {'loss': 1.2120, 'learning_rate': 8.6771e-07, 'epoch': 2.75}

[INFO|2025-03-20 04:20:50] logging.py:157 >> {'loss': 1.2997, 'learning_rate': 8.5595e-07, 'epoch': 2.75}

[INFO|2025-03-20 04:21:04] logging.py:157 >> {'loss': 1.3848, 'learning_rate': 8.4427e-07, 'epoch': 2.75}

[INFO|2025-03-20 04:21:20] logging.py:157 >> {'loss': 1.4069, 'learning_rate': 8.3267e-07, 'epoch': 2.75}

[INFO|2025-03-20 04:21:35] logging.py:157 >> {'loss': 1.3701, 'learning_rate': 8.2114e-07, 'epoch': 2.75}

[INFO|2025-03-20 04:21:50] logging.py:157 >> {'loss': 1.2399, 'learning_rate': 8.0969e-07, 'epoch': 2.76}

[INFO|2025-03-20 04:22:06] logging.py:157 >> {'loss': 1.1932, 'learning_rate': 7.9833e-07, 'epoch': 2.76}

[INFO|2025-03-20 04:22:21] logging.py:157 >> {'loss': 1.3976, 'learning_rate': 7.8704e-07, 'epoch': 2.76}

[INFO|2025-03-20 04:22:36] logging.py:157 >> {'loss': 1.2621, 'learning_rate': 7.7583e-07, 'epoch': 2.76}

[INFO|2025-03-20 04:22:36] trainer.py:4117 >> 
***** Running Evaluation *****

[INFO|2025-03-20 04:22:36] trainer.py:4119 >>   Num examples = 5152

[INFO|2025-03-20 04:22:36] trainer.py:4122 >>   Batch size = 2

[INFO|2025-03-20 04:27:39] trainer.py:3801 >> Saving model checkpoint to saves/Llama-3.2-3B-Instruct/lora/train_llama_3_2_v3/checkpoint-8000

[INFO|2025-03-20 04:27:40] tokenization_utils_base.py:2646 >> tokenizer config file saved in saves/Llama-3.2-3B-Instruct/lora/train_llama_3_2_v3/checkpoint-8000/tokenizer_config.json

[INFO|2025-03-20 04:27:40] tokenization_utils_base.py:2655 >> Special tokens file saved in saves/Llama-3.2-3B-Instruct/lora/train_llama_3_2_v3/checkpoint-8000/special_tokens_map.json

[INFO|2025-03-20 04:27:54] logging.py:157 >> {'loss': 1.4538, 'learning_rate': 7.6470e-07, 'epoch': 2.76}

[INFO|2025-03-20 04:28:08] logging.py:157 >> {'loss': 1.1544, 'learning_rate': 7.5365e-07, 'epoch': 2.76}

[INFO|2025-03-20 04:28:23] logging.py:157 >> {'loss': 1.2585, 'learning_rate': 7.4268e-07, 'epoch': 2.77}

[INFO|2025-03-20 04:28:39] logging.py:157 >> {'loss': 1.1263, 'learning_rate': 7.3179e-07, 'epoch': 2.77}

[INFO|2025-03-20 04:28:54] logging.py:157 >> {'loss': 1.3758, 'learning_rate': 7.2097e-07, 'epoch': 2.77}

[INFO|2025-03-20 04:29:09] logging.py:157 >> {'loss': 1.2707, 'learning_rate': 7.1024e-07, 'epoch': 2.77}

[INFO|2025-03-20 04:29:22] logging.py:157 >> {'loss': 1.3572, 'learning_rate': 6.9959e-07, 'epoch': 2.77}

[INFO|2025-03-20 04:29:36] logging.py:157 >> {'loss': 1.4091, 'learning_rate': 6.8901e-07, 'epoch': 2.77}

[INFO|2025-03-20 04:29:51] logging.py:157 >> {'loss': 1.2528, 'learning_rate': 6.7852e-07, 'epoch': 2.78}

[INFO|2025-03-20 04:30:04] logging.py:157 >> {'loss': 1.3164, 'learning_rate': 6.6810e-07, 'epoch': 2.78}

[INFO|2025-03-20 04:30:18] logging.py:157 >> {'loss': 1.3143, 'learning_rate': 6.5777e-07, 'epoch': 2.78}

[INFO|2025-03-20 04:30:33] logging.py:157 >> {'loss': 1.4393, 'learning_rate': 6.4751e-07, 'epoch': 2.78}

[INFO|2025-03-20 04:30:48] logging.py:157 >> {'loss': 1.2664, 'learning_rate': 6.3733e-07, 'epoch': 2.78}

[INFO|2025-03-20 04:31:04] logging.py:157 >> {'loss': 1.3012, 'learning_rate': 6.2723e-07, 'epoch': 2.79}

[INFO|2025-03-20 04:31:19] logging.py:157 >> {'loss': 1.2957, 'learning_rate': 6.1722e-07, 'epoch': 2.79}

[INFO|2025-03-20 04:31:34] logging.py:157 >> {'loss': 1.3757, 'learning_rate': 6.0728e-07, 'epoch': 2.79}

[INFO|2025-03-20 04:31:49] logging.py:157 >> {'loss': 1.2442, 'learning_rate': 5.9742e-07, 'epoch': 2.79}

[INFO|2025-03-20 04:32:03] logging.py:157 >> {'loss': 1.2303, 'learning_rate': 5.8764e-07, 'epoch': 2.79}

[INFO|2025-03-20 04:32:17] logging.py:157 >> {'loss': 1.3245, 'learning_rate': 5.7794e-07, 'epoch': 2.79}

[INFO|2025-03-20 04:32:31] logging.py:157 >> {'loss': 1.2879, 'learning_rate': 5.6832e-07, 'epoch': 2.80}

[INFO|2025-03-20 04:32:31] trainer.py:4117 >> 
***** Running Evaluation *****

[INFO|2025-03-20 04:32:31] trainer.py:4119 >>   Num examples = 5152

[INFO|2025-03-20 04:32:31] trainer.py:4122 >>   Batch size = 2

[INFO|2025-03-20 04:37:34] trainer.py:3801 >> Saving model checkpoint to saves/Llama-3.2-3B-Instruct/lora/train_llama_3_2_v3/checkpoint-8100

[INFO|2025-03-20 04:37:34] tokenization_utils_base.py:2646 >> tokenizer config file saved in saves/Llama-3.2-3B-Instruct/lora/train_llama_3_2_v3/checkpoint-8100/tokenizer_config.json

[INFO|2025-03-20 04:37:34] tokenization_utils_base.py:2655 >> Special tokens file saved in saves/Llama-3.2-3B-Instruct/lora/train_llama_3_2_v3/checkpoint-8100/special_tokens_map.json

[INFO|2025-03-20 04:37:50] logging.py:157 >> {'loss': 1.2728, 'learning_rate': 5.5878e-07, 'epoch': 2.80}

[INFO|2025-03-20 04:38:05] logging.py:157 >> {'loss': 1.3562, 'learning_rate': 5.4932e-07, 'epoch': 2.80}

[INFO|2025-03-20 04:38:22] logging.py:157 >> {'loss': 1.2211, 'learning_rate': 5.3994e-07, 'epoch': 2.80}

[INFO|2025-03-20 04:38:37] logging.py:157 >> {'loss': 1.2047, 'learning_rate': 5.3064e-07, 'epoch': 2.80}

[INFO|2025-03-20 04:38:53] logging.py:157 >> {'loss': 1.1948, 'learning_rate': 5.2142e-07, 'epoch': 2.80}

[INFO|2025-03-20 04:39:09] logging.py:157 >> {'loss': 1.2811, 'learning_rate': 5.1228e-07, 'epoch': 2.81}

[INFO|2025-03-20 04:39:22] logging.py:157 >> {'loss': 1.2840, 'learning_rate': 5.0322e-07, 'epoch': 2.81}

[INFO|2025-03-20 04:39:38] logging.py:157 >> {'loss': 1.2711, 'learning_rate': 4.9424e-07, 'epoch': 2.81}

[INFO|2025-03-20 04:39:52] logging.py:157 >> {'loss': 1.2147, 'learning_rate': 4.8534e-07, 'epoch': 2.81}

[INFO|2025-03-20 04:40:08] logging.py:157 >> {'loss': 1.2844, 'learning_rate': 4.7652e-07, 'epoch': 2.81}

[INFO|2025-03-20 04:40:23] logging.py:157 >> {'loss': 1.1862, 'learning_rate': 4.6778e-07, 'epoch': 2.81}

[INFO|2025-03-20 04:40:38] logging.py:157 >> {'loss': 1.3558, 'learning_rate': 4.5912e-07, 'epoch': 2.82}

[INFO|2025-03-20 04:40:53] logging.py:157 >> {'loss': 1.3858, 'learning_rate': 4.5054e-07, 'epoch': 2.82}

[INFO|2025-03-20 04:41:09] logging.py:157 >> {'loss': 1.1028, 'learning_rate': 4.4204e-07, 'epoch': 2.82}

[INFO|2025-03-20 04:41:24] logging.py:157 >> {'loss': 1.2000, 'learning_rate': 4.3362e-07, 'epoch': 2.82}

[INFO|2025-03-20 04:41:39] logging.py:157 >> {'loss': 1.3576, 'learning_rate': 4.2528e-07, 'epoch': 2.82}

[INFO|2025-03-20 04:41:52] logging.py:157 >> {'loss': 1.3558, 'learning_rate': 4.1702e-07, 'epoch': 2.82}

[INFO|2025-03-20 04:42:08] logging.py:157 >> {'loss': 1.4184, 'learning_rate': 4.0884e-07, 'epoch': 2.83}

[INFO|2025-03-20 04:42:23] logging.py:157 >> {'loss': 1.2986, 'learning_rate': 4.0075e-07, 'epoch': 2.83}

[INFO|2025-03-20 04:42:36] logging.py:157 >> {'loss': 1.3778, 'learning_rate': 3.9273e-07, 'epoch': 2.83}

[INFO|2025-03-20 04:42:36] trainer.py:4117 >> 
***** Running Evaluation *****

[INFO|2025-03-20 04:42:36] trainer.py:4119 >>   Num examples = 5152

[INFO|2025-03-20 04:42:36] trainer.py:4122 >>   Batch size = 2

[INFO|2025-03-20 04:47:39] trainer.py:3801 >> Saving model checkpoint to saves/Llama-3.2-3B-Instruct/lora/train_llama_3_2_v3/checkpoint-8200

[INFO|2025-03-20 04:47:39] tokenization_utils_base.py:2646 >> tokenizer config file saved in saves/Llama-3.2-3B-Instruct/lora/train_llama_3_2_v3/checkpoint-8200/tokenizer_config.json

[INFO|2025-03-20 04:47:39] tokenization_utils_base.py:2655 >> Special tokens file saved in saves/Llama-3.2-3B-Instruct/lora/train_llama_3_2_v3/checkpoint-8200/special_tokens_map.json

[INFO|2025-03-20 04:47:55] logging.py:157 >> {'loss': 1.2945, 'learning_rate': 3.8479e-07, 'epoch': 2.83}

[INFO|2025-03-20 04:48:11] logging.py:157 >> {'loss': 1.3216, 'learning_rate': 3.7693e-07, 'epoch': 2.83}

[INFO|2025-03-20 04:48:25] logging.py:157 >> {'loss': 1.3189, 'learning_rate': 3.6916e-07, 'epoch': 2.84}

[INFO|2025-03-20 04:48:40] logging.py:157 >> {'loss': 1.2554, 'learning_rate': 3.6146e-07, 'epoch': 2.84}

[INFO|2025-03-20 04:48:55] logging.py:157 >> {'loss': 1.1747, 'learning_rate': 3.5385e-07, 'epoch': 2.84}

[INFO|2025-03-20 04:49:09] logging.py:157 >> {'loss': 1.2989, 'learning_rate': 3.4631e-07, 'epoch': 2.84}

[INFO|2025-03-20 04:49:23] logging.py:157 >> {'loss': 1.2198, 'learning_rate': 3.3886e-07, 'epoch': 2.84}

[INFO|2025-03-20 04:49:38] logging.py:157 >> {'loss': 1.3091, 'learning_rate': 3.3148e-07, 'epoch': 2.84}

[INFO|2025-03-20 04:49:52] logging.py:157 >> {'loss': 1.3292, 'learning_rate': 3.2419e-07, 'epoch': 2.85}

[INFO|2025-03-20 04:50:07] logging.py:157 >> {'loss': 1.2538, 'learning_rate': 3.1698e-07, 'epoch': 2.85}

[INFO|2025-03-20 04:50:21] logging.py:157 >> {'loss': 1.2452, 'learning_rate': 3.0984e-07, 'epoch': 2.85}

[INFO|2025-03-20 04:50:35] logging.py:157 >> {'loss': 1.3837, 'learning_rate': 3.0279e-07, 'epoch': 2.85}

[INFO|2025-03-20 04:50:51] logging.py:157 >> {'loss': 1.1868, 'learning_rate': 2.9582e-07, 'epoch': 2.85}

[INFO|2025-03-20 04:51:05] logging.py:157 >> {'loss': 1.3161, 'learning_rate': 2.8893e-07, 'epoch': 2.85}

[INFO|2025-03-20 04:51:20] logging.py:157 >> {'loss': 1.3549, 'learning_rate': 2.8212e-07, 'epoch': 2.86}

[INFO|2025-03-20 04:51:36] logging.py:157 >> {'loss': 1.2818, 'learning_rate': 2.7539e-07, 'epoch': 2.86}

[INFO|2025-03-20 04:51:49] logging.py:157 >> {'loss': 1.2560, 'learning_rate': 2.6875e-07, 'epoch': 2.86}

[INFO|2025-03-20 04:52:04] logging.py:157 >> {'loss': 1.2708, 'learning_rate': 2.6218e-07, 'epoch': 2.86}

[INFO|2025-03-20 04:52:19] logging.py:157 >> {'loss': 1.1172, 'learning_rate': 2.5569e-07, 'epoch': 2.86}

[INFO|2025-03-20 04:52:34] logging.py:157 >> {'loss': 1.1425, 'learning_rate': 2.4929e-07, 'epoch': 2.86}

[INFO|2025-03-20 04:52:34] trainer.py:4117 >> 
***** Running Evaluation *****

[INFO|2025-03-20 04:52:34] trainer.py:4119 >>   Num examples = 5152

[INFO|2025-03-20 04:52:34] trainer.py:4122 >>   Batch size = 2

[INFO|2025-03-20 04:57:37] trainer.py:3801 >> Saving model checkpoint to saves/Llama-3.2-3B-Instruct/lora/train_llama_3_2_v3/checkpoint-8300

[INFO|2025-03-20 04:57:37] tokenization_utils_base.py:2646 >> tokenizer config file saved in saves/Llama-3.2-3B-Instruct/lora/train_llama_3_2_v3/checkpoint-8300/tokenizer_config.json

[INFO|2025-03-20 04:57:37] tokenization_utils_base.py:2655 >> Special tokens file saved in saves/Llama-3.2-3B-Instruct/lora/train_llama_3_2_v3/checkpoint-8300/special_tokens_map.json

[INFO|2025-03-20 04:57:52] logging.py:157 >> {'loss': 1.2300, 'learning_rate': 2.4296e-07, 'epoch': 2.87}

[INFO|2025-03-20 04:58:09] logging.py:157 >> {'loss': 1.3739, 'learning_rate': 2.3672e-07, 'epoch': 2.87}

[INFO|2025-03-20 04:58:24] logging.py:157 >> {'loss': 1.2505, 'learning_rate': 2.3056e-07, 'epoch': 2.87}

[INFO|2025-03-20 04:58:40] logging.py:157 >> {'loss': 1.2730, 'learning_rate': 2.2447e-07, 'epoch': 2.87}

[INFO|2025-03-20 04:58:56] logging.py:157 >> {'loss': 1.2715, 'learning_rate': 2.1847e-07, 'epoch': 2.87}

[INFO|2025-03-20 04:59:10] logging.py:157 >> {'loss': 1.2123, 'learning_rate': 2.1255e-07, 'epoch': 2.87}

[INFO|2025-03-20 04:59:24] logging.py:157 >> {'loss': 1.2431, 'learning_rate': 2.0671e-07, 'epoch': 2.88}

[INFO|2025-03-20 04:59:40] logging.py:157 >> {'loss': 1.2402, 'learning_rate': 2.0096e-07, 'epoch': 2.88}

[INFO|2025-03-20 04:59:54] logging.py:157 >> {'loss': 1.1735, 'learning_rate': 1.9528e-07, 'epoch': 2.88}

[INFO|2025-03-20 05:00:10] logging.py:157 >> {'loss': 1.1333, 'learning_rate': 1.8968e-07, 'epoch': 2.88}

[INFO|2025-03-20 05:00:24] logging.py:157 >> {'loss': 1.2308, 'learning_rate': 1.8417e-07, 'epoch': 2.88}

[INFO|2025-03-20 05:00:39] logging.py:157 >> {'loss': 1.2521, 'learning_rate': 1.7873e-07, 'epoch': 2.89}

[INFO|2025-03-20 05:00:54] logging.py:157 >> {'loss': 1.2035, 'learning_rate': 1.7338e-07, 'epoch': 2.89}

[INFO|2025-03-20 05:01:10] logging.py:157 >> {'loss': 1.2397, 'learning_rate': 1.6811e-07, 'epoch': 2.89}

[INFO|2025-03-20 05:01:23] logging.py:157 >> {'loss': 1.2364, 'learning_rate': 1.6292e-07, 'epoch': 2.89}

[INFO|2025-03-20 05:01:39] logging.py:157 >> {'loss': 1.3240, 'learning_rate': 1.5781e-07, 'epoch': 2.89}

[INFO|2025-03-20 05:01:53] logging.py:157 >> {'loss': 1.2859, 'learning_rate': 1.5278e-07, 'epoch': 2.89}

[INFO|2025-03-20 05:02:08] logging.py:157 >> {'loss': 1.2228, 'learning_rate': 1.4783e-07, 'epoch': 2.90}

[INFO|2025-03-20 05:02:23] logging.py:157 >> {'loss': 1.3099, 'learning_rate': 1.4297e-07, 'epoch': 2.90}

[INFO|2025-03-20 05:02:37] logging.py:157 >> {'loss': 1.2659, 'learning_rate': 1.3818e-07, 'epoch': 2.90}

[INFO|2025-03-20 05:02:37] trainer.py:4117 >> 
***** Running Evaluation *****

[INFO|2025-03-20 05:02:37] trainer.py:4119 >>   Num examples = 5152

[INFO|2025-03-20 05:02:37] trainer.py:4122 >>   Batch size = 2

[INFO|2025-03-20 05:07:40] trainer.py:3801 >> Saving model checkpoint to saves/Llama-3.2-3B-Instruct/lora/train_llama_3_2_v3/checkpoint-8400

[INFO|2025-03-20 05:07:40] tokenization_utils_base.py:2646 >> tokenizer config file saved in saves/Llama-3.2-3B-Instruct/lora/train_llama_3_2_v3/checkpoint-8400/tokenizer_config.json

[INFO|2025-03-20 05:07:40] tokenization_utils_base.py:2655 >> Special tokens file saved in saves/Llama-3.2-3B-Instruct/lora/train_llama_3_2_v3/checkpoint-8400/special_tokens_map.json

[INFO|2025-03-20 05:07:54] logging.py:157 >> {'loss': 1.2670, 'learning_rate': 1.3348e-07, 'epoch': 2.90}

[INFO|2025-03-20 05:08:09] logging.py:157 >> {'loss': 1.2542, 'learning_rate': 1.2886e-07, 'epoch': 2.90}

[INFO|2025-03-20 05:08:24] logging.py:157 >> {'loss': 1.4975, 'learning_rate': 1.2432e-07, 'epoch': 2.90}

[INFO|2025-03-20 05:08:39] logging.py:157 >> {'loss': 1.2743, 'learning_rate': 1.1986e-07, 'epoch': 2.91}

[INFO|2025-03-20 05:08:53] logging.py:157 >> {'loss': 1.1793, 'learning_rate': 1.1548e-07, 'epoch': 2.91}

[INFO|2025-03-20 05:09:08] logging.py:157 >> {'loss': 1.1817, 'learning_rate': 1.1118e-07, 'epoch': 2.91}

[INFO|2025-03-20 05:09:23] logging.py:157 >> {'loss': 1.1950, 'learning_rate': 1.0696e-07, 'epoch': 2.91}

[INFO|2025-03-20 05:09:36] logging.py:157 >> {'loss': 1.3073, 'learning_rate': 1.0283e-07, 'epoch': 2.91}

[INFO|2025-03-20 05:09:50] logging.py:157 >> {'loss': 1.2365, 'learning_rate': 9.8777e-08, 'epoch': 2.91}

[INFO|2025-03-20 05:10:06] logging.py:157 >> {'loss': 1.3162, 'learning_rate': 9.4805e-08, 'epoch': 2.92}

[INFO|2025-03-20 05:10:21] logging.py:157 >> {'loss': 1.3502, 'learning_rate': 9.0914e-08, 'epoch': 2.92}

[INFO|2025-03-20 05:10:37] logging.py:157 >> {'loss': 1.3522, 'learning_rate': 8.7105e-08, 'epoch': 2.92}

[INFO|2025-03-20 05:10:52] logging.py:157 >> {'loss': 1.3510, 'learning_rate': 8.3377e-08, 'epoch': 2.92}

[INFO|2025-03-20 05:11:07] logging.py:157 >> {'loss': 1.2393, 'learning_rate': 7.9730e-08, 'epoch': 2.92}

[INFO|2025-03-20 05:11:22] logging.py:157 >> {'loss': 1.3281, 'learning_rate': 7.6165e-08, 'epoch': 2.92}

[INFO|2025-03-20 05:11:36] logging.py:157 >> {'loss': 1.3449, 'learning_rate': 7.2682e-08, 'epoch': 2.93}

[INFO|2025-03-20 05:11:50] logging.py:157 >> {'loss': 1.3053, 'learning_rate': 6.9279e-08, 'epoch': 2.93}

[INFO|2025-03-20 05:12:07] logging.py:157 >> {'loss': 1.3599, 'learning_rate': 6.5959e-08, 'epoch': 2.93}

[INFO|2025-03-20 05:12:20] logging.py:157 >> {'loss': 1.3304, 'learning_rate': 6.2719e-08, 'epoch': 2.93}

[INFO|2025-03-20 05:12:36] logging.py:157 >> {'loss': 1.2662, 'learning_rate': 5.9561e-08, 'epoch': 2.93}

[INFO|2025-03-20 05:12:36] trainer.py:4117 >> 
***** Running Evaluation *****

[INFO|2025-03-20 05:12:36] trainer.py:4119 >>   Num examples = 5152

[INFO|2025-03-20 05:12:36] trainer.py:4122 >>   Batch size = 2

[INFO|2025-03-20 05:17:39] trainer.py:3801 >> Saving model checkpoint to saves/Llama-3.2-3B-Instruct/lora/train_llama_3_2_v3/checkpoint-8500

[INFO|2025-03-20 05:17:39] tokenization_utils_base.py:2646 >> tokenizer config file saved in saves/Llama-3.2-3B-Instruct/lora/train_llama_3_2_v3/checkpoint-8500/tokenizer_config.json

[INFO|2025-03-20 05:17:39] tokenization_utils_base.py:2655 >> Special tokens file saved in saves/Llama-3.2-3B-Instruct/lora/train_llama_3_2_v3/checkpoint-8500/special_tokens_map.json

[INFO|2025-03-20 05:17:53] logging.py:157 >> {'loss': 1.2588, 'learning_rate': 5.6485e-08, 'epoch': 2.94}

[INFO|2025-03-20 05:18:08] logging.py:157 >> {'loss': 1.3009, 'learning_rate': 5.3490e-08, 'epoch': 2.94}

[INFO|2025-03-20 05:18:22] logging.py:157 >> {'loss': 1.3549, 'learning_rate': 5.0577e-08, 'epoch': 2.94}

[INFO|2025-03-20 05:18:37] logging.py:157 >> {'loss': 1.3736, 'learning_rate': 4.7745e-08, 'epoch': 2.94}

[INFO|2025-03-20 05:18:53] logging.py:157 >> {'loss': 1.2908, 'learning_rate': 4.4994e-08, 'epoch': 2.94}

[INFO|2025-03-20 05:19:07] logging.py:157 >> {'loss': 1.2974, 'learning_rate': 4.2325e-08, 'epoch': 2.94}

[INFO|2025-03-20 05:19:21] logging.py:157 >> {'loss': 1.3040, 'learning_rate': 3.9738e-08, 'epoch': 2.95}

[INFO|2025-03-20 05:19:35] logging.py:157 >> {'loss': 1.2502, 'learning_rate': 3.7232e-08, 'epoch': 2.95}

[INFO|2025-03-20 05:19:50] logging.py:157 >> {'loss': 1.4265, 'learning_rate': 3.4808e-08, 'epoch': 2.95}

[INFO|2025-03-20 05:20:05] logging.py:157 >> {'loss': 1.3818, 'learning_rate': 3.2465e-08, 'epoch': 2.95}

[INFO|2025-03-20 05:20:19] logging.py:157 >> {'loss': 1.4131, 'learning_rate': 3.0204e-08, 'epoch': 2.95}

[INFO|2025-03-20 05:20:34] logging.py:157 >> {'loss': 1.4041, 'learning_rate': 2.8024e-08, 'epoch': 2.95}

[INFO|2025-03-20 05:20:48] logging.py:157 >> {'loss': 1.2613, 'learning_rate': 2.5926e-08, 'epoch': 2.96}

[INFO|2025-03-20 05:21:02] logging.py:157 >> {'loss': 1.3918, 'learning_rate': 2.3910e-08, 'epoch': 2.96}

[INFO|2025-03-20 05:21:15] logging.py:157 >> {'loss': 1.3193, 'learning_rate': 2.1975e-08, 'epoch': 2.96}

[INFO|2025-03-20 05:21:31] logging.py:157 >> {'loss': 1.2900, 'learning_rate': 2.0121e-08, 'epoch': 2.96}

[INFO|2025-03-20 05:21:47] logging.py:157 >> {'loss': 1.1608, 'learning_rate': 1.8350e-08, 'epoch': 2.96}

[INFO|2025-03-20 05:22:02] logging.py:157 >> {'loss': 1.3316, 'learning_rate': 1.6660e-08, 'epoch': 2.96}

[INFO|2025-03-20 05:22:17] logging.py:157 >> {'loss': 1.3103, 'learning_rate': 1.5051e-08, 'epoch': 2.97}

[INFO|2025-03-20 05:22:31] logging.py:157 >> {'loss': 1.3609, 'learning_rate': 1.3524e-08, 'epoch': 2.97}

[INFO|2025-03-20 05:22:31] trainer.py:4117 >> 
***** Running Evaluation *****

[INFO|2025-03-20 05:22:31] trainer.py:4119 >>   Num examples = 5152

[INFO|2025-03-20 05:22:31] trainer.py:4122 >>   Batch size = 2

[INFO|2025-03-20 05:27:33] trainer.py:3801 >> Saving model checkpoint to saves/Llama-3.2-3B-Instruct/lora/train_llama_3_2_v3/checkpoint-8600

[INFO|2025-03-20 05:27:33] tokenization_utils_base.py:2646 >> tokenizer config file saved in saves/Llama-3.2-3B-Instruct/lora/train_llama_3_2_v3/checkpoint-8600/tokenizer_config.json

[INFO|2025-03-20 05:27:33] tokenization_utils_base.py:2655 >> Special tokens file saved in saves/Llama-3.2-3B-Instruct/lora/train_llama_3_2_v3/checkpoint-8600/special_tokens_map.json

[INFO|2025-03-20 05:27:48] logging.py:157 >> {'loss': 1.3730, 'learning_rate': 1.2079e-08, 'epoch': 2.97}

[INFO|2025-03-20 05:28:02] logging.py:157 >> {'loss': 1.2092, 'learning_rate': 1.0715e-08, 'epoch': 2.97}

[INFO|2025-03-20 05:28:18] logging.py:157 >> {'loss': 1.3444, 'learning_rate': 9.4334e-09, 'epoch': 2.97}

[INFO|2025-03-20 05:28:33] logging.py:157 >> {'loss': 1.2180, 'learning_rate': 8.2331e-09, 'epoch': 2.97}

[INFO|2025-03-20 05:28:49] logging.py:157 >> {'loss': 1.2718, 'learning_rate': 7.1144e-09, 'epoch': 2.98}

[INFO|2025-03-20 05:29:04] logging.py:157 >> {'loss': 1.3101, 'learning_rate': 6.0773e-09, 'epoch': 2.98}

[INFO|2025-03-20 05:29:21] logging.py:157 >> {'loss': 1.3460, 'learning_rate': 5.1219e-09, 'epoch': 2.98}

[INFO|2025-03-20 05:29:35] logging.py:157 >> {'loss': 1.1318, 'learning_rate': 4.2481e-09, 'epoch': 2.98}

[INFO|2025-03-20 05:29:51] logging.py:157 >> {'loss': 1.1988, 'learning_rate': 3.4560e-09, 'epoch': 2.98}

[INFO|2025-03-20 05:30:06] logging.py:157 >> {'loss': 1.2368, 'learning_rate': 2.7456e-09, 'epoch': 2.99}

[INFO|2025-03-20 05:30:20] logging.py:157 >> {'loss': 1.3579, 'learning_rate': 2.1167e-09, 'epoch': 2.99}

[INFO|2025-03-20 05:30:34] logging.py:157 >> {'loss': 1.3144, 'learning_rate': 1.5696e-09, 'epoch': 2.99}

[INFO|2025-03-20 05:30:51] logging.py:157 >> {'loss': 1.2397, 'learning_rate': 1.1041e-09, 'epoch': 2.99}

[INFO|2025-03-20 05:31:05] logging.py:157 >> {'loss': 1.3182, 'learning_rate': 7.2029e-10, 'epoch': 2.99}

[INFO|2025-03-20 05:31:18] logging.py:157 >> {'loss': 1.2938, 'learning_rate': 4.1813e-10, 'epoch': 2.99}

[INFO|2025-03-20 05:31:32] logging.py:157 >> {'loss': 1.2355, 'learning_rate': 1.9763e-10, 'epoch': 3.00}

[INFO|2025-03-20 05:31:48] logging.py:157 >> {'loss': 1.2438, 'learning_rate': 5.8799e-11, 'epoch': 3.00}

[INFO|2025-03-20 05:32:04] logging.py:157 >> {'loss': 1.2052, 'learning_rate': 1.6333e-12, 'epoch': 3.00}

[INFO|2025-03-20 05:32:07] trainer.py:3801 >> Saving model checkpoint to saves/Llama-3.2-3B-Instruct/lora/train_llama_3_2_v3/checkpoint-8691

[INFO|2025-03-20 05:32:07] tokenization_utils_base.py:2646 >> tokenizer config file saved in saves/Llama-3.2-3B-Instruct/lora/train_llama_3_2_v3/checkpoint-8691/tokenizer_config.json

[INFO|2025-03-20 05:32:07] tokenization_utils_base.py:2655 >> Special tokens file saved in saves/Llama-3.2-3B-Instruct/lora/train_llama_3_2_v3/checkpoint-8691/special_tokens_map.json

[INFO|2025-03-20 05:32:07] trainer.py:2584 >> 

Training completed. Do not forget to share your model on huggingface.co/models =)



[INFO|2025-03-20 05:32:07] trainer.py:3801 >> Saving model checkpoint to saves/Llama-3.2-3B-Instruct/lora/train_llama_3_2_v3

[INFO|2025-03-20 05:32:07] tokenization_utils_base.py:2646 >> tokenizer config file saved in saves/Llama-3.2-3B-Instruct/lora/train_llama_3_2_v3/tokenizer_config.json

[INFO|2025-03-20 05:32:07] tokenization_utils_base.py:2655 >> Special tokens file saved in saves/Llama-3.2-3B-Instruct/lora/train_llama_3_2_v3/special_tokens_map.json

[WARNING|2025-03-20 05:32:08] logging.py:162 >> No metric eval_accuracy to plot.

[INFO|2025-03-20 05:32:08] trainer.py:4117 >> 
***** Running Evaluation *****

[INFO|2025-03-20 05:32:08] trainer.py:4119 >>   Num examples = 5152

[INFO|2025-03-20 05:32:08] trainer.py:4122 >>   Batch size = 2

[INFO|2025-03-20 05:37:10] modelcard.py:449 >> Dropping the following result as it does not have all the necessary fields:
{'task': {'name': 'Causal Language Modeling', 'type': 'text-generation'}}

