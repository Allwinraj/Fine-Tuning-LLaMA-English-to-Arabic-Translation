{
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 2.9994823123382224,
  "eval_steps": 100,
  "global_step": 8691,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.001725625539257981,
      "grad_norm": 1.171879529953003,
      "learning_rate": 4.9999959167107156e-05,
      "loss": 3.0639,
      "step": 5
    },
    {
      "epoch": 0.003451251078515962,
      "grad_norm": 1.1204882860183716,
      "learning_rate": 4.9999836668562015e-05,
      "loss": 2.9133,
      "step": 10
    },
    {
      "epoch": 0.005176876617773943,
      "grad_norm": 1.1098626852035522,
      "learning_rate": 4.999963250476472e-05,
      "loss": 2.698,
      "step": 15
    },
    {
      "epoch": 0.006902502157031924,
      "grad_norm": 1.202887773513794,
      "learning_rate": 4.999934667638223e-05,
      "loss": 2.9149,
      "step": 20
    },
    {
      "epoch": 0.008628127696289905,
      "grad_norm": 0.8373826146125793,
      "learning_rate": 4.9998979184348206e-05,
      "loss": 2.7929,
      "step": 25
    },
    {
      "epoch": 0.010353753235547885,
      "grad_norm": 0.9752951860427856,
      "learning_rate": 4.999853002986313e-05,
      "loss": 2.5564,
      "step": 30
    },
    {
      "epoch": 0.012079378774805867,
      "grad_norm": 0.8685281872749329,
      "learning_rate": 4.999799921439421e-05,
      "loss": 2.6361,
      "step": 35
    },
    {
      "epoch": 0.013805004314063849,
      "grad_norm": 0.8551307916641235,
      "learning_rate": 4.999738673967544e-05,
      "loss": 2.7439,
      "step": 40
    },
    {
      "epoch": 0.015530629853321829,
      "grad_norm": 0.9201048612594604,
      "learning_rate": 4.999669260770754e-05,
      "loss": 2.8151,
      "step": 45
    },
    {
      "epoch": 0.01725625539257981,
      "grad_norm": 1.0082595348358154,
      "learning_rate": 4.9995916820757996e-05,
      "loss": 2.7487,
      "step": 50
    },
    {
      "epoch": 0.01898188093183779,
      "grad_norm": 1.225569248199463,
      "learning_rate": 4.999505938136099e-05,
      "loss": 2.595,
      "step": 55
    },
    {
      "epoch": 0.02070750647109577,
      "grad_norm": 0.8180311918258667,
      "learning_rate": 4.9994120292317485e-05,
      "loss": 2.5964,
      "step": 60
    },
    {
      "epoch": 0.022433132010353754,
      "grad_norm": 1.2826229333877563,
      "learning_rate": 4.999309955669513e-05,
      "loss": 2.3327,
      "step": 65
    },
    {
      "epoch": 0.024158757549611734,
      "grad_norm": 1.105729579925537,
      "learning_rate": 4.9991997177828296e-05,
      "loss": 2.8352,
      "step": 70
    },
    {
      "epoch": 0.025884383088869714,
      "grad_norm": 1.1152763366699219,
      "learning_rate": 4.9990813159318046e-05,
      "loss": 2.5801,
      "step": 75
    },
    {
      "epoch": 0.027610008628127698,
      "grad_norm": 0.9702067971229553,
      "learning_rate": 4.998954750503213e-05,
      "loss": 2.6452,
      "step": 80
    },
    {
      "epoch": 0.029335634167385678,
      "grad_norm": 1.2268390655517578,
      "learning_rate": 4.998820021910498e-05,
      "loss": 2.5326,
      "step": 85
    },
    {
      "epoch": 0.031061259706643658,
      "grad_norm": 1.1395580768585205,
      "learning_rate": 4.9986771305937677e-05,
      "loss": 2.623,
      "step": 90
    },
    {
      "epoch": 0.03278688524590164,
      "grad_norm": 1.0229861736297607,
      "learning_rate": 4.998526077019795e-05,
      "loss": 2.5364,
      "step": 95
    },
    {
      "epoch": 0.03451251078515962,
      "grad_norm": 1.3238660097122192,
      "learning_rate": 4.998366861682017e-05,
      "loss": 2.5154,
      "step": 100
    },
    {
      "epoch": 0.03451251078515962,
      "eval_loss": 2.585365056991577,
      "eval_runtime": 298.21,
      "eval_samples_per_second": 17.276,
      "eval_steps_per_second": 8.638,
      "step": 100
    },
    {
      "epoch": 0.0362381363244176,
      "grad_norm": 1.1390138864517212,
      "learning_rate": 4.998199485100532e-05,
      "loss": 2.4436,
      "step": 105
    },
    {
      "epoch": 0.03796376186367558,
      "grad_norm": 0.950290322303772,
      "learning_rate": 4.998023947822096e-05,
      "loss": 2.699,
      "step": 110
    },
    {
      "epoch": 0.03968938740293356,
      "grad_norm": 1.1728464365005493,
      "learning_rate": 4.997840250420126e-05,
      "loss": 2.5616,
      "step": 115
    },
    {
      "epoch": 0.04141501294219154,
      "grad_norm": 1.246417760848999,
      "learning_rate": 4.997648393494693e-05,
      "loss": 2.5055,
      "step": 120
    },
    {
      "epoch": 0.04314063848144953,
      "grad_norm": 1.0357606410980225,
      "learning_rate": 4.9974483776725236e-05,
      "loss": 2.6821,
      "step": 125
    },
    {
      "epoch": 0.04486626402070751,
      "grad_norm": 1.1253727674484253,
      "learning_rate": 4.9972402036069944e-05,
      "loss": 2.4352,
      "step": 130
    },
    {
      "epoch": 0.04659188955996549,
      "grad_norm": 1.0134752988815308,
      "learning_rate": 4.9970238719781346e-05,
      "loss": 2.3418,
      "step": 135
    },
    {
      "epoch": 0.04831751509922347,
      "grad_norm": 1.0620002746582031,
      "learning_rate": 4.99679938349262e-05,
      "loss": 2.5329,
      "step": 140
    },
    {
      "epoch": 0.05004314063848145,
      "grad_norm": 1.1916712522506714,
      "learning_rate": 4.99656673888377e-05,
      "loss": 2.6425,
      "step": 145
    },
    {
      "epoch": 0.05176876617773943,
      "grad_norm": 1.5718456506729126,
      "learning_rate": 4.9963259389115516e-05,
      "loss": 2.6799,
      "step": 150
    },
    {
      "epoch": 0.05349439171699741,
      "grad_norm": 1.7244681119918823,
      "learning_rate": 4.9960769843625665e-05,
      "loss": 2.4738,
      "step": 155
    },
    {
      "epoch": 0.055220017256255395,
      "grad_norm": 1.2195000648498535,
      "learning_rate": 4.99581987605006e-05,
      "loss": 2.5802,
      "step": 160
    },
    {
      "epoch": 0.056945642795513375,
      "grad_norm": 1.0953749418258667,
      "learning_rate": 4.995554614813909e-05,
      "loss": 2.5372,
      "step": 165
    },
    {
      "epoch": 0.058671268334771355,
      "grad_norm": 1.5183136463165283,
      "learning_rate": 4.9952812015206234e-05,
      "loss": 2.3607,
      "step": 170
    },
    {
      "epoch": 0.060396893874029335,
      "grad_norm": 1.3528670072555542,
      "learning_rate": 4.994999637063346e-05,
      "loss": 2.5214,
      "step": 175
    },
    {
      "epoch": 0.062122519413287315,
      "grad_norm": 1.467327356338501,
      "learning_rate": 4.994709922361841e-05,
      "loss": 2.4176,
      "step": 180
    },
    {
      "epoch": 0.0638481449525453,
      "grad_norm": 1.5685474872589111,
      "learning_rate": 4.994412058362502e-05,
      "loss": 2.565,
      "step": 185
    },
    {
      "epoch": 0.06557377049180328,
      "grad_norm": 1.3131853342056274,
      "learning_rate": 4.9941060460383406e-05,
      "loss": 2.4194,
      "step": 190
    },
    {
      "epoch": 0.06729939603106126,
      "grad_norm": 1.2037630081176758,
      "learning_rate": 4.9937918863889856e-05,
      "loss": 2.1303,
      "step": 195
    },
    {
      "epoch": 0.06902502157031924,
      "grad_norm": 1.0833059549331665,
      "learning_rate": 4.993469580440681e-05,
      "loss": 2.5308,
      "step": 200
    },
    {
      "epoch": 0.06902502157031924,
      "eval_loss": 2.457124710083008,
      "eval_runtime": 298.3978,
      "eval_samples_per_second": 17.266,
      "eval_steps_per_second": 8.633,
      "step": 200
    },
    {
      "epoch": 0.07075064710957722,
      "grad_norm": 1.2390352487564087,
      "learning_rate": 4.993139129246281e-05,
      "loss": 2.4309,
      "step": 205
    },
    {
      "epoch": 0.0724762726488352,
      "grad_norm": 1.4137686491012573,
      "learning_rate": 4.992800533885248e-05,
      "loss": 2.3531,
      "step": 210
    },
    {
      "epoch": 0.07420189818809318,
      "grad_norm": 1.3939151763916016,
      "learning_rate": 4.99245379546365e-05,
      "loss": 2.2896,
      "step": 215
    },
    {
      "epoch": 0.07592752372735116,
      "grad_norm": 1.492080569267273,
      "learning_rate": 4.992098915114152e-05,
      "loss": 2.1973,
      "step": 220
    },
    {
      "epoch": 0.07765314926660914,
      "grad_norm": 1.6030033826828003,
      "learning_rate": 4.991735893996017e-05,
      "loss": 2.269,
      "step": 225
    },
    {
      "epoch": 0.07937877480586712,
      "grad_norm": 1.0596750974655151,
      "learning_rate": 4.991364733295101e-05,
      "loss": 2.3742,
      "step": 230
    },
    {
      "epoch": 0.0811044003451251,
      "grad_norm": 1.2446796894073486,
      "learning_rate": 4.9909854342238515e-05,
      "loss": 2.3458,
      "step": 235
    },
    {
      "epoch": 0.08283002588438308,
      "grad_norm": 1.2864347696304321,
      "learning_rate": 4.990597998021296e-05,
      "loss": 2.3757,
      "step": 240
    },
    {
      "epoch": 0.08455565142364108,
      "grad_norm": 1.1131469011306763,
      "learning_rate": 4.990202425953048e-05,
      "loss": 2.1179,
      "step": 245
    },
    {
      "epoch": 0.08628127696289906,
      "grad_norm": 1.5165259838104248,
      "learning_rate": 4.989798719311294e-05,
      "loss": 2.2831,
      "step": 250
    },
    {
      "epoch": 0.08800690250215704,
      "grad_norm": 1.3661569356918335,
      "learning_rate": 4.9893868794147957e-05,
      "loss": 2.4704,
      "step": 255
    },
    {
      "epoch": 0.08973252804141502,
      "grad_norm": 1.2325431108474731,
      "learning_rate": 4.988966907608882e-05,
      "loss": 2.3346,
      "step": 260
    },
    {
      "epoch": 0.091458153580673,
      "grad_norm": 1.1636569499969482,
      "learning_rate": 4.988538805265446e-05,
      "loss": 2.486,
      "step": 265
    },
    {
      "epoch": 0.09318377911993098,
      "grad_norm": 1.3245099782943726,
      "learning_rate": 4.98810257378294e-05,
      "loss": 2.531,
      "step": 270
    },
    {
      "epoch": 0.09490940465918896,
      "grad_norm": 1.4259451627731323,
      "learning_rate": 4.987658214586373e-05,
      "loss": 2.4333,
      "step": 275
    },
    {
      "epoch": 0.09663503019844694,
      "grad_norm": 1.239599585533142,
      "learning_rate": 4.9872057291273e-05,
      "loss": 2.3257,
      "step": 280
    },
    {
      "epoch": 0.09836065573770492,
      "grad_norm": 1.4054960012435913,
      "learning_rate": 4.9867451188838266e-05,
      "loss": 2.2532,
      "step": 285
    },
    {
      "epoch": 0.1000862812769629,
      "grad_norm": 1.3107566833496094,
      "learning_rate": 4.986276385360595e-05,
      "loss": 2.4443,
      "step": 290
    },
    {
      "epoch": 0.10181190681622088,
      "grad_norm": 1.0942374467849731,
      "learning_rate": 4.9857995300887864e-05,
      "loss": 2.0684,
      "step": 295
    },
    {
      "epoch": 0.10353753235547886,
      "grad_norm": 1.346616268157959,
      "learning_rate": 4.985314554626109e-05,
      "loss": 2.4777,
      "step": 300
    },
    {
      "epoch": 0.10353753235547886,
      "eval_loss": 2.3707845211029053,
      "eval_runtime": 298.9363,
      "eval_samples_per_second": 17.234,
      "eval_steps_per_second": 8.617,
      "step": 300
    },
    {
      "epoch": 0.10526315789473684,
      "grad_norm": 1.400611162185669,
      "learning_rate": 4.9848214605568025e-05,
      "loss": 2.3342,
      "step": 305
    },
    {
      "epoch": 0.10698878343399482,
      "grad_norm": 1.326123833656311,
      "learning_rate": 4.98432024949162e-05,
      "loss": 2.4897,
      "step": 310
    },
    {
      "epoch": 0.10871440897325281,
      "grad_norm": 1.2778810262680054,
      "learning_rate": 4.983810923067834e-05,
      "loss": 2.3988,
      "step": 315
    },
    {
      "epoch": 0.11044003451251079,
      "grad_norm": 1.267640471458435,
      "learning_rate": 4.9832934829492286e-05,
      "loss": 2.4035,
      "step": 320
    },
    {
      "epoch": 0.11216566005176877,
      "grad_norm": 1.6156353950500488,
      "learning_rate": 4.982767930826086e-05,
      "loss": 2.4272,
      "step": 325
    },
    {
      "epoch": 0.11389128559102675,
      "grad_norm": 1.365553855895996,
      "learning_rate": 4.9822342684151954e-05,
      "loss": 2.4074,
      "step": 330
    },
    {
      "epoch": 0.11561691113028473,
      "grad_norm": 1.23431396484375,
      "learning_rate": 4.9816924974598325e-05,
      "loss": 2.3908,
      "step": 335
    },
    {
      "epoch": 0.11734253666954271,
      "grad_norm": 1.7175791263580322,
      "learning_rate": 4.981142619729765e-05,
      "loss": 2.2674,
      "step": 340
    },
    {
      "epoch": 0.11906816220880069,
      "grad_norm": 1.648626685142517,
      "learning_rate": 4.980584637021239e-05,
      "loss": 2.3568,
      "step": 345
    },
    {
      "epoch": 0.12079378774805867,
      "grad_norm": 1.3537479639053345,
      "learning_rate": 4.9800185511569794e-05,
      "loss": 2.2729,
      "step": 350
    },
    {
      "epoch": 0.12251941328731665,
      "grad_norm": 1.33171546459198,
      "learning_rate": 4.97944436398618e-05,
      "loss": 2.0944,
      "step": 355
    },
    {
      "epoch": 0.12424503882657463,
      "grad_norm": 1.4572689533233643,
      "learning_rate": 4.9788620773844985e-05,
      "loss": 2.3943,
      "step": 360
    },
    {
      "epoch": 0.12597066436583262,
      "grad_norm": 1.3906233310699463,
      "learning_rate": 4.978271693254051e-05,
      "loss": 2.2141,
      "step": 365
    },
    {
      "epoch": 0.1276962899050906,
      "grad_norm": 1.7860262393951416,
      "learning_rate": 4.9776732135234036e-05,
      "loss": 2.2272,
      "step": 370
    },
    {
      "epoch": 0.12942191544434858,
      "grad_norm": 1.5309275388717651,
      "learning_rate": 4.977066640147571e-05,
      "loss": 2.1491,
      "step": 375
    },
    {
      "epoch": 0.13114754098360656,
      "grad_norm": 1.488477349281311,
      "learning_rate": 4.976451975108003e-05,
      "loss": 2.341,
      "step": 380
    },
    {
      "epoch": 0.13287316652286454,
      "grad_norm": 2.043149948120117,
      "learning_rate": 4.9758292204125846e-05,
      "loss": 2.1661,
      "step": 385
    },
    {
      "epoch": 0.13459879206212252,
      "grad_norm": 1.5204994678497314,
      "learning_rate": 4.975198378095626e-05,
      "loss": 2.2555,
      "step": 390
    },
    {
      "epoch": 0.1363244176013805,
      "grad_norm": 1.5745229721069336,
      "learning_rate": 4.9745594502178564e-05,
      "loss": 2.3371,
      "step": 395
    },
    {
      "epoch": 0.13805004314063848,
      "grad_norm": 1.4878687858581543,
      "learning_rate": 4.9739124388664183e-05,
      "loss": 2.3957,
      "step": 400
    },
    {
      "epoch": 0.13805004314063848,
      "eval_loss": 2.3136448860168457,
      "eval_runtime": 298.509,
      "eval_samples_per_second": 17.259,
      "eval_steps_per_second": 8.63,
      "step": 400
    },
    {
      "epoch": 0.13977566867989646,
      "grad_norm": 1.3731036186218262,
      "learning_rate": 4.973257346154857e-05,
      "loss": 2.3218,
      "step": 405
    },
    {
      "epoch": 0.14150129421915444,
      "grad_norm": 1.6308315992355347,
      "learning_rate": 4.972594174223122e-05,
      "loss": 2.2238,
      "step": 410
    },
    {
      "epoch": 0.14322691975841242,
      "grad_norm": 1.5282137393951416,
      "learning_rate": 4.97192292523755e-05,
      "loss": 2.3566,
      "step": 415
    },
    {
      "epoch": 0.1449525452976704,
      "grad_norm": 1.4284656047821045,
      "learning_rate": 4.9712436013908634e-05,
      "loss": 2.1779,
      "step": 420
    },
    {
      "epoch": 0.14667817083692838,
      "grad_norm": 1.432648777961731,
      "learning_rate": 4.9705562049021635e-05,
      "loss": 2.0932,
      "step": 425
    },
    {
      "epoch": 0.14840379637618636,
      "grad_norm": 1.0987752676010132,
      "learning_rate": 4.969860738016922e-05,
      "loss": 2.1759,
      "step": 430
    },
    {
      "epoch": 0.15012942191544434,
      "grad_norm": 1.4856981039047241,
      "learning_rate": 4.9691572030069724e-05,
      "loss": 2.1634,
      "step": 435
    },
    {
      "epoch": 0.15185504745470232,
      "grad_norm": 1.3907802104949951,
      "learning_rate": 4.968445602170504e-05,
      "loss": 2.2092,
      "step": 440
    },
    {
      "epoch": 0.1535806729939603,
      "grad_norm": 1.6158409118652344,
      "learning_rate": 4.967725937832054e-05,
      "loss": 2.4238,
      "step": 445
    },
    {
      "epoch": 0.15530629853321828,
      "grad_norm": 1.4527686834335327,
      "learning_rate": 4.966998212342502e-05,
      "loss": 2.0791,
      "step": 450
    },
    {
      "epoch": 0.15703192407247626,
      "grad_norm": 1.6499561071395874,
      "learning_rate": 4.966262428079057e-05,
      "loss": 2.3626,
      "step": 455
    },
    {
      "epoch": 0.15875754961173424,
      "grad_norm": 1.4204705953598022,
      "learning_rate": 4.965518587445257e-05,
      "loss": 2.2533,
      "step": 460
    },
    {
      "epoch": 0.16048317515099222,
      "grad_norm": 1.3667917251586914,
      "learning_rate": 4.964766692870954e-05,
      "loss": 2.2541,
      "step": 465
    },
    {
      "epoch": 0.1622088006902502,
      "grad_norm": 1.5079591274261475,
      "learning_rate": 4.964006746812311e-05,
      "loss": 2.1631,
      "step": 470
    },
    {
      "epoch": 0.16393442622950818,
      "grad_norm": 1.6768440008163452,
      "learning_rate": 4.963238751751791e-05,
      "loss": 2.2108,
      "step": 475
    },
    {
      "epoch": 0.16566005176876616,
      "grad_norm": 1.686747431755066,
      "learning_rate": 4.962462710198151e-05,
      "loss": 2.2916,
      "step": 480
    },
    {
      "epoch": 0.16738567730802417,
      "grad_norm": 1.5553113222122192,
      "learning_rate": 4.9616786246864335e-05,
      "loss": 2.1684,
      "step": 485
    },
    {
      "epoch": 0.16911130284728215,
      "grad_norm": 1.4523143768310547,
      "learning_rate": 4.960886497777956e-05,
      "loss": 2.3523,
      "step": 490
    },
    {
      "epoch": 0.17083692838654013,
      "grad_norm": 1.6090010404586792,
      "learning_rate": 4.9600863320603055e-05,
      "loss": 2.3186,
      "step": 495
    },
    {
      "epoch": 0.1725625539257981,
      "grad_norm": 1.40208899974823,
      "learning_rate": 4.959278130147328e-05,
      "loss": 2.3156,
      "step": 500
    },
    {
      "epoch": 0.1725625539257981,
      "eval_loss": 2.2639997005462646,
      "eval_runtime": 298.9421,
      "eval_samples_per_second": 17.234,
      "eval_steps_per_second": 8.617,
      "step": 500
    },
    {
      "epoch": 0.1742881794650561,
      "grad_norm": 1.5821549892425537,
      "learning_rate": 4.958461894679122e-05,
      "loss": 2.4019,
      "step": 505
    },
    {
      "epoch": 0.17601380500431407,
      "grad_norm": 1.6521435976028442,
      "learning_rate": 4.9576376283220275e-05,
      "loss": 2.1778,
      "step": 510
    },
    {
      "epoch": 0.17773943054357205,
      "grad_norm": 1.731370210647583,
      "learning_rate": 4.9568053337686194e-05,
      "loss": 2.4433,
      "step": 515
    },
    {
      "epoch": 0.17946505608283003,
      "grad_norm": 1.7494231462478638,
      "learning_rate": 4.955965013737697e-05,
      "loss": 2.3352,
      "step": 520
    },
    {
      "epoch": 0.181190681622088,
      "grad_norm": 1.690248727798462,
      "learning_rate": 4.955116670974276e-05,
      "loss": 2.4199,
      "step": 525
    },
    {
      "epoch": 0.182916307161346,
      "grad_norm": 1.9150863885879517,
      "learning_rate": 4.954260308249579e-05,
      "loss": 2.1983,
      "step": 530
    },
    {
      "epoch": 0.18464193270060397,
      "grad_norm": 2.171036958694458,
      "learning_rate": 4.953395928361028e-05,
      "loss": 2.0496,
      "step": 535
    },
    {
      "epoch": 0.18636755823986195,
      "grad_norm": 1.9233424663543701,
      "learning_rate": 4.9525235341322335e-05,
      "loss": 2.1684,
      "step": 540
    },
    {
      "epoch": 0.18809318377911993,
      "grad_norm": 1.6034351587295532,
      "learning_rate": 4.951643128412986e-05,
      "loss": 2.3501,
      "step": 545
    },
    {
      "epoch": 0.1898188093183779,
      "grad_norm": 1.39032781124115,
      "learning_rate": 4.950754714079246e-05,
      "loss": 2.1182,
      "step": 550
    },
    {
      "epoch": 0.1915444348576359,
      "grad_norm": 1.9623140096664429,
      "learning_rate": 4.949858294033136e-05,
      "loss": 2.2475,
      "step": 555
    },
    {
      "epoch": 0.19327006039689387,
      "grad_norm": 1.7478488683700562,
      "learning_rate": 4.9489538712029304e-05,
      "loss": 2.2725,
      "step": 560
    },
    {
      "epoch": 0.19499568593615185,
      "grad_norm": 1.4059470891952515,
      "learning_rate": 4.948041448543045e-05,
      "loss": 2.1812,
      "step": 565
    },
    {
      "epoch": 0.19672131147540983,
      "grad_norm": 1.5507289171218872,
      "learning_rate": 4.947121029034027e-05,
      "loss": 2.1325,
      "step": 570
    },
    {
      "epoch": 0.1984469370146678,
      "grad_norm": 1.7474100589752197,
      "learning_rate": 4.946192615682549e-05,
      "loss": 2.2281,
      "step": 575
    },
    {
      "epoch": 0.2001725625539258,
      "grad_norm": 1.3968349695205688,
      "learning_rate": 4.9452562115213954e-05,
      "loss": 2.3448,
      "step": 580
    },
    {
      "epoch": 0.20189818809318377,
      "grad_norm": 1.4363501071929932,
      "learning_rate": 4.944311819609453e-05,
      "loss": 2.3067,
      "step": 585
    },
    {
      "epoch": 0.20362381363244175,
      "grad_norm": 1.3049367666244507,
      "learning_rate": 4.943359443031702e-05,
      "loss": 2.0494,
      "step": 590
    },
    {
      "epoch": 0.20534943917169973,
      "grad_norm": 1.6441020965576172,
      "learning_rate": 4.9423990848992066e-05,
      "loss": 2.1319,
      "step": 595
    },
    {
      "epoch": 0.2070750647109577,
      "grad_norm": 1.755426049232483,
      "learning_rate": 4.9414307483491005e-05,
      "loss": 2.2053,
      "step": 600
    },
    {
      "epoch": 0.2070750647109577,
      "eval_loss": 2.2295775413513184,
      "eval_runtime": 297.5907,
      "eval_samples_per_second": 17.312,
      "eval_steps_per_second": 8.656,
      "step": 600
    },
    {
      "epoch": 0.2088006902502157,
      "grad_norm": 1.8523564338684082,
      "learning_rate": 4.940454436544585e-05,
      "loss": 2.1068,
      "step": 605
    },
    {
      "epoch": 0.21052631578947367,
      "grad_norm": 1.757505178451538,
      "learning_rate": 4.93947015267491e-05,
      "loss": 2.4698,
      "step": 610
    },
    {
      "epoch": 0.21225194132873165,
      "grad_norm": 1.5350556373596191,
      "learning_rate": 4.938477899955367e-05,
      "loss": 2.1859,
      "step": 615
    },
    {
      "epoch": 0.21397756686798963,
      "grad_norm": 1.6293895244598389,
      "learning_rate": 4.937477681627282e-05,
      "loss": 2.2248,
      "step": 620
    },
    {
      "epoch": 0.21570319240724764,
      "grad_norm": 1.8347748517990112,
      "learning_rate": 4.9364695009579975e-05,
      "loss": 2.1314,
      "step": 625
    },
    {
      "epoch": 0.21742881794650562,
      "grad_norm": 1.8480342626571655,
      "learning_rate": 4.935453361240869e-05,
      "loss": 2.1414,
      "step": 630
    },
    {
      "epoch": 0.2191544434857636,
      "grad_norm": 1.755234956741333,
      "learning_rate": 4.934429265795252e-05,
      "loss": 2.2937,
      "step": 635
    },
    {
      "epoch": 0.22088006902502158,
      "grad_norm": 1.6302249431610107,
      "learning_rate": 4.933397217966486e-05,
      "loss": 2.1937,
      "step": 640
    },
    {
      "epoch": 0.22260569456427956,
      "grad_norm": 1.772567868232727,
      "learning_rate": 4.932357221125893e-05,
      "loss": 2.0589,
      "step": 645
    },
    {
      "epoch": 0.22433132010353754,
      "grad_norm": 1.720592737197876,
      "learning_rate": 4.9313092786707585e-05,
      "loss": 2.3008,
      "step": 650
    },
    {
      "epoch": 0.22605694564279552,
      "grad_norm": 1.6820414066314697,
      "learning_rate": 4.9302533940243246e-05,
      "loss": 2.1266,
      "step": 655
    },
    {
      "epoch": 0.2277825711820535,
      "grad_norm": 1.5678553581237793,
      "learning_rate": 4.9291895706357764e-05,
      "loss": 2.1829,
      "step": 660
    },
    {
      "epoch": 0.22950819672131148,
      "grad_norm": 1.780059814453125,
      "learning_rate": 4.928117811980234e-05,
      "loss": 2.1902,
      "step": 665
    },
    {
      "epoch": 0.23123382226056946,
      "grad_norm": 1.624338984489441,
      "learning_rate": 4.927038121558738e-05,
      "loss": 2.0872,
      "step": 670
    },
    {
      "epoch": 0.23295944779982744,
      "grad_norm": 1.5947136878967285,
      "learning_rate": 4.9259505028982376e-05,
      "loss": 2.1579,
      "step": 675
    },
    {
      "epoch": 0.23468507333908542,
      "grad_norm": 1.70228910446167,
      "learning_rate": 4.924854959551583e-05,
      "loss": 2.2228,
      "step": 680
    },
    {
      "epoch": 0.2364106988783434,
      "grad_norm": 1.5770936012268066,
      "learning_rate": 4.923751495097511e-05,
      "loss": 2.0935,
      "step": 685
    },
    {
      "epoch": 0.23813632441760138,
      "grad_norm": 2.113193988800049,
      "learning_rate": 4.922640113140632e-05,
      "loss": 2.1655,
      "step": 690
    },
    {
      "epoch": 0.23986194995685936,
      "grad_norm": 1.4440374374389648,
      "learning_rate": 4.921520817311423e-05,
      "loss": 1.9711,
      "step": 695
    },
    {
      "epoch": 0.24158757549611734,
      "grad_norm": 1.480539083480835,
      "learning_rate": 4.9203936112662094e-05,
      "loss": 2.2898,
      "step": 700
    },
    {
      "epoch": 0.24158757549611734,
      "eval_loss": 2.191189765930176,
      "eval_runtime": 297.5655,
      "eval_samples_per_second": 17.314,
      "eval_steps_per_second": 8.657,
      "step": 700
    },
    {
      "epoch": 0.24331320103537532,
      "grad_norm": 1.6065410375595093,
      "learning_rate": 4.919258498687158e-05,
      "loss": 2.2466,
      "step": 705
    },
    {
      "epoch": 0.2450388265746333,
      "grad_norm": 1.5341306924819946,
      "learning_rate": 4.9181154832822645e-05,
      "loss": 2.096,
      "step": 710
    },
    {
      "epoch": 0.24676445211389128,
      "grad_norm": 1.658954381942749,
      "learning_rate": 4.916964568785338e-05,
      "loss": 2.0803,
      "step": 715
    },
    {
      "epoch": 0.24849007765314926,
      "grad_norm": 1.736007571220398,
      "learning_rate": 4.915805758955991e-05,
      "loss": 2.221,
      "step": 720
    },
    {
      "epoch": 0.25021570319240727,
      "grad_norm": 1.6981269121170044,
      "learning_rate": 4.914639057579631e-05,
      "loss": 2.2833,
      "step": 725
    },
    {
      "epoch": 0.25194132873166525,
      "grad_norm": 1.6472980976104736,
      "learning_rate": 4.913464468467438e-05,
      "loss": 2.3213,
      "step": 730
    },
    {
      "epoch": 0.25366695427092323,
      "grad_norm": 1.8021278381347656,
      "learning_rate": 4.912281995456364e-05,
      "loss": 2.2672,
      "step": 735
    },
    {
      "epoch": 0.2553925798101812,
      "grad_norm": 1.7590833902359009,
      "learning_rate": 4.9110916424091116e-05,
      "loss": 2.0955,
      "step": 740
    },
    {
      "epoch": 0.2571182053494392,
      "grad_norm": 2.0475499629974365,
      "learning_rate": 4.909893413214125e-05,
      "loss": 2.2688,
      "step": 745
    },
    {
      "epoch": 0.25884383088869717,
      "grad_norm": 1.505622386932373,
      "learning_rate": 4.9086873117855784e-05,
      "loss": 2.1893,
      "step": 750
    },
    {
      "epoch": 0.26056945642795515,
      "grad_norm": 1.7035466432571411,
      "learning_rate": 4.9074733420633604e-05,
      "loss": 2.0338,
      "step": 755
    },
    {
      "epoch": 0.26229508196721313,
      "grad_norm": 1.6571747064590454,
      "learning_rate": 4.906251508013062e-05,
      "loss": 2.057,
      "step": 760
    },
    {
      "epoch": 0.2640207075064711,
      "grad_norm": 1.770891547203064,
      "learning_rate": 4.9050218136259665e-05,
      "loss": 2.0493,
      "step": 765
    },
    {
      "epoch": 0.2657463330457291,
      "grad_norm": 1.7919869422912598,
      "learning_rate": 4.903784262919029e-05,
      "loss": 2.2591,
      "step": 770
    },
    {
      "epoch": 0.26747195858498707,
      "grad_norm": 1.891771674156189,
      "learning_rate": 4.9025388599348744e-05,
      "loss": 2.0581,
      "step": 775
    },
    {
      "epoch": 0.26919758412424505,
      "grad_norm": 2.0812430381774902,
      "learning_rate": 4.901285608741775e-05,
      "loss": 2.2549,
      "step": 780
    },
    {
      "epoch": 0.27092320966350303,
      "grad_norm": 1.6045522689819336,
      "learning_rate": 4.9000245134336385e-05,
      "loss": 2.114,
      "step": 785
    },
    {
      "epoch": 0.272648835202761,
      "grad_norm": 1.5223075151443481,
      "learning_rate": 4.89875557813e-05,
      "loss": 2.3829,
      "step": 790
    },
    {
      "epoch": 0.274374460742019,
      "grad_norm": 1.6016426086425781,
      "learning_rate": 4.897478806976004e-05,
      "loss": 2.0212,
      "step": 795
    },
    {
      "epoch": 0.27610008628127697,
      "grad_norm": 2.1308770179748535,
      "learning_rate": 4.896194204142389e-05,
      "loss": 2.1766,
      "step": 800
    },
    {
      "epoch": 0.27610008628127697,
      "eval_loss": 2.1627941131591797,
      "eval_runtime": 298.0971,
      "eval_samples_per_second": 17.283,
      "eval_steps_per_second": 8.641,
      "step": 800
    },
    {
      "epoch": 0.27782571182053495,
      "grad_norm": 1.7696852684020996,
      "learning_rate": 4.894901773825482e-05,
      "loss": 2.1126,
      "step": 805
    },
    {
      "epoch": 0.27955133735979293,
      "grad_norm": 1.7832732200622559,
      "learning_rate": 4.893601520247174e-05,
      "loss": 2.2462,
      "step": 810
    },
    {
      "epoch": 0.2812769628990509,
      "grad_norm": 1.739332914352417,
      "learning_rate": 4.892293447654915e-05,
      "loss": 2.1222,
      "step": 815
    },
    {
      "epoch": 0.2830025884383089,
      "grad_norm": 2.1342945098876953,
      "learning_rate": 4.890977560321697e-05,
      "loss": 2.0995,
      "step": 820
    },
    {
      "epoch": 0.28472821397756687,
      "grad_norm": 1.9025094509124756,
      "learning_rate": 4.8896538625460376e-05,
      "loss": 2.1781,
      "step": 825
    },
    {
      "epoch": 0.28645383951682485,
      "grad_norm": 1.895943522453308,
      "learning_rate": 4.88832235865197e-05,
      "loss": 2.1221,
      "step": 830
    },
    {
      "epoch": 0.28817946505608283,
      "grad_norm": 1.836187481880188,
      "learning_rate": 4.886983052989027e-05,
      "loss": 2.1311,
      "step": 835
    },
    {
      "epoch": 0.2899050905953408,
      "grad_norm": 2.035156726837158,
      "learning_rate": 4.8856359499322266e-05,
      "loss": 1.9454,
      "step": 840
    },
    {
      "epoch": 0.2916307161345988,
      "grad_norm": 1.8809115886688232,
      "learning_rate": 4.884281053882057e-05,
      "loss": 2.1065,
      "step": 845
    },
    {
      "epoch": 0.29335634167385677,
      "grad_norm": 1.6701077222824097,
      "learning_rate": 4.8829183692644655e-05,
      "loss": 2.1358,
      "step": 850
    },
    {
      "epoch": 0.29508196721311475,
      "grad_norm": 1.8634690046310425,
      "learning_rate": 4.881547900530839e-05,
      "loss": 2.1951,
      "step": 855
    },
    {
      "epoch": 0.29680759275237273,
      "grad_norm": 1.6474437713623047,
      "learning_rate": 4.880169652157996e-05,
      "loss": 2.0331,
      "step": 860
    },
    {
      "epoch": 0.2985332182916307,
      "grad_norm": 2.1215007305145264,
      "learning_rate": 4.878783628648164e-05,
      "loss": 2.2122,
      "step": 865
    },
    {
      "epoch": 0.3002588438308887,
      "grad_norm": 1.9157708883285522,
      "learning_rate": 4.8773898345289707e-05,
      "loss": 2.036,
      "step": 870
    },
    {
      "epoch": 0.30198446937014667,
      "grad_norm": 2.283205509185791,
      "learning_rate": 4.8759882743534294e-05,
      "loss": 2.1637,
      "step": 875
    },
    {
      "epoch": 0.30371009490940465,
      "grad_norm": 1.493594765663147,
      "learning_rate": 4.87457895269992e-05,
      "loss": 2.1569,
      "step": 880
    },
    {
      "epoch": 0.30543572044866263,
      "grad_norm": 1.6503015756607056,
      "learning_rate": 4.873161874172176e-05,
      "loss": 2.2446,
      "step": 885
    },
    {
      "epoch": 0.3071613459879206,
      "grad_norm": 2.4047868251800537,
      "learning_rate": 4.871737043399272e-05,
      "loss": 2.2542,
      "step": 890
    },
    {
      "epoch": 0.3088869715271786,
      "grad_norm": 2.059051990509033,
      "learning_rate": 4.870304465035604e-05,
      "loss": 2.0549,
      "step": 895
    },
    {
      "epoch": 0.31061259706643657,
      "grad_norm": 2.2624361515045166,
      "learning_rate": 4.868864143760878e-05,
      "loss": 2.044,
      "step": 900
    },
    {
      "epoch": 0.31061259706643657,
      "eval_loss": 2.1374049186706543,
      "eval_runtime": 299.2851,
      "eval_samples_per_second": 17.214,
      "eval_steps_per_second": 8.607,
      "step": 900
    },
    {
      "epoch": 0.31233822260569455,
      "grad_norm": 2.3494479656219482,
      "learning_rate": 4.867416084280093e-05,
      "loss": 2.1711,
      "step": 905
    },
    {
      "epoch": 0.31406384814495253,
      "grad_norm": 2.080193042755127,
      "learning_rate": 4.865960291323525e-05,
      "loss": 2.0948,
      "step": 910
    },
    {
      "epoch": 0.3157894736842105,
      "grad_norm": 1.5302717685699463,
      "learning_rate": 4.864496769646713e-05,
      "loss": 2.0326,
      "step": 915
    },
    {
      "epoch": 0.3175150992234685,
      "grad_norm": 2.017754316329956,
      "learning_rate": 4.863025524030444e-05,
      "loss": 2.0287,
      "step": 920
    },
    {
      "epoch": 0.31924072476272647,
      "grad_norm": 1.9387108087539673,
      "learning_rate": 4.861546559280733e-05,
      "loss": 2.0706,
      "step": 925
    },
    {
      "epoch": 0.32096635030198445,
      "grad_norm": 2.3518857955932617,
      "learning_rate": 4.8600598802288145e-05,
      "loss": 2.0982,
      "step": 930
    },
    {
      "epoch": 0.32269197584124243,
      "grad_norm": 1.8500474691390991,
      "learning_rate": 4.858565491731121e-05,
      "loss": 1.9799,
      "step": 935
    },
    {
      "epoch": 0.3244176013805004,
      "grad_norm": 1.7568647861480713,
      "learning_rate": 4.8570633986692684e-05,
      "loss": 2.1802,
      "step": 940
    },
    {
      "epoch": 0.3261432269197584,
      "grad_norm": 1.9889591932296753,
      "learning_rate": 4.85555360595004e-05,
      "loss": 1.9173,
      "step": 945
    },
    {
      "epoch": 0.32786885245901637,
      "grad_norm": 1.8929383754730225,
      "learning_rate": 4.854036118505375e-05,
      "loss": 2.3385,
      "step": 950
    },
    {
      "epoch": 0.32959447799827435,
      "grad_norm": 1.5897283554077148,
      "learning_rate": 4.852510941292343e-05,
      "loss": 1.989,
      "step": 955
    },
    {
      "epoch": 0.33132010353753233,
      "grad_norm": 1.6889313459396362,
      "learning_rate": 4.8509780792931374e-05,
      "loss": 2.0081,
      "step": 960
    },
    {
      "epoch": 0.3330457290767903,
      "grad_norm": 1.7437189817428589,
      "learning_rate": 4.8494375375150526e-05,
      "loss": 2.0374,
      "step": 965
    },
    {
      "epoch": 0.33477135461604834,
      "grad_norm": 1.7107973098754883,
      "learning_rate": 4.847889320990471e-05,
      "loss": 2.1058,
      "step": 970
    },
    {
      "epoch": 0.3364969801553063,
      "grad_norm": 1.8482376337051392,
      "learning_rate": 4.846333434776844e-05,
      "loss": 2.2172,
      "step": 975
    },
    {
      "epoch": 0.3382226056945643,
      "grad_norm": 1.9361377954483032,
      "learning_rate": 4.8447698839566814e-05,
      "loss": 2.0587,
      "step": 980
    },
    {
      "epoch": 0.3399482312338223,
      "grad_norm": 1.6789389848709106,
      "learning_rate": 4.843198673637524e-05,
      "loss": 2.1366,
      "step": 985
    },
    {
      "epoch": 0.34167385677308026,
      "grad_norm": 2.07148814201355,
      "learning_rate": 4.84161980895194e-05,
      "loss": 2.0369,
      "step": 990
    },
    {
      "epoch": 0.34339948231233824,
      "grad_norm": 1.7525694370269775,
      "learning_rate": 4.840033295057496e-05,
      "loss": 2.3653,
      "step": 995
    },
    {
      "epoch": 0.3451251078515962,
      "grad_norm": 1.7752684354782104,
      "learning_rate": 4.83843913713675e-05,
      "loss": 2.1752,
      "step": 1000
    },
    {
      "epoch": 0.3451251078515962,
      "eval_loss": 2.110550880432129,
      "eval_runtime": 301.597,
      "eval_samples_per_second": 17.082,
      "eval_steps_per_second": 8.541,
      "step": 1000
    },
    {
      "epoch": 0.3468507333908542,
      "grad_norm": 2.036938428878784,
      "learning_rate": 4.836837340397227e-05,
      "loss": 2.1316,
      "step": 1005
    },
    {
      "epoch": 0.3485763589301122,
      "grad_norm": 2.4628772735595703,
      "learning_rate": 4.835227910071406e-05,
      "loss": 2.0874,
      "step": 1010
    },
    {
      "epoch": 0.35030198446937016,
      "grad_norm": 1.6664172410964966,
      "learning_rate": 4.833610851416705e-05,
      "loss": 1.9885,
      "step": 1015
    },
    {
      "epoch": 0.35202761000862814,
      "grad_norm": 2.0503649711608887,
      "learning_rate": 4.8319861697154565e-05,
      "loss": 1.8906,
      "step": 1020
    },
    {
      "epoch": 0.3537532355478861,
      "grad_norm": 2.018519401550293,
      "learning_rate": 4.830353870274898e-05,
      "loss": 2.0319,
      "step": 1025
    },
    {
      "epoch": 0.3554788610871441,
      "grad_norm": 2.248966693878174,
      "learning_rate": 4.82871395842715e-05,
      "loss": 2.1344,
      "step": 1030
    },
    {
      "epoch": 0.3572044866264021,
      "grad_norm": 1.5493916273117065,
      "learning_rate": 4.8270664395292e-05,
      "loss": 2.1988,
      "step": 1035
    },
    {
      "epoch": 0.35893011216566006,
      "grad_norm": 2.4241743087768555,
      "learning_rate": 4.8254113189628835e-05,
      "loss": 2.1588,
      "step": 1040
    },
    {
      "epoch": 0.36065573770491804,
      "grad_norm": 1.8321337699890137,
      "learning_rate": 4.823748602134872e-05,
      "loss": 1.9679,
      "step": 1045
    },
    {
      "epoch": 0.362381363244176,
      "grad_norm": 1.608406901359558,
      "learning_rate": 4.822078294476647e-05,
      "loss": 2.0148,
      "step": 1050
    },
    {
      "epoch": 0.364106988783434,
      "grad_norm": 1.9559013843536377,
      "learning_rate": 4.820400401444488e-05,
      "loss": 2.1805,
      "step": 1055
    },
    {
      "epoch": 0.365832614322692,
      "grad_norm": 1.8080440759658813,
      "learning_rate": 4.8187149285194535e-05,
      "loss": 2.0601,
      "step": 1060
    },
    {
      "epoch": 0.36755823986194996,
      "grad_norm": 1.7483359575271606,
      "learning_rate": 4.817021881207362e-05,
      "loss": 1.9234,
      "step": 1065
    },
    {
      "epoch": 0.36928386540120794,
      "grad_norm": 1.8371673822402954,
      "learning_rate": 4.8153212650387755e-05,
      "loss": 1.9845,
      "step": 1070
    },
    {
      "epoch": 0.3710094909404659,
      "grad_norm": 2.121731996536255,
      "learning_rate": 4.81361308556898e-05,
      "loss": 1.9463,
      "step": 1075
    },
    {
      "epoch": 0.3727351164797239,
      "grad_norm": 2.2433228492736816,
      "learning_rate": 4.811897348377968e-05,
      "loss": 1.9322,
      "step": 1080
    },
    {
      "epoch": 0.3744607420189819,
      "grad_norm": 1.8046493530273438,
      "learning_rate": 4.810174059070421e-05,
      "loss": 2.0132,
      "step": 1085
    },
    {
      "epoch": 0.37618636755823986,
      "grad_norm": 1.7008891105651855,
      "learning_rate": 4.808443223275689e-05,
      "loss": 2.0649,
      "step": 1090
    },
    {
      "epoch": 0.37791199309749784,
      "grad_norm": 1.796025276184082,
      "learning_rate": 4.8067048466477765e-05,
      "loss": 2.2614,
      "step": 1095
    },
    {
      "epoch": 0.3796376186367558,
      "grad_norm": 1.6005311012268066,
      "learning_rate": 4.804958934865318e-05,
      "loss": 2.0766,
      "step": 1100
    },
    {
      "epoch": 0.3796376186367558,
      "eval_loss": 2.091653347015381,
      "eval_runtime": 298.9108,
      "eval_samples_per_second": 17.236,
      "eval_steps_per_second": 8.618,
      "step": 1100
    },
    {
      "epoch": 0.3813632441760138,
      "grad_norm": 1.9330732822418213,
      "learning_rate": 4.803205493631563e-05,
      "loss": 2.0655,
      "step": 1105
    },
    {
      "epoch": 0.3830888697152718,
      "grad_norm": 2.046771764755249,
      "learning_rate": 4.801444528674359e-05,
      "loss": 2.3026,
      "step": 1110
    },
    {
      "epoch": 0.38481449525452976,
      "grad_norm": 2.2148685455322266,
      "learning_rate": 4.799676045746129e-05,
      "loss": 2.1253,
      "step": 1115
    },
    {
      "epoch": 0.38654012079378774,
      "grad_norm": 1.689501404762268,
      "learning_rate": 4.797900050623855e-05,
      "loss": 2.0622,
      "step": 1120
    },
    {
      "epoch": 0.3882657463330457,
      "grad_norm": 1.9001619815826416,
      "learning_rate": 4.796116549109059e-05,
      "loss": 2.1748,
      "step": 1125
    },
    {
      "epoch": 0.3899913718723037,
      "grad_norm": 2.1616365909576416,
      "learning_rate": 4.794325547027782e-05,
      "loss": 2.0795,
      "step": 1130
    },
    {
      "epoch": 0.3917169974115617,
      "grad_norm": 1.7954487800598145,
      "learning_rate": 4.792527050230568e-05,
      "loss": 1.8628,
      "step": 1135
    },
    {
      "epoch": 0.39344262295081966,
      "grad_norm": 1.555237889289856,
      "learning_rate": 4.790721064592444e-05,
      "loss": 1.9739,
      "step": 1140
    },
    {
      "epoch": 0.39516824849007764,
      "grad_norm": 2.149811267852783,
      "learning_rate": 4.788907596012898e-05,
      "loss": 2.1656,
      "step": 1145
    },
    {
      "epoch": 0.3968938740293356,
      "grad_norm": 1.6309471130371094,
      "learning_rate": 4.787086650415865e-05,
      "loss": 1.9276,
      "step": 1150
    },
    {
      "epoch": 0.3986194995685936,
      "grad_norm": 2.221778392791748,
      "learning_rate": 4.785258233749703e-05,
      "loss": 1.9992,
      "step": 1155
    },
    {
      "epoch": 0.4003451251078516,
      "grad_norm": 1.86026132106781,
      "learning_rate": 4.783422351987174e-05,
      "loss": 2.0083,
      "step": 1160
    },
    {
      "epoch": 0.40207075064710956,
      "grad_norm": 1.7954680919647217,
      "learning_rate": 4.7815790111254284e-05,
      "loss": 1.997,
      "step": 1165
    },
    {
      "epoch": 0.40379637618636754,
      "grad_norm": 2.060120105743408,
      "learning_rate": 4.77972821718598e-05,
      "loss": 2.1092,
      "step": 1170
    },
    {
      "epoch": 0.4055220017256255,
      "grad_norm": 1.837094783782959,
      "learning_rate": 4.777869976214692e-05,
      "loss": 2.1241,
      "step": 1175
    },
    {
      "epoch": 0.4072476272648835,
      "grad_norm": 1.7906615734100342,
      "learning_rate": 4.776004294281752e-05,
      "loss": 2.0598,
      "step": 1180
    },
    {
      "epoch": 0.4089732528041415,
      "grad_norm": 1.7088558673858643,
      "learning_rate": 4.774131177481655e-05,
      "loss": 2.2654,
      "step": 1185
    },
    {
      "epoch": 0.41069887834339946,
      "grad_norm": 1.746313214302063,
      "learning_rate": 4.7722506319331833e-05,
      "loss": 2.0275,
      "step": 1190
    },
    {
      "epoch": 0.41242450388265744,
      "grad_norm": 2.3366527557373047,
      "learning_rate": 4.770362663779386e-05,
      "loss": 2.1198,
      "step": 1195
    },
    {
      "epoch": 0.4141501294219154,
      "grad_norm": 2.5186712741851807,
      "learning_rate": 4.768467279187561e-05,
      "loss": 2.0813,
      "step": 1200
    },
    {
      "epoch": 0.4141501294219154,
      "eval_loss": 2.0686533451080322,
      "eval_runtime": 298.0919,
      "eval_samples_per_second": 17.283,
      "eval_steps_per_second": 8.642,
      "step": 1200
    },
    {
      "epoch": 0.4158757549611734,
      "grad_norm": 1.803131341934204,
      "learning_rate": 4.766564484349228e-05,
      "loss": 2.0323,
      "step": 1205
    },
    {
      "epoch": 0.4176013805004314,
      "grad_norm": 1.9920885562896729,
      "learning_rate": 4.764654285480119e-05,
      "loss": 2.0312,
      "step": 1210
    },
    {
      "epoch": 0.41932700603968937,
      "grad_norm": 1.7912334203720093,
      "learning_rate": 4.7627366888201484e-05,
      "loss": 1.9332,
      "step": 1215
    },
    {
      "epoch": 0.42105263157894735,
      "grad_norm": 2.1539347171783447,
      "learning_rate": 4.760811700633398e-05,
      "loss": 2.1288,
      "step": 1220
    },
    {
      "epoch": 0.4227782571182053,
      "grad_norm": 1.721699833869934,
      "learning_rate": 4.7588793272080943e-05,
      "loss": 2.1283,
      "step": 1225
    },
    {
      "epoch": 0.4245038826574633,
      "grad_norm": 1.954767107963562,
      "learning_rate": 4.75693957485659e-05,
      "loss": 2.1865,
      "step": 1230
    },
    {
      "epoch": 0.4262295081967213,
      "grad_norm": 1.706720232963562,
      "learning_rate": 4.754992449915341e-05,
      "loss": 2.024,
      "step": 1235
    },
    {
      "epoch": 0.42795513373597927,
      "grad_norm": 1.9083635807037354,
      "learning_rate": 4.7530379587448856e-05,
      "loss": 2.1885,
      "step": 1240
    },
    {
      "epoch": 0.4296807592752373,
      "grad_norm": 1.8185663223266602,
      "learning_rate": 4.751076107729828e-05,
      "loss": 2.1115,
      "step": 1245
    },
    {
      "epoch": 0.4314063848144953,
      "grad_norm": 2.0510191917419434,
      "learning_rate": 4.74910690327881e-05,
      "loss": 1.8673,
      "step": 1250
    },
    {
      "epoch": 0.43313201035375326,
      "grad_norm": 1.9676625728607178,
      "learning_rate": 4.7471303518244995e-05,
      "loss": 1.9817,
      "step": 1255
    },
    {
      "epoch": 0.43485763589301124,
      "grad_norm": 1.8866620063781738,
      "learning_rate": 4.745146459823559e-05,
      "loss": 2.0835,
      "step": 1260
    },
    {
      "epoch": 0.4365832614322692,
      "grad_norm": 2.1793904304504395,
      "learning_rate": 4.7431552337566343e-05,
      "loss": 2.0635,
      "step": 1265
    },
    {
      "epoch": 0.4383088869715272,
      "grad_norm": 1.973541259765625,
      "learning_rate": 4.741156680128326e-05,
      "loss": 1.8495,
      "step": 1270
    },
    {
      "epoch": 0.4400345125107852,
      "grad_norm": 2.194605827331543,
      "learning_rate": 4.7391508054671725e-05,
      "loss": 1.9912,
      "step": 1275
    },
    {
      "epoch": 0.44176013805004316,
      "grad_norm": 2.0515828132629395,
      "learning_rate": 4.737137616325627e-05,
      "loss": 2.1167,
      "step": 1280
    },
    {
      "epoch": 0.44348576358930114,
      "grad_norm": 1.999423623085022,
      "learning_rate": 4.735117119280036e-05,
      "loss": 2.0697,
      "step": 1285
    },
    {
      "epoch": 0.4452113891285591,
      "grad_norm": 2.0163159370422363,
      "learning_rate": 4.7330893209306204e-05,
      "loss": 2.1627,
      "step": 1290
    },
    {
      "epoch": 0.4469370146678171,
      "grad_norm": 1.8161561489105225,
      "learning_rate": 4.731054227901448e-05,
      "loss": 2.0382,
      "step": 1295
    },
    {
      "epoch": 0.4486626402070751,
      "grad_norm": 1.5435680150985718,
      "learning_rate": 4.729011846840418e-05,
      "loss": 2.0522,
      "step": 1300
    },
    {
      "epoch": 0.4486626402070751,
      "eval_loss": 2.053919553756714,
      "eval_runtime": 298.8189,
      "eval_samples_per_second": 17.241,
      "eval_steps_per_second": 8.621,
      "step": 1300
    },
    {
      "epoch": 0.45038826574633306,
      "grad_norm": 1.8935664892196655,
      "learning_rate": 4.7269621844192374e-05,
      "loss": 2.0693,
      "step": 1305
    },
    {
      "epoch": 0.45211389128559104,
      "grad_norm": 1.8718078136444092,
      "learning_rate": 4.7249052473333974e-05,
      "loss": 1.9574,
      "step": 1310
    },
    {
      "epoch": 0.453839516824849,
      "grad_norm": 2.2417118549346924,
      "learning_rate": 4.722841042302153e-05,
      "loss": 1.8897,
      "step": 1315
    },
    {
      "epoch": 0.455565142364107,
      "grad_norm": 1.812849521636963,
      "learning_rate": 4.7207695760685025e-05,
      "loss": 1.9002,
      "step": 1320
    },
    {
      "epoch": 0.457290767903365,
      "grad_norm": 2.256197690963745,
      "learning_rate": 4.71869085539916e-05,
      "loss": 2.0336,
      "step": 1325
    },
    {
      "epoch": 0.45901639344262296,
      "grad_norm": 2.0173308849334717,
      "learning_rate": 4.716604887084542e-05,
      "loss": 2.0218,
      "step": 1330
    },
    {
      "epoch": 0.46074201898188094,
      "grad_norm": 2.0301578044891357,
      "learning_rate": 4.7145116779387386e-05,
      "loss": 2.1213,
      "step": 1335
    },
    {
      "epoch": 0.4624676445211389,
      "grad_norm": 1.9819315671920776,
      "learning_rate": 4.712411234799491e-05,
      "loss": 2.1139,
      "step": 1340
    },
    {
      "epoch": 0.4641932700603969,
      "grad_norm": 2.535456895828247,
      "learning_rate": 4.7103035645281736e-05,
      "loss": 2.0214,
      "step": 1345
    },
    {
      "epoch": 0.4659188955996549,
      "grad_norm": 1.7566274404525757,
      "learning_rate": 4.708188674009768e-05,
      "loss": 2.2374,
      "step": 1350
    },
    {
      "epoch": 0.46764452113891286,
      "grad_norm": 1.9934757947921753,
      "learning_rate": 4.7060665701528414e-05,
      "loss": 1.9283,
      "step": 1355
    },
    {
      "epoch": 0.46937014667817084,
      "grad_norm": 1.965567708015442,
      "learning_rate": 4.703937259889527e-05,
      "loss": 2.0128,
      "step": 1360
    },
    {
      "epoch": 0.4710957722174288,
      "grad_norm": 2.2244584560394287,
      "learning_rate": 4.7018007501754944e-05,
      "loss": 2.1867,
      "step": 1365
    },
    {
      "epoch": 0.4728213977566868,
      "grad_norm": 2.006596803665161,
      "learning_rate": 4.699657047989935e-05,
      "loss": 2.0296,
      "step": 1370
    },
    {
      "epoch": 0.4745470232959448,
      "grad_norm": 2.735225200653076,
      "learning_rate": 4.697506160335532e-05,
      "loss": 2.2181,
      "step": 1375
    },
    {
      "epoch": 0.47627264883520276,
      "grad_norm": 2.692042827606201,
      "learning_rate": 4.6953480942384445e-05,
      "loss": 2.2256,
      "step": 1380
    },
    {
      "epoch": 0.47799827437446074,
      "grad_norm": 2.3452019691467285,
      "learning_rate": 4.6931828567482774e-05,
      "loss": 1.9509,
      "step": 1385
    },
    {
      "epoch": 0.4797238999137187,
      "grad_norm": 1.9308534860610962,
      "learning_rate": 4.6910104549380644e-05,
      "loss": 1.8911,
      "step": 1390
    },
    {
      "epoch": 0.4814495254529767,
      "grad_norm": 1.7176262140274048,
      "learning_rate": 4.6888308959042414e-05,
      "loss": 1.9871,
      "step": 1395
    },
    {
      "epoch": 0.4831751509922347,
      "grad_norm": 2.004417896270752,
      "learning_rate": 4.686644186766625e-05,
      "loss": 2.0214,
      "step": 1400
    },
    {
      "epoch": 0.4831751509922347,
      "eval_loss": 2.034879207611084,
      "eval_runtime": 298.3387,
      "eval_samples_per_second": 17.269,
      "eval_steps_per_second": 8.634,
      "step": 1400
    },
    {
      "epoch": 0.48490077653149266,
      "grad_norm": 2.4669666290283203,
      "learning_rate": 4.6844503346683873e-05,
      "loss": 2.055,
      "step": 1405
    },
    {
      "epoch": 0.48662640207075064,
      "grad_norm": 2.137235641479492,
      "learning_rate": 4.682249346776034e-05,
      "loss": 2.1833,
      "step": 1410
    },
    {
      "epoch": 0.4883520276100086,
      "grad_norm": 1.932140827178955,
      "learning_rate": 4.680041230279383e-05,
      "loss": 2.0331,
      "step": 1415
    },
    {
      "epoch": 0.4900776531492666,
      "grad_norm": 2.166050672531128,
      "learning_rate": 4.677825992391535e-05,
      "loss": 2.1866,
      "step": 1420
    },
    {
      "epoch": 0.4918032786885246,
      "grad_norm": 2.025742530822754,
      "learning_rate": 4.675603640348857e-05,
      "loss": 2.0931,
      "step": 1425
    },
    {
      "epoch": 0.49352890422778256,
      "grad_norm": 1.7914679050445557,
      "learning_rate": 4.6733741814109544e-05,
      "loss": 2.0541,
      "step": 1430
    },
    {
      "epoch": 0.49525452976704054,
      "grad_norm": 1.9705636501312256,
      "learning_rate": 4.671137622860646e-05,
      "loss": 2.0277,
      "step": 1435
    },
    {
      "epoch": 0.4969801553062985,
      "grad_norm": 2.03914213180542,
      "learning_rate": 4.668893972003947e-05,
      "loss": 1.9609,
      "step": 1440
    },
    {
      "epoch": 0.4987057808455565,
      "grad_norm": 2.0856990814208984,
      "learning_rate": 4.666643236170035e-05,
      "loss": 2.0248,
      "step": 1445
    },
    {
      "epoch": 0.5004314063848145,
      "grad_norm": 2.0721311569213867,
      "learning_rate": 4.664385422711236e-05,
      "loss": 1.9029,
      "step": 1450
    },
    {
      "epoch": 0.5021570319240725,
      "grad_norm": 2.034546375274658,
      "learning_rate": 4.662120539002994e-05,
      "loss": 1.9818,
      "step": 1455
    },
    {
      "epoch": 0.5038826574633305,
      "grad_norm": 1.825484275817871,
      "learning_rate": 4.6598485924438494e-05,
      "loss": 1.9109,
      "step": 1460
    },
    {
      "epoch": 0.5056082830025884,
      "grad_norm": 2.190164804458618,
      "learning_rate": 4.6575695904554136e-05,
      "loss": 2.0952,
      "step": 1465
    },
    {
      "epoch": 0.5073339085418465,
      "grad_norm": 1.7783215045928955,
      "learning_rate": 4.655283540482348e-05,
      "loss": 1.8731,
      "step": 1470
    },
    {
      "epoch": 0.5090595340811044,
      "grad_norm": 2.0479629039764404,
      "learning_rate": 4.652990449992333e-05,
      "loss": 1.9922,
      "step": 1475
    },
    {
      "epoch": 0.5107851596203624,
      "grad_norm": 2.383244276046753,
      "learning_rate": 4.650690326476051e-05,
      "loss": 2.0678,
      "step": 1480
    },
    {
      "epoch": 0.5125107851596203,
      "grad_norm": 2.4112610816955566,
      "learning_rate": 4.648383177447159e-05,
      "loss": 1.992,
      "step": 1485
    },
    {
      "epoch": 0.5142364106988784,
      "grad_norm": 1.9002820253372192,
      "learning_rate": 4.64606901044226e-05,
      "loss": 1.9752,
      "step": 1490
    },
    {
      "epoch": 0.5159620362381363,
      "grad_norm": 2.0392487049102783,
      "learning_rate": 4.643747833020886e-05,
      "loss": 1.9302,
      "step": 1495
    },
    {
      "epoch": 0.5176876617773943,
      "grad_norm": 1.90013587474823,
      "learning_rate": 4.641419652765469e-05,
      "loss": 2.0858,
      "step": 1500
    },
    {
      "epoch": 0.5176876617773943,
      "eval_loss": 2.0195789337158203,
      "eval_runtime": 299.9214,
      "eval_samples_per_second": 17.178,
      "eval_steps_per_second": 8.589,
      "step": 1500
    },
    {
      "epoch": 0.5194132873166523,
      "grad_norm": 2.200148582458496,
      "learning_rate": 4.6390844772813146e-05,
      "loss": 2.0094,
      "step": 1505
    },
    {
      "epoch": 0.5211389128559103,
      "grad_norm": 2.3988349437713623,
      "learning_rate": 4.636742314196581e-05,
      "loss": 2.147,
      "step": 1510
    },
    {
      "epoch": 0.5228645383951682,
      "grad_norm": 1.980859637260437,
      "learning_rate": 4.63439317116225e-05,
      "loss": 1.9092,
      "step": 1515
    },
    {
      "epoch": 0.5245901639344263,
      "grad_norm": 2.0871481895446777,
      "learning_rate": 4.632037055852109e-05,
      "loss": 2.0713,
      "step": 1520
    },
    {
      "epoch": 0.5263157894736842,
      "grad_norm": 2.8766510486602783,
      "learning_rate": 4.6296739759627164e-05,
      "loss": 2.1138,
      "step": 1525
    },
    {
      "epoch": 0.5280414150129422,
      "grad_norm": 2.0827245712280273,
      "learning_rate": 4.627303939213384e-05,
      "loss": 2.0708,
      "step": 1530
    },
    {
      "epoch": 0.5297670405522001,
      "grad_norm": 2.3557016849517822,
      "learning_rate": 4.624926953346147e-05,
      "loss": 2.1973,
      "step": 1535
    },
    {
      "epoch": 0.5314926660914582,
      "grad_norm": 1.9957250356674194,
      "learning_rate": 4.622543026125744e-05,
      "loss": 2.0903,
      "step": 1540
    },
    {
      "epoch": 0.5332182916307161,
      "grad_norm": 1.7183220386505127,
      "learning_rate": 4.620152165339586e-05,
      "loss": 1.9515,
      "step": 1545
    },
    {
      "epoch": 0.5349439171699741,
      "grad_norm": 1.8653461933135986,
      "learning_rate": 4.617754378797733e-05,
      "loss": 1.9562,
      "step": 1550
    },
    {
      "epoch": 0.5366695427092321,
      "grad_norm": 2.0691616535186768,
      "learning_rate": 4.615349674332871e-05,
      "loss": 2.0227,
      "step": 1555
    },
    {
      "epoch": 0.5383951682484901,
      "grad_norm": 2.3933169841766357,
      "learning_rate": 4.6129380598002826e-05,
      "loss": 2.0206,
      "step": 1560
    },
    {
      "epoch": 0.540120793787748,
      "grad_norm": 1.9234048128128052,
      "learning_rate": 4.6105195430778236e-05,
      "loss": 2.1611,
      "step": 1565
    },
    {
      "epoch": 0.5418464193270061,
      "grad_norm": 2.023736000061035,
      "learning_rate": 4.608094132065898e-05,
      "loss": 2.0698,
      "step": 1570
    },
    {
      "epoch": 0.543572044866264,
      "grad_norm": 2.286128282546997,
      "learning_rate": 4.605661834687427e-05,
      "loss": 1.9853,
      "step": 1575
    },
    {
      "epoch": 0.545297670405522,
      "grad_norm": 2.5283796787261963,
      "learning_rate": 4.6032226588878326e-05,
      "loss": 1.916,
      "step": 1580
    },
    {
      "epoch": 0.5470232959447799,
      "grad_norm": 2.3092868328094482,
      "learning_rate": 4.6007766126350005e-05,
      "loss": 2.0816,
      "step": 1585
    },
    {
      "epoch": 0.548748921484038,
      "grad_norm": 2.367680788040161,
      "learning_rate": 4.598323703919264e-05,
      "loss": 1.9823,
      "step": 1590
    },
    {
      "epoch": 0.5504745470232959,
      "grad_norm": 1.8564969301223755,
      "learning_rate": 4.595863940753372e-05,
      "loss": 1.9839,
      "step": 1595
    },
    {
      "epoch": 0.5522001725625539,
      "grad_norm": 1.8801124095916748,
      "learning_rate": 4.593397331172462e-05,
      "loss": 1.9037,
      "step": 1600
    },
    {
      "epoch": 0.5522001725625539,
      "eval_loss": 2.003876209259033,
      "eval_runtime": 299.8391,
      "eval_samples_per_second": 17.183,
      "eval_steps_per_second": 8.591,
      "step": 1600
    },
    {
      "epoch": 0.5539257981018119,
      "grad_norm": 2.492088556289673,
      "learning_rate": 4.590923883234041e-05,
      "loss": 2.0711,
      "step": 1605
    },
    {
      "epoch": 0.5556514236410699,
      "grad_norm": 3.467514753341675,
      "learning_rate": 4.58844360501795e-05,
      "loss": 2.1041,
      "step": 1610
    },
    {
      "epoch": 0.5573770491803278,
      "grad_norm": 2.1982955932617188,
      "learning_rate": 4.585956504626345e-05,
      "loss": 2.072,
      "step": 1615
    },
    {
      "epoch": 0.5591026747195859,
      "grad_norm": 2.0754458904266357,
      "learning_rate": 4.5834625901836655e-05,
      "loss": 2.0973,
      "step": 1620
    },
    {
      "epoch": 0.5608283002588438,
      "grad_norm": 2.422495126724243,
      "learning_rate": 4.580961869836611e-05,
      "loss": 2.1166,
      "step": 1625
    },
    {
      "epoch": 0.5625539257981018,
      "grad_norm": 2.1862411499023438,
      "learning_rate": 4.578454351754112e-05,
      "loss": 1.9936,
      "step": 1630
    },
    {
      "epoch": 0.5642795513373597,
      "grad_norm": 1.8133416175842285,
      "learning_rate": 4.5759400441273084e-05,
      "loss": 1.9613,
      "step": 1635
    },
    {
      "epoch": 0.5660051768766178,
      "grad_norm": 2.0354931354522705,
      "learning_rate": 4.573418955169515e-05,
      "loss": 2.0208,
      "step": 1640
    },
    {
      "epoch": 0.5677308024158757,
      "grad_norm": 2.085541009902954,
      "learning_rate": 4.5708910931162e-05,
      "loss": 1.9969,
      "step": 1645
    },
    {
      "epoch": 0.5694564279551337,
      "grad_norm": 1.951822280883789,
      "learning_rate": 4.568356466224957e-05,
      "loss": 2.0009,
      "step": 1650
    },
    {
      "epoch": 0.5711820534943917,
      "grad_norm": 2.351294994354248,
      "learning_rate": 4.5658150827754795e-05,
      "loss": 1.8788,
      "step": 1655
    },
    {
      "epoch": 0.5729076790336497,
      "grad_norm": 2.280491590499878,
      "learning_rate": 4.563266951069528e-05,
      "loss": 2.0793,
      "step": 1660
    },
    {
      "epoch": 0.5746333045729077,
      "grad_norm": 2.1696114540100098,
      "learning_rate": 4.560712079430912e-05,
      "loss": 1.9349,
      "step": 1665
    },
    {
      "epoch": 0.5763589301121657,
      "grad_norm": 4.815619468688965,
      "learning_rate": 4.558150476205453e-05,
      "loss": 2.0543,
      "step": 1670
    },
    {
      "epoch": 0.5780845556514237,
      "grad_norm": 2.2255890369415283,
      "learning_rate": 4.555582149760968e-05,
      "loss": 1.9572,
      "step": 1675
    },
    {
      "epoch": 0.5798101811906816,
      "grad_norm": 2.4027962684631348,
      "learning_rate": 4.553007108487229e-05,
      "loss": 2.0726,
      "step": 1680
    },
    {
      "epoch": 0.5815358067299397,
      "grad_norm": 1.9163432121276855,
      "learning_rate": 4.550425360795949e-05,
      "loss": 1.9222,
      "step": 1685
    },
    {
      "epoch": 0.5832614322691976,
      "grad_norm": 2.40969181060791,
      "learning_rate": 4.547836915120746e-05,
      "loss": 2.0171,
      "step": 1690
    },
    {
      "epoch": 0.5849870578084556,
      "grad_norm": 2.0709285736083984,
      "learning_rate": 4.545241779917118e-05,
      "loss": 2.1007,
      "step": 1695
    },
    {
      "epoch": 0.5867126833477135,
      "grad_norm": 1.7776316404342651,
      "learning_rate": 4.542639963662415e-05,
      "loss": 1.9161,
      "step": 1700
    },
    {
      "epoch": 0.5867126833477135,
      "eval_loss": 1.9879587888717651,
      "eval_runtime": 300.5936,
      "eval_samples_per_second": 17.139,
      "eval_steps_per_second": 8.57,
      "step": 1700
    },
    {
      "epoch": 0.5884383088869716,
      "grad_norm": 2.001645565032959,
      "learning_rate": 4.5400314748558114e-05,
      "loss": 1.9774,
      "step": 1705
    },
    {
      "epoch": 0.5901639344262295,
      "grad_norm": 1.9404841661453247,
      "learning_rate": 4.537416322018279e-05,
      "loss": 1.9594,
      "step": 1710
    },
    {
      "epoch": 0.5918895599654875,
      "grad_norm": 2.035784959793091,
      "learning_rate": 4.534794513692558e-05,
      "loss": 2.0426,
      "step": 1715
    },
    {
      "epoch": 0.5936151855047455,
      "grad_norm": 2.599898099899292,
      "learning_rate": 4.5321660584431315e-05,
      "loss": 2.1053,
      "step": 1720
    },
    {
      "epoch": 0.5953408110440035,
      "grad_norm": 2.112393617630005,
      "learning_rate": 4.529530964856191e-05,
      "loss": 1.9926,
      "step": 1725
    },
    {
      "epoch": 0.5970664365832614,
      "grad_norm": 2.0296459197998047,
      "learning_rate": 4.526889241539619e-05,
      "loss": 1.9232,
      "step": 1730
    },
    {
      "epoch": 0.5987920621225195,
      "grad_norm": 2.122162103652954,
      "learning_rate": 4.52424089712295e-05,
      "loss": 2.0811,
      "step": 1735
    },
    {
      "epoch": 0.6005176876617774,
      "grad_norm": 2.1231472492218018,
      "learning_rate": 4.52158594025735e-05,
      "loss": 1.9687,
      "step": 1740
    },
    {
      "epoch": 0.6022433132010354,
      "grad_norm": 3.4523134231567383,
      "learning_rate": 4.518924379615584e-05,
      "loss": 2.124,
      "step": 1745
    },
    {
      "epoch": 0.6039689387402933,
      "grad_norm": 2.6019978523254395,
      "learning_rate": 4.5162562238919896e-05,
      "loss": 2.0424,
      "step": 1750
    },
    {
      "epoch": 0.6056945642795514,
      "grad_norm": 1.9610544443130493,
      "learning_rate": 4.513581481802449e-05,
      "loss": 1.9513,
      "step": 1755
    },
    {
      "epoch": 0.6074201898188093,
      "grad_norm": 1.7158355712890625,
      "learning_rate": 4.510900162084358e-05,
      "loss": 1.7127,
      "step": 1760
    },
    {
      "epoch": 0.6091458153580673,
      "grad_norm": 2.3491170406341553,
      "learning_rate": 4.5082122734966e-05,
      "loss": 2.0056,
      "step": 1765
    },
    {
      "epoch": 0.6108714408973253,
      "grad_norm": 2.2071216106414795,
      "learning_rate": 4.505517824819516e-05,
      "loss": 2.1124,
      "step": 1770
    },
    {
      "epoch": 0.6125970664365833,
      "grad_norm": 2.2730484008789062,
      "learning_rate": 4.502816824854878e-05,
      "loss": 1.9526,
      "step": 1775
    },
    {
      "epoch": 0.6143226919758412,
      "grad_norm": 2.4928908348083496,
      "learning_rate": 4.500109282425856e-05,
      "loss": 2.0324,
      "step": 1780
    },
    {
      "epoch": 0.6160483175150993,
      "grad_norm": 2.221308946609497,
      "learning_rate": 4.4973952063769935e-05,
      "loss": 1.9035,
      "step": 1785
    },
    {
      "epoch": 0.6177739430543572,
      "grad_norm": 1.7891730070114136,
      "learning_rate": 4.4946746055741775e-05,
      "loss": 2.0047,
      "step": 1790
    },
    {
      "epoch": 0.6194995685936152,
      "grad_norm": 1.8066099882125854,
      "learning_rate": 4.4919474889046064e-05,
      "loss": 2.1412,
      "step": 1795
    },
    {
      "epoch": 0.6212251941328731,
      "grad_norm": 2.6809751987457275,
      "learning_rate": 4.489213865276767e-05,
      "loss": 2.0706,
      "step": 1800
    },
    {
      "epoch": 0.6212251941328731,
      "eval_loss": 1.9772852659225464,
      "eval_runtime": 301.8375,
      "eval_samples_per_second": 17.069,
      "eval_steps_per_second": 8.534,
      "step": 1800
    },
    {
      "epoch": 0.6229508196721312,
      "grad_norm": 2.0904645919799805,
      "learning_rate": 4.486473743620398e-05,
      "loss": 2.0059,
      "step": 1805
    },
    {
      "epoch": 0.6246764452113891,
      "grad_norm": 1.8338253498077393,
      "learning_rate": 4.48372713288647e-05,
      "loss": 1.8939,
      "step": 1810
    },
    {
      "epoch": 0.6264020707506471,
      "grad_norm": 2.111502170562744,
      "learning_rate": 4.480974042047146e-05,
      "loss": 2.1363,
      "step": 1815
    },
    {
      "epoch": 0.6281276962899051,
      "grad_norm": 1.7228646278381348,
      "learning_rate": 4.478214480095758e-05,
      "loss": 2.0059,
      "step": 1820
    },
    {
      "epoch": 0.6298533218291631,
      "grad_norm": 2.6498165130615234,
      "learning_rate": 4.47544845604678e-05,
      "loss": 1.8838,
      "step": 1825
    },
    {
      "epoch": 0.631578947368421,
      "grad_norm": 2.2919602394104004,
      "learning_rate": 4.472675978935792e-05,
      "loss": 2.0848,
      "step": 1830
    },
    {
      "epoch": 0.633304572907679,
      "grad_norm": 1.911704421043396,
      "learning_rate": 4.4698970578194554e-05,
      "loss": 1.8603,
      "step": 1835
    },
    {
      "epoch": 0.635030198446937,
      "grad_norm": 2.2868568897247314,
      "learning_rate": 4.4671117017754806e-05,
      "loss": 1.8979,
      "step": 1840
    },
    {
      "epoch": 0.636755823986195,
      "grad_norm": 2.3684237003326416,
      "learning_rate": 4.4643199199025994e-05,
      "loss": 1.9555,
      "step": 1845
    },
    {
      "epoch": 0.6384814495254529,
      "grad_norm": 1.9478389024734497,
      "learning_rate": 4.461521721320534e-05,
      "loss": 1.8661,
      "step": 1850
    },
    {
      "epoch": 0.640207075064711,
      "grad_norm": 1.8417296409606934,
      "learning_rate": 4.4587171151699684e-05,
      "loss": 1.948,
      "step": 1855
    },
    {
      "epoch": 0.6419327006039689,
      "grad_norm": 2.3368594646453857,
      "learning_rate": 4.455906110612516e-05,
      "loss": 2.0742,
      "step": 1860
    },
    {
      "epoch": 0.6436583261432269,
      "grad_norm": 2.044684648513794,
      "learning_rate": 4.453088716830695e-05,
      "loss": 1.9246,
      "step": 1865
    },
    {
      "epoch": 0.6453839516824849,
      "grad_norm": 2.0836079120635986,
      "learning_rate": 4.450264943027891e-05,
      "loss": 1.8086,
      "step": 1870
    },
    {
      "epoch": 0.6471095772217429,
      "grad_norm": 2.0095160007476807,
      "learning_rate": 4.447434798428331e-05,
      "loss": 1.8276,
      "step": 1875
    },
    {
      "epoch": 0.6488352027610008,
      "grad_norm": 1.9243816137313843,
      "learning_rate": 4.444598292277056e-05,
      "loss": 1.8928,
      "step": 1880
    },
    {
      "epoch": 0.6505608283002589,
      "grad_norm": 2.024962902069092,
      "learning_rate": 4.441755433839886e-05,
      "loss": 2.0232,
      "step": 1885
    },
    {
      "epoch": 0.6522864538395168,
      "grad_norm": 1.9237401485443115,
      "learning_rate": 4.438906232403391e-05,
      "loss": 1.9489,
      "step": 1890
    },
    {
      "epoch": 0.6540120793787748,
      "grad_norm": 2.0573010444641113,
      "learning_rate": 4.436050697274863e-05,
      "loss": 2.0592,
      "step": 1895
    },
    {
      "epoch": 0.6557377049180327,
      "grad_norm": 1.7272156476974487,
      "learning_rate": 4.4331888377822816e-05,
      "loss": 2.0887,
      "step": 1900
    },
    {
      "epoch": 0.6557377049180327,
      "eval_loss": 1.9652717113494873,
      "eval_runtime": 304.1986,
      "eval_samples_per_second": 16.936,
      "eval_steps_per_second": 8.468,
      "step": 1900
    },
    {
      "epoch": 0.6574633304572908,
      "grad_norm": 1.7747788429260254,
      "learning_rate": 4.430320663274287e-05,
      "loss": 1.9851,
      "step": 1905
    },
    {
      "epoch": 0.6591889559965487,
      "grad_norm": 2.438150405883789,
      "learning_rate": 4.42744618312015e-05,
      "loss": 2.0314,
      "step": 1910
    },
    {
      "epoch": 0.6609145815358067,
      "grad_norm": 2.1393933296203613,
      "learning_rate": 4.424565406709735e-05,
      "loss": 1.9171,
      "step": 1915
    },
    {
      "epoch": 0.6626402070750647,
      "grad_norm": 2.3325514793395996,
      "learning_rate": 4.4216783434534793e-05,
      "loss": 1.9374,
      "step": 1920
    },
    {
      "epoch": 0.6643658326143227,
      "grad_norm": 2.450467586517334,
      "learning_rate": 4.418785002782353e-05,
      "loss": 1.8045,
      "step": 1925
    },
    {
      "epoch": 0.6660914581535806,
      "grad_norm": 1.9813157320022583,
      "learning_rate": 4.415885394147834e-05,
      "loss": 1.8496,
      "step": 1930
    },
    {
      "epoch": 0.6678170836928387,
      "grad_norm": 1.810402750968933,
      "learning_rate": 4.4129795270218755e-05,
      "loss": 2.0473,
      "step": 1935
    },
    {
      "epoch": 0.6695427092320967,
      "grad_norm": 2.3101141452789307,
      "learning_rate": 4.4100674108968734e-05,
      "loss": 1.9551,
      "step": 1940
    },
    {
      "epoch": 0.6712683347713546,
      "grad_norm": 2.320033311843872,
      "learning_rate": 4.4071490552856384e-05,
      "loss": 1.7924,
      "step": 1945
    },
    {
      "epoch": 0.6729939603106126,
      "grad_norm": 2.1702380180358887,
      "learning_rate": 4.404224469721363e-05,
      "loss": 1.8151,
      "step": 1950
    },
    {
      "epoch": 0.6747195858498706,
      "grad_norm": 2.4775383472442627,
      "learning_rate": 4.4012936637575894e-05,
      "loss": 2.0678,
      "step": 1955
    },
    {
      "epoch": 0.6764452113891286,
      "grad_norm": 2.350881814956665,
      "learning_rate": 4.398356646968181e-05,
      "loss": 1.9672,
      "step": 1960
    },
    {
      "epoch": 0.6781708369283865,
      "grad_norm": 2.1317129135131836,
      "learning_rate": 4.395413428947289e-05,
      "loss": 2.0697,
      "step": 1965
    },
    {
      "epoch": 0.6798964624676446,
      "grad_norm": 1.75834059715271,
      "learning_rate": 4.392464019309323e-05,
      "loss": 1.9999,
      "step": 1970
    },
    {
      "epoch": 0.6816220880069025,
      "grad_norm": 2.153074026107788,
      "learning_rate": 4.389508427688915e-05,
      "loss": 1.8463,
      "step": 1975
    },
    {
      "epoch": 0.6833477135461605,
      "grad_norm": 1.9967632293701172,
      "learning_rate": 4.3865466637408945e-05,
      "loss": 1.9961,
      "step": 1980
    },
    {
      "epoch": 0.6850733390854185,
      "grad_norm": 2.0273869037628174,
      "learning_rate": 4.383578737140254e-05,
      "loss": 1.7512,
      "step": 1985
    },
    {
      "epoch": 0.6867989646246765,
      "grad_norm": 2.3156003952026367,
      "learning_rate": 4.3806046575821135e-05,
      "loss": 1.9542,
      "step": 1990
    },
    {
      "epoch": 0.6885245901639344,
      "grad_norm": 2.0354394912719727,
      "learning_rate": 4.3776244347816966e-05,
      "loss": 2.0422,
      "step": 1995
    },
    {
      "epoch": 0.6902502157031924,
      "grad_norm": 2.047268867492676,
      "learning_rate": 4.374638078474293e-05,
      "loss": 1.6599,
      "step": 2000
    },
    {
      "epoch": 0.6902502157031924,
      "eval_loss": 1.9549179077148438,
      "eval_runtime": 366.113,
      "eval_samples_per_second": 14.072,
      "eval_steps_per_second": 7.036,
      "step": 2000
    },
    {
      "epoch": 0.6919758412424504,
      "grad_norm": 2.9674506187438965,
      "learning_rate": 4.371645598415226e-05,
      "loss": 1.936,
      "step": 2005
    },
    {
      "epoch": 0.6937014667817084,
      "grad_norm": 2.359924793243408,
      "learning_rate": 4.368647004379826e-05,
      "loss": 2.0227,
      "step": 2010
    },
    {
      "epoch": 0.6954270923209663,
      "grad_norm": 2.115330457687378,
      "learning_rate": 4.3656423061633946e-05,
      "loss": 2.2042,
      "step": 2015
    },
    {
      "epoch": 0.6971527178602244,
      "grad_norm": 2.347052574157715,
      "learning_rate": 4.362631513581174e-05,
      "loss": 2.0298,
      "step": 2020
    },
    {
      "epoch": 0.6988783433994823,
      "grad_norm": 1.912441611289978,
      "learning_rate": 4.359614636468313e-05,
      "loss": 2.0525,
      "step": 2025
    },
    {
      "epoch": 0.7006039689387403,
      "grad_norm": 2.2142274379730225,
      "learning_rate": 4.356591684679838e-05,
      "loss": 2.0098,
      "step": 2030
    },
    {
      "epoch": 0.7023295944779983,
      "grad_norm": 2.4859611988067627,
      "learning_rate": 4.353562668090618e-05,
      "loss": 1.9084,
      "step": 2035
    },
    {
      "epoch": 0.7040552200172563,
      "grad_norm": 2.136645555496216,
      "learning_rate": 4.350527596595333e-05,
      "loss": 1.9375,
      "step": 2040
    },
    {
      "epoch": 0.7057808455565142,
      "grad_norm": 2.2982261180877686,
      "learning_rate": 4.347486480108443e-05,
      "loss": 1.9967,
      "step": 2045
    },
    {
      "epoch": 0.7075064710957722,
      "grad_norm": 2.137610912322998,
      "learning_rate": 4.3444393285641575e-05,
      "loss": 1.9215,
      "step": 2050
    },
    {
      "epoch": 0.7092320966350302,
      "grad_norm": 2.7512998580932617,
      "learning_rate": 4.341386151916394e-05,
      "loss": 2.1129,
      "step": 2055
    },
    {
      "epoch": 0.7109577221742882,
      "grad_norm": 2.3991599082946777,
      "learning_rate": 4.338326960138756e-05,
      "loss": 1.9731,
      "step": 2060
    },
    {
      "epoch": 0.7126833477135461,
      "grad_norm": 2.196777820587158,
      "learning_rate": 4.3352617632244964e-05,
      "loss": 2.0439,
      "step": 2065
    },
    {
      "epoch": 0.7144089732528042,
      "grad_norm": 2.6872622966766357,
      "learning_rate": 4.332190571186484e-05,
      "loss": 1.7532,
      "step": 2070
    },
    {
      "epoch": 0.7161345987920621,
      "grad_norm": 2.4815425872802734,
      "learning_rate": 4.32911339405717e-05,
      "loss": 1.9934,
      "step": 2075
    },
    {
      "epoch": 0.7178602243313201,
      "grad_norm": 2.832016944885254,
      "learning_rate": 4.326030241888559e-05,
      "loss": 1.78,
      "step": 2080
    },
    {
      "epoch": 0.719585849870578,
      "grad_norm": 2.831998348236084,
      "learning_rate": 4.322941124752172e-05,
      "loss": 1.8294,
      "step": 2085
    },
    {
      "epoch": 0.7213114754098361,
      "grad_norm": 2.875422954559326,
      "learning_rate": 4.319846052739017e-05,
      "loss": 1.8816,
      "step": 2090
    },
    {
      "epoch": 0.723037100949094,
      "grad_norm": 1.8628934621810913,
      "learning_rate": 4.316745035959553e-05,
      "loss": 1.7207,
      "step": 2095
    },
    {
      "epoch": 0.724762726488352,
      "grad_norm": 2.896866798400879,
      "learning_rate": 4.313638084543658e-05,
      "loss": 1.8803,
      "step": 2100
    },
    {
      "epoch": 0.724762726488352,
      "eval_loss": 1.9417611360549927,
      "eval_runtime": 301.9821,
      "eval_samples_per_second": 17.061,
      "eval_steps_per_second": 8.53,
      "step": 2100
    },
    {
      "epoch": 0.72648835202761,
      "grad_norm": 2.288973808288574,
      "learning_rate": 4.310525208640599e-05,
      "loss": 1.991,
      "step": 2105
    },
    {
      "epoch": 0.728213977566868,
      "grad_norm": 2.271137237548828,
      "learning_rate": 4.3074064184189926e-05,
      "loss": 2.0103,
      "step": 2110
    },
    {
      "epoch": 0.7299396031061259,
      "grad_norm": 2.6417338848114014,
      "learning_rate": 4.304281724066778e-05,
      "loss": 1.8334,
      "step": 2115
    },
    {
      "epoch": 0.731665228645384,
      "grad_norm": 2.1027321815490723,
      "learning_rate": 4.3011511357911796e-05,
      "loss": 1.859,
      "step": 2120
    },
    {
      "epoch": 0.7333908541846419,
      "grad_norm": 2.1980559825897217,
      "learning_rate": 4.2980146638186747e-05,
      "loss": 2.0876,
      "step": 2125
    },
    {
      "epoch": 0.7351164797238999,
      "grad_norm": 2.2400944232940674,
      "learning_rate": 4.2948723183949624e-05,
      "loss": 1.981,
      "step": 2130
    },
    {
      "epoch": 0.7368421052631579,
      "grad_norm": 2.4947400093078613,
      "learning_rate": 4.291724109784927e-05,
      "loss": 1.8639,
      "step": 2135
    },
    {
      "epoch": 0.7385677308024159,
      "grad_norm": 2.065059185028076,
      "learning_rate": 4.2885700482726044e-05,
      "loss": 1.9021,
      "step": 2140
    },
    {
      "epoch": 0.7402933563416738,
      "grad_norm": 2.289163589477539,
      "learning_rate": 4.2854101441611526e-05,
      "loss": 1.8677,
      "step": 2145
    },
    {
      "epoch": 0.7420189818809318,
      "grad_norm": 2.431619167327881,
      "learning_rate": 4.282244407772813e-05,
      "loss": 1.8776,
      "step": 2150
    },
    {
      "epoch": 0.7437446074201898,
      "grad_norm": 2.33530330657959,
      "learning_rate": 4.279072849448879e-05,
      "loss": 1.7641,
      "step": 2155
    },
    {
      "epoch": 0.7454702329594478,
      "grad_norm": 2.076512098312378,
      "learning_rate": 4.275895479549664e-05,
      "loss": 1.8181,
      "step": 2160
    },
    {
      "epoch": 0.7471958584987057,
      "grad_norm": 2.1952059268951416,
      "learning_rate": 4.272712308454463e-05,
      "loss": 1.9576,
      "step": 2165
    },
    {
      "epoch": 0.7489214840379638,
      "grad_norm": 2.3722097873687744,
      "learning_rate": 4.269523346561524e-05,
      "loss": 1.9412,
      "step": 2170
    },
    {
      "epoch": 0.7506471095772217,
      "grad_norm": 1.965968370437622,
      "learning_rate": 4.266328604288008e-05,
      "loss": 1.8499,
      "step": 2175
    },
    {
      "epoch": 0.7523727351164797,
      "grad_norm": 1.914233684539795,
      "learning_rate": 4.263128092069963e-05,
      "loss": 1.8637,
      "step": 2180
    },
    {
      "epoch": 0.7540983606557377,
      "grad_norm": 2.8417506217956543,
      "learning_rate": 4.2599218203622824e-05,
      "loss": 2.1193,
      "step": 2185
    },
    {
      "epoch": 0.7558239861949957,
      "grad_norm": 2.111912727355957,
      "learning_rate": 4.256709799638673e-05,
      "loss": 1.9514,
      "step": 2190
    },
    {
      "epoch": 0.7575496117342536,
      "grad_norm": 2.490781307220459,
      "learning_rate": 4.253492040391623e-05,
      "loss": 1.9244,
      "step": 2195
    },
    {
      "epoch": 0.7592752372735116,
      "grad_norm": 2.67514705657959,
      "learning_rate": 4.2502685531323664e-05,
      "loss": 1.9146,
      "step": 2200
    },
    {
      "epoch": 0.7592752372735116,
      "eval_loss": 1.9297266006469727,
      "eval_runtime": 302.2199,
      "eval_samples_per_second": 17.047,
      "eval_steps_per_second": 8.524,
      "step": 2200
    },
    {
      "epoch": 0.7610008628127696,
      "grad_norm": 2.626239061355591,
      "learning_rate": 4.247039348390848e-05,
      "loss": 1.884,
      "step": 2205
    },
    {
      "epoch": 0.7627264883520276,
      "grad_norm": 2.3088107109069824,
      "learning_rate": 4.243804436715689e-05,
      "loss": 2.0083,
      "step": 2210
    },
    {
      "epoch": 0.7644521138912856,
      "grad_norm": 2.086609125137329,
      "learning_rate": 4.240563828674153e-05,
      "loss": 1.8736,
      "step": 2215
    },
    {
      "epoch": 0.7661777394305436,
      "grad_norm": 2.0209407806396484,
      "learning_rate": 4.237317534852113e-05,
      "loss": 2.0333,
      "step": 2220
    },
    {
      "epoch": 0.7679033649698016,
      "grad_norm": 2.1504063606262207,
      "learning_rate": 4.234065565854014e-05,
      "loss": 2.0334,
      "step": 2225
    },
    {
      "epoch": 0.7696289905090595,
      "grad_norm": 2.197681188583374,
      "learning_rate": 4.2308079323028406e-05,
      "loss": 2.0562,
      "step": 2230
    },
    {
      "epoch": 0.7713546160483176,
      "grad_norm": 2.4555180072784424,
      "learning_rate": 4.227544644840081e-05,
      "loss": 1.9342,
      "step": 2235
    },
    {
      "epoch": 0.7730802415875755,
      "grad_norm": 2.0174148082733154,
      "learning_rate": 4.224275714125692e-05,
      "loss": 2.029,
      "step": 2240
    },
    {
      "epoch": 0.7748058671268335,
      "grad_norm": 2.2484474182128906,
      "learning_rate": 4.221001150838065e-05,
      "loss": 2.0019,
      "step": 2245
    },
    {
      "epoch": 0.7765314926660914,
      "grad_norm": 2.566082715988159,
      "learning_rate": 4.2177209656739926e-05,
      "loss": 1.8626,
      "step": 2250
    },
    {
      "epoch": 0.7782571182053495,
      "grad_norm": 1.6003049612045288,
      "learning_rate": 4.21443516934863e-05,
      "loss": 1.8814,
      "step": 2255
    },
    {
      "epoch": 0.7799827437446074,
      "grad_norm": 2.45615553855896,
      "learning_rate": 4.211143772595463e-05,
      "loss": 1.9089,
      "step": 2260
    },
    {
      "epoch": 0.7817083692838654,
      "grad_norm": 2.2095563411712646,
      "learning_rate": 4.207846786166272e-05,
      "loss": 1.9159,
      "step": 2265
    },
    {
      "epoch": 0.7834339948231234,
      "grad_norm": 2.210899591445923,
      "learning_rate": 4.2045442208310945e-05,
      "loss": 1.9492,
      "step": 2270
    },
    {
      "epoch": 0.7851596203623814,
      "grad_norm": 1.975152850151062,
      "learning_rate": 4.201236087378198e-05,
      "loss": 1.8727,
      "step": 2275
    },
    {
      "epoch": 0.7868852459016393,
      "grad_norm": 2.727417469024658,
      "learning_rate": 4.197922396614031e-05,
      "loss": 2.0871,
      "step": 2280
    },
    {
      "epoch": 0.7886108714408974,
      "grad_norm": 2.2407732009887695,
      "learning_rate": 4.194603159363203e-05,
      "loss": 2.0113,
      "step": 2285
    },
    {
      "epoch": 0.7903364969801553,
      "grad_norm": 2.2222049236297607,
      "learning_rate": 4.1912783864684365e-05,
      "loss": 1.7017,
      "step": 2290
    },
    {
      "epoch": 0.7920621225194133,
      "grad_norm": 2.2753288745880127,
      "learning_rate": 4.187948088790541e-05,
      "loss": 1.96,
      "step": 2295
    },
    {
      "epoch": 0.7937877480586712,
      "grad_norm": 2.654926300048828,
      "learning_rate": 4.1846122772083704e-05,
      "loss": 1.9548,
      "step": 2300
    },
    {
      "epoch": 0.7937877480586712,
      "eval_loss": 1.9208946228027344,
      "eval_runtime": 301.6497,
      "eval_samples_per_second": 17.079,
      "eval_steps_per_second": 8.54,
      "step": 2300
    },
    {
      "epoch": 0.7955133735979293,
      "grad_norm": 1.782000184059143,
      "learning_rate": 4.1812709626187915e-05,
      "loss": 1.911,
      "step": 2305
    },
    {
      "epoch": 0.7972389991371872,
      "grad_norm": 1.9762415885925293,
      "learning_rate": 4.177924155936649e-05,
      "loss": 2.1067,
      "step": 2310
    },
    {
      "epoch": 0.7989646246764452,
      "grad_norm": 2.457301139831543,
      "learning_rate": 4.1745718680947254e-05,
      "loss": 1.8514,
      "step": 2315
    },
    {
      "epoch": 0.8006902502157032,
      "grad_norm": 2.135556697845459,
      "learning_rate": 4.1712141100437097e-05,
      "loss": 1.9824,
      "step": 2320
    },
    {
      "epoch": 0.8024158757549612,
      "grad_norm": 1.9532650709152222,
      "learning_rate": 4.167850892752161e-05,
      "loss": 1.951,
      "step": 2325
    },
    {
      "epoch": 0.8041415012942191,
      "grad_norm": 2.566249370574951,
      "learning_rate": 4.164482227206469e-05,
      "loss": 1.9881,
      "step": 2330
    },
    {
      "epoch": 0.8058671268334772,
      "grad_norm": 2.3885719776153564,
      "learning_rate": 4.161108124410823e-05,
      "loss": 1.8149,
      "step": 2335
    },
    {
      "epoch": 0.8075927523727351,
      "grad_norm": 2.2104086875915527,
      "learning_rate": 4.157728595387174e-05,
      "loss": 1.95,
      "step": 2340
    },
    {
      "epoch": 0.8093183779119931,
      "grad_norm": 2.3621983528137207,
      "learning_rate": 4.154343651175197e-05,
      "loss": 1.9801,
      "step": 2345
    },
    {
      "epoch": 0.811044003451251,
      "grad_norm": 2.2502288818359375,
      "learning_rate": 4.150953302832257e-05,
      "loss": 2.0279,
      "step": 2350
    },
    {
      "epoch": 0.8127696289905091,
      "grad_norm": 2.232738971710205,
      "learning_rate": 4.147557561433372e-05,
      "loss": 1.9564,
      "step": 2355
    },
    {
      "epoch": 0.814495254529767,
      "grad_norm": 2.3644847869873047,
      "learning_rate": 4.1441564380711794e-05,
      "loss": 1.96,
      "step": 2360
    },
    {
      "epoch": 0.816220880069025,
      "grad_norm": 2.3607490062713623,
      "learning_rate": 4.140749943855894e-05,
      "loss": 1.7972,
      "step": 2365
    },
    {
      "epoch": 0.817946505608283,
      "grad_norm": 2.5824742317199707,
      "learning_rate": 4.137338089915278e-05,
      "loss": 2.1149,
      "step": 2370
    },
    {
      "epoch": 0.819672131147541,
      "grad_norm": 2.218419075012207,
      "learning_rate": 4.1339208873946e-05,
      "loss": 1.8961,
      "step": 2375
    },
    {
      "epoch": 0.8213977566867989,
      "grad_norm": 2.3559157848358154,
      "learning_rate": 4.1304983474566e-05,
      "loss": 1.7649,
      "step": 2380
    },
    {
      "epoch": 0.823123382226057,
      "grad_norm": 2.258378744125366,
      "learning_rate": 4.127070481281457e-05,
      "loss": 1.8757,
      "step": 2385
    },
    {
      "epoch": 0.8248490077653149,
      "grad_norm": 2.4603896141052246,
      "learning_rate": 4.123637300066745e-05,
      "loss": 1.8669,
      "step": 2390
    },
    {
      "epoch": 0.8265746333045729,
      "grad_norm": 2.8625454902648926,
      "learning_rate": 4.1201988150274016e-05,
      "loss": 1.9864,
      "step": 2395
    },
    {
      "epoch": 0.8283002588438308,
      "grad_norm": 2.662597417831421,
      "learning_rate": 4.1167550373956906e-05,
      "loss": 1.8417,
      "step": 2400
    },
    {
      "epoch": 0.8283002588438308,
      "eval_loss": 1.9119234085083008,
      "eval_runtime": 302.0038,
      "eval_samples_per_second": 17.059,
      "eval_steps_per_second": 8.53,
      "step": 2400
    },
    {
      "epoch": 0.8300258843830889,
      "grad_norm": 2.1084907054901123,
      "learning_rate": 4.113305978421164e-05,
      "loss": 1.9729,
      "step": 2405
    },
    {
      "epoch": 0.8317515099223468,
      "grad_norm": 2.881934642791748,
      "learning_rate": 4.1098516493706255e-05,
      "loss": 1.9123,
      "step": 2410
    },
    {
      "epoch": 0.8334771354616048,
      "grad_norm": 2.4902803897857666,
      "learning_rate": 4.106392061528096e-05,
      "loss": 1.9329,
      "step": 2415
    },
    {
      "epoch": 0.8352027610008628,
      "grad_norm": 2.3666210174560547,
      "learning_rate": 4.102927226194774e-05,
      "loss": 1.9862,
      "step": 2420
    },
    {
      "epoch": 0.8369283865401208,
      "grad_norm": 2.1313118934631348,
      "learning_rate": 4.099457154688998e-05,
      "loss": 1.9119,
      "step": 2425
    },
    {
      "epoch": 0.8386540120793787,
      "grad_norm": 2.4353229999542236,
      "learning_rate": 4.095981858346214e-05,
      "loss": 2.1046,
      "step": 2430
    },
    {
      "epoch": 0.8403796376186368,
      "grad_norm": 2.237800121307373,
      "learning_rate": 4.092501348518934e-05,
      "loss": 1.9122,
      "step": 2435
    },
    {
      "epoch": 0.8421052631578947,
      "grad_norm": 2.092580556869507,
      "learning_rate": 4.089015636576701e-05,
      "loss": 1.9083,
      "step": 2440
    },
    {
      "epoch": 0.8438308886971527,
      "grad_norm": 2.435600519180298,
      "learning_rate": 4.085524733906051e-05,
      "loss": 2.1172,
      "step": 2445
    },
    {
      "epoch": 0.8455565142364107,
      "grad_norm": 2.481276512145996,
      "learning_rate": 4.082028651910475e-05,
      "loss": 1.8157,
      "step": 2450
    },
    {
      "epoch": 0.8472821397756687,
      "grad_norm": 2.514195442199707,
      "learning_rate": 4.078527402010387e-05,
      "loss": 1.8188,
      "step": 2455
    },
    {
      "epoch": 0.8490077653149266,
      "grad_norm": 2.2904932498931885,
      "learning_rate": 4.075020995643078e-05,
      "loss": 1.899,
      "step": 2460
    },
    {
      "epoch": 0.8507333908541846,
      "grad_norm": 2.3825368881225586,
      "learning_rate": 4.0715094442626854e-05,
      "loss": 1.8865,
      "step": 2465
    },
    {
      "epoch": 0.8524590163934426,
      "grad_norm": 2.578972101211548,
      "learning_rate": 4.067992759340153e-05,
      "loss": 2.015,
      "step": 2470
    },
    {
      "epoch": 0.8541846419327006,
      "grad_norm": 2.1254749298095703,
      "learning_rate": 4.0644709523631965e-05,
      "loss": 1.8813,
      "step": 2475
    },
    {
      "epoch": 0.8559102674719585,
      "grad_norm": 2.1583614349365234,
      "learning_rate": 4.060944034836258e-05,
      "loss": 1.9358,
      "step": 2480
    },
    {
      "epoch": 0.8576358930112166,
      "grad_norm": 2.4616079330444336,
      "learning_rate": 4.05741201828048e-05,
      "loss": 1.8427,
      "step": 2485
    },
    {
      "epoch": 0.8593615185504746,
      "grad_norm": 2.3183586597442627,
      "learning_rate": 4.0538749142336575e-05,
      "loss": 1.9045,
      "step": 2490
    },
    {
      "epoch": 0.8610871440897325,
      "grad_norm": 2.034942865371704,
      "learning_rate": 4.050332734250205e-05,
      "loss": 1.9459,
      "step": 2495
    },
    {
      "epoch": 0.8628127696289906,
      "grad_norm": 2.6776649951934814,
      "learning_rate": 4.0467854899011205e-05,
      "loss": 1.8702,
      "step": 2500
    },
    {
      "epoch": 0.8628127696289906,
      "eval_loss": 1.8977159261703491,
      "eval_runtime": 302.1102,
      "eval_samples_per_second": 17.053,
      "eval_steps_per_second": 8.527,
      "step": 2500
    },
    {
      "epoch": 0.8645383951682485,
      "grad_norm": 2.167349100112915,
      "learning_rate": 4.043233192773943e-05,
      "loss": 1.8343,
      "step": 2505
    },
    {
      "epoch": 0.8662640207075065,
      "grad_norm": 2.327655076980591,
      "learning_rate": 4.0396758544727184e-05,
      "loss": 1.974,
      "step": 2510
    },
    {
      "epoch": 0.8679896462467644,
      "grad_norm": 2.2462151050567627,
      "learning_rate": 4.0361134866179594e-05,
      "loss": 1.8235,
      "step": 2515
    },
    {
      "epoch": 0.8697152717860225,
      "grad_norm": 1.9900916814804077,
      "learning_rate": 4.032546100846608e-05,
      "loss": 1.9141,
      "step": 2520
    },
    {
      "epoch": 0.8714408973252804,
      "grad_norm": 2.410219192504883,
      "learning_rate": 4.0289737088120005e-05,
      "loss": 1.7344,
      "step": 2525
    },
    {
      "epoch": 0.8731665228645384,
      "grad_norm": 2.5869226455688477,
      "learning_rate": 4.025396322183823e-05,
      "loss": 1.9614,
      "step": 2530
    },
    {
      "epoch": 0.8748921484037964,
      "grad_norm": 2.80130934715271,
      "learning_rate": 4.0218139526480817e-05,
      "loss": 1.9612,
      "step": 2535
    },
    {
      "epoch": 0.8766177739430544,
      "grad_norm": 2.0265555381774902,
      "learning_rate": 4.018226611907054e-05,
      "loss": 1.7857,
      "step": 2540
    },
    {
      "epoch": 0.8783433994823123,
      "grad_norm": 2.6124918460845947,
      "learning_rate": 4.014634311679264e-05,
      "loss": 1.8979,
      "step": 2545
    },
    {
      "epoch": 0.8800690250215704,
      "grad_norm": 2.981497049331665,
      "learning_rate": 4.011037063699429e-05,
      "loss": 1.8758,
      "step": 2550
    },
    {
      "epoch": 0.8817946505608283,
      "grad_norm": 2.2397921085357666,
      "learning_rate": 4.007434879718434e-05,
      "loss": 1.9139,
      "step": 2555
    },
    {
      "epoch": 0.8835202761000863,
      "grad_norm": 2.3853726387023926,
      "learning_rate": 4.0038277715032854e-05,
      "loss": 1.9357,
      "step": 2560
    },
    {
      "epoch": 0.8852459016393442,
      "grad_norm": 2.377612829208374,
      "learning_rate": 4.000215750837079e-05,
      "loss": 2.0071,
      "step": 2565
    },
    {
      "epoch": 0.8869715271786023,
      "grad_norm": 2.7966396808624268,
      "learning_rate": 3.99659882951895e-05,
      "loss": 2.0054,
      "step": 2570
    },
    {
      "epoch": 0.8886971527178602,
      "grad_norm": 2.617861270904541,
      "learning_rate": 3.9929770193640524e-05,
      "loss": 1.6684,
      "step": 2575
    },
    {
      "epoch": 0.8904227782571182,
      "grad_norm": 2.1549582481384277,
      "learning_rate": 3.9893503322035016e-05,
      "loss": 1.8011,
      "step": 2580
    },
    {
      "epoch": 0.8921484037963762,
      "grad_norm": 1.9650652408599854,
      "learning_rate": 3.98571877988435e-05,
      "loss": 1.8561,
      "step": 2585
    },
    {
      "epoch": 0.8938740293356342,
      "grad_norm": 2.5645365715026855,
      "learning_rate": 3.982082374269539e-05,
      "loss": 1.871,
      "step": 2590
    },
    {
      "epoch": 0.8955996548748921,
      "grad_norm": 2.3952324390411377,
      "learning_rate": 3.978441127237867e-05,
      "loss": 1.9874,
      "step": 2595
    },
    {
      "epoch": 0.8973252804141502,
      "grad_norm": 2.7994961738586426,
      "learning_rate": 3.9747950506839446e-05,
      "loss": 1.9438,
      "step": 2600
    },
    {
      "epoch": 0.8973252804141502,
      "eval_loss": 1.8913017511367798,
      "eval_runtime": 302.3935,
      "eval_samples_per_second": 17.037,
      "eval_steps_per_second": 8.519,
      "step": 2600
    },
    {
      "epoch": 0.8990509059534081,
      "grad_norm": 2.626634359359741,
      "learning_rate": 3.971144156518161e-05,
      "loss": 1.9613,
      "step": 2605
    },
    {
      "epoch": 0.9007765314926661,
      "grad_norm": 2.3398921489715576,
      "learning_rate": 3.96748845666664e-05,
      "loss": 1.832,
      "step": 2610
    },
    {
      "epoch": 0.902502157031924,
      "grad_norm": 2.216082811355591,
      "learning_rate": 3.963827963071208e-05,
      "loss": 1.9109,
      "step": 2615
    },
    {
      "epoch": 0.9042277825711821,
      "grad_norm": 2.771697759628296,
      "learning_rate": 3.960162687689348e-05,
      "loss": 1.7883,
      "step": 2620
    },
    {
      "epoch": 0.90595340811044,
      "grad_norm": 2.369140148162842,
      "learning_rate": 3.9564926424941626e-05,
      "loss": 1.8389,
      "step": 2625
    },
    {
      "epoch": 0.907679033649698,
      "grad_norm": 2.9057772159576416,
      "learning_rate": 3.952817839474337e-05,
      "loss": 1.8656,
      "step": 2630
    },
    {
      "epoch": 0.909404659188956,
      "grad_norm": 2.2163801193237305,
      "learning_rate": 3.9491382906341e-05,
      "loss": 1.8329,
      "step": 2635
    },
    {
      "epoch": 0.911130284728214,
      "grad_norm": 2.076720714569092,
      "learning_rate": 3.945454007993179e-05,
      "loss": 1.8743,
      "step": 2640
    },
    {
      "epoch": 0.9128559102674719,
      "grad_norm": 2.7837471961975098,
      "learning_rate": 3.941765003586768e-05,
      "loss": 1.8988,
      "step": 2645
    },
    {
      "epoch": 0.91458153580673,
      "grad_norm": 2.590425729751587,
      "learning_rate": 3.938071289465486e-05,
      "loss": 1.9835,
      "step": 2650
    },
    {
      "epoch": 0.9163071613459879,
      "grad_norm": 2.4109318256378174,
      "learning_rate": 3.934372877695335e-05,
      "loss": 1.8392,
      "step": 2655
    },
    {
      "epoch": 0.9180327868852459,
      "grad_norm": 2.7282626628875732,
      "learning_rate": 3.930669780357663e-05,
      "loss": 1.749,
      "step": 2660
    },
    {
      "epoch": 0.9197584124245038,
      "grad_norm": 2.4104480743408203,
      "learning_rate": 3.926962009549124e-05,
      "loss": 1.8788,
      "step": 2665
    },
    {
      "epoch": 0.9214840379637619,
      "grad_norm": 1.9991230964660645,
      "learning_rate": 3.923249577381639e-05,
      "loss": 1.8989,
      "step": 2670
    },
    {
      "epoch": 0.9232096635030198,
      "grad_norm": 2.0167860984802246,
      "learning_rate": 3.9195324959823556e-05,
      "loss": 1.9059,
      "step": 2675
    },
    {
      "epoch": 0.9249352890422778,
      "grad_norm": 2.7636561393737793,
      "learning_rate": 3.915810777493608e-05,
      "loss": 1.9785,
      "step": 2680
    },
    {
      "epoch": 0.9266609145815358,
      "grad_norm": 2.691650390625,
      "learning_rate": 3.912084434072879e-05,
      "loss": 2.0172,
      "step": 2685
    },
    {
      "epoch": 0.9283865401207938,
      "grad_norm": 1.787191390991211,
      "learning_rate": 3.90835347789276e-05,
      "loss": 1.8876,
      "step": 2690
    },
    {
      "epoch": 0.9301121656600517,
      "grad_norm": 2.5893642902374268,
      "learning_rate": 3.90461792114091e-05,
      "loss": 1.8251,
      "step": 2695
    },
    {
      "epoch": 0.9318377911993098,
      "grad_norm": 2.3669049739837646,
      "learning_rate": 3.900877776020014e-05,
      "loss": 1.8781,
      "step": 2700
    },
    {
      "epoch": 0.9318377911993098,
      "eval_loss": 1.882214903831482,
      "eval_runtime": 302.3896,
      "eval_samples_per_second": 17.038,
      "eval_steps_per_second": 8.519,
      "step": 2700
    },
    {
      "epoch": 0.9335634167385677,
      "grad_norm": 2.6125574111938477,
      "learning_rate": 3.897133054747749e-05,
      "loss": 1.8778,
      "step": 2705
    },
    {
      "epoch": 0.9352890422778257,
      "grad_norm": 2.380119562149048,
      "learning_rate": 3.893383769556739e-05,
      "loss": 1.653,
      "step": 2710
    },
    {
      "epoch": 0.9370146678170836,
      "grad_norm": 2.2669785022735596,
      "learning_rate": 3.889629932694517e-05,
      "loss": 1.8139,
      "step": 2715
    },
    {
      "epoch": 0.9387402933563417,
      "grad_norm": 2.4353675842285156,
      "learning_rate": 3.885871556423484e-05,
      "loss": 1.8168,
      "step": 2720
    },
    {
      "epoch": 0.9404659188955996,
      "grad_norm": 1.8357574939727783,
      "learning_rate": 3.8821086530208705e-05,
      "loss": 1.8369,
      "step": 2725
    },
    {
      "epoch": 0.9421915444348576,
      "grad_norm": 2.4109318256378174,
      "learning_rate": 3.878341234778695e-05,
      "loss": 1.9347,
      "step": 2730
    },
    {
      "epoch": 0.9439171699741156,
      "grad_norm": 2.440887451171875,
      "learning_rate": 3.874569314003725e-05,
      "loss": 1.8198,
      "step": 2735
    },
    {
      "epoch": 0.9456427955133736,
      "grad_norm": 2.4406683444976807,
      "learning_rate": 3.870792903017434e-05,
      "loss": 1.8764,
      "step": 2740
    },
    {
      "epoch": 0.9473684210526315,
      "grad_norm": 2.6665756702423096,
      "learning_rate": 3.867012014155965e-05,
      "loss": 1.837,
      "step": 2745
    },
    {
      "epoch": 0.9490940465918896,
      "grad_norm": 2.401564121246338,
      "learning_rate": 3.86322665977009e-05,
      "loss": 2.0625,
      "step": 2750
    },
    {
      "epoch": 0.9508196721311475,
      "grad_norm": 2.2134623527526855,
      "learning_rate": 3.8594368522251645e-05,
      "loss": 1.8547,
      "step": 2755
    },
    {
      "epoch": 0.9525452976704055,
      "grad_norm": 2.3341317176818848,
      "learning_rate": 3.855642603901094e-05,
      "loss": 1.8551,
      "step": 2760
    },
    {
      "epoch": 0.9542709232096636,
      "grad_norm": 2.3352952003479004,
      "learning_rate": 3.851843927192289e-05,
      "loss": 1.6988,
      "step": 2765
    },
    {
      "epoch": 0.9559965487489215,
      "grad_norm": 2.667637825012207,
      "learning_rate": 3.848040834507627e-05,
      "loss": 2.0281,
      "step": 2770
    },
    {
      "epoch": 0.9577221742881795,
      "grad_norm": 2.0425801277160645,
      "learning_rate": 3.844233338270409e-05,
      "loss": 1.8869,
      "step": 2775
    },
    {
      "epoch": 0.9594477998274374,
      "grad_norm": 2.460533380508423,
      "learning_rate": 3.840421450918323e-05,
      "loss": 1.7047,
      "step": 2780
    },
    {
      "epoch": 0.9611734253666955,
      "grad_norm": 2.3015127182006836,
      "learning_rate": 3.836605184903398e-05,
      "loss": 1.9686,
      "step": 2785
    },
    {
      "epoch": 0.9628990509059534,
      "grad_norm": 2.19460391998291,
      "learning_rate": 3.8327845526919714e-05,
      "loss": 1.8924,
      "step": 2790
    },
    {
      "epoch": 0.9646246764452114,
      "grad_norm": 2.4556567668914795,
      "learning_rate": 3.828959566764639e-05,
      "loss": 1.7634,
      "step": 2795
    },
    {
      "epoch": 0.9663503019844694,
      "grad_norm": 2.8660101890563965,
      "learning_rate": 3.825130239616219e-05,
      "loss": 2.0422,
      "step": 2800
    },
    {
      "epoch": 0.9663503019844694,
      "eval_loss": 1.8701239824295044,
      "eval_runtime": 302.5197,
      "eval_samples_per_second": 17.03,
      "eval_steps_per_second": 8.515,
      "step": 2800
    },
    {
      "epoch": 0.9680759275237274,
      "grad_norm": 2.3150482177734375,
      "learning_rate": 3.8212965837557135e-05,
      "loss": 2.0257,
      "step": 2805
    },
    {
      "epoch": 0.9698015530629853,
      "grad_norm": 2.5035393238067627,
      "learning_rate": 3.8174586117062624e-05,
      "loss": 1.882,
      "step": 2810
    },
    {
      "epoch": 0.9715271786022434,
      "grad_norm": 2.113621473312378,
      "learning_rate": 3.813616336005106e-05,
      "loss": 1.9902,
      "step": 2815
    },
    {
      "epoch": 0.9732528041415013,
      "grad_norm": 2.3732941150665283,
      "learning_rate": 3.8097697692035416e-05,
      "loss": 1.788,
      "step": 2820
    },
    {
      "epoch": 0.9749784296807593,
      "grad_norm": 3.191437244415283,
      "learning_rate": 3.805918923866887e-05,
      "loss": 1.8787,
      "step": 2825
    },
    {
      "epoch": 0.9767040552200172,
      "grad_norm": 2.5514721870422363,
      "learning_rate": 3.802063812574435e-05,
      "loss": 2.0136,
      "step": 2830
    },
    {
      "epoch": 0.9784296807592753,
      "grad_norm": 2.447261333465576,
      "learning_rate": 3.7982044479194115e-05,
      "loss": 1.7947,
      "step": 2835
    },
    {
      "epoch": 0.9801553062985332,
      "grad_norm": 2.3448352813720703,
      "learning_rate": 3.7943408425089395e-05,
      "loss": 1.8601,
      "step": 2840
    },
    {
      "epoch": 0.9818809318377912,
      "grad_norm": 2.356313467025757,
      "learning_rate": 3.790473008963993e-05,
      "loss": 1.9215,
      "step": 2845
    },
    {
      "epoch": 0.9836065573770492,
      "grad_norm": 1.9191211462020874,
      "learning_rate": 3.78660095991936e-05,
      "loss": 2.012,
      "step": 2850
    },
    {
      "epoch": 0.9853321829163072,
      "grad_norm": 2.591017961502075,
      "learning_rate": 3.782724708023596e-05,
      "loss": 1.8174,
      "step": 2855
    },
    {
      "epoch": 0.9870578084555651,
      "grad_norm": 2.191560983657837,
      "learning_rate": 3.7788442659389874e-05,
      "loss": 1.8889,
      "step": 2860
    },
    {
      "epoch": 0.9887834339948232,
      "grad_norm": 3.2295660972595215,
      "learning_rate": 3.7749596463415104e-05,
      "loss": 1.8359,
      "step": 2865
    },
    {
      "epoch": 0.9905090595340811,
      "grad_norm": 2.273087739944458,
      "learning_rate": 3.771070861920783e-05,
      "loss": 1.852,
      "step": 2870
    },
    {
      "epoch": 0.9922346850733391,
      "grad_norm": 2.3086605072021484,
      "learning_rate": 3.767177925380032e-05,
      "loss": 1.8708,
      "step": 2875
    },
    {
      "epoch": 0.993960310612597,
      "grad_norm": 2.7244808673858643,
      "learning_rate": 3.763280849436045e-05,
      "loss": 1.9009,
      "step": 2880
    },
    {
      "epoch": 0.9956859361518551,
      "grad_norm": 2.008148431777954,
      "learning_rate": 3.759379646819133e-05,
      "loss": 1.7552,
      "step": 2885
    },
    {
      "epoch": 0.997411561691113,
      "grad_norm": 2.3033111095428467,
      "learning_rate": 3.755474330273089e-05,
      "loss": 1.9489,
      "step": 2890
    },
    {
      "epoch": 0.999137187230371,
      "grad_norm": 2.8332157135009766,
      "learning_rate": 3.751564912555141e-05,
      "loss": 1.854,
      "step": 2895
    },
    {
      "epoch": 1.000862812769629,
      "grad_norm": 2.137007236480713,
      "learning_rate": 3.747651406435916e-05,
      "loss": 1.9024,
      "step": 2900
    },
    {
      "epoch": 1.000862812769629,
      "eval_loss": 1.8636908531188965,
      "eval_runtime": 302.7045,
      "eval_samples_per_second": 17.02,
      "eval_steps_per_second": 8.51,
      "step": 2900
    },
    {
      "epoch": 1.002588438308887,
      "grad_norm": 2.8075873851776123,
      "learning_rate": 3.7437338246993966e-05,
      "loss": 1.5814,
      "step": 2905
    },
    {
      "epoch": 1.004314063848145,
      "grad_norm": 2.293431043624878,
      "learning_rate": 3.7398121801428776e-05,
      "loss": 1.5944,
      "step": 2910
    },
    {
      "epoch": 1.0060396893874028,
      "grad_norm": 2.21459698677063,
      "learning_rate": 3.735886485576927e-05,
      "loss": 1.6369,
      "step": 2915
    },
    {
      "epoch": 1.007765314926661,
      "grad_norm": 2.4781854152679443,
      "learning_rate": 3.731956753825342e-05,
      "loss": 1.5003,
      "step": 2920
    },
    {
      "epoch": 1.009490940465919,
      "grad_norm": 2.826211929321289,
      "learning_rate": 3.728022997725107e-05,
      "loss": 1.6768,
      "step": 2925
    },
    {
      "epoch": 1.0112165660051768,
      "grad_norm": 2.873595952987671,
      "learning_rate": 3.724085230126355e-05,
      "loss": 1.7238,
      "step": 2930
    },
    {
      "epoch": 1.0129421915444348,
      "grad_norm": 2.912402868270874,
      "learning_rate": 3.72014346389232e-05,
      "loss": 1.6192,
      "step": 2935
    },
    {
      "epoch": 1.014667817083693,
      "grad_norm": 2.2147858142852783,
      "learning_rate": 3.7161977118992996e-05,
      "loss": 1.5605,
      "step": 2940
    },
    {
      "epoch": 1.0163934426229508,
      "grad_norm": 1.9899293184280396,
      "learning_rate": 3.7122479870366115e-05,
      "loss": 1.4791,
      "step": 2945
    },
    {
      "epoch": 1.0181190681622088,
      "grad_norm": 2.2203919887542725,
      "learning_rate": 3.708294302206551e-05,
      "loss": 1.493,
      "step": 2950
    },
    {
      "epoch": 1.0198446937014667,
      "grad_norm": 2.681340217590332,
      "learning_rate": 3.70433667032435e-05,
      "loss": 1.5772,
      "step": 2955
    },
    {
      "epoch": 1.0215703192407248,
      "grad_norm": 2.4843997955322266,
      "learning_rate": 3.700375104318131e-05,
      "loss": 1.5512,
      "step": 2960
    },
    {
      "epoch": 1.0232959447799828,
      "grad_norm": 2.694164991378784,
      "learning_rate": 3.6964096171288734e-05,
      "loss": 1.7054,
      "step": 2965
    },
    {
      "epoch": 1.0250215703192407,
      "grad_norm": 2.515258312225342,
      "learning_rate": 3.692440221710359e-05,
      "loss": 1.8071,
      "step": 2970
    },
    {
      "epoch": 1.0267471958584986,
      "grad_norm": 2.1348695755004883,
      "learning_rate": 3.6884669310291416e-05,
      "loss": 1.5788,
      "step": 2975
    },
    {
      "epoch": 1.0284728213977568,
      "grad_norm": 2.3541526794433594,
      "learning_rate": 3.6844897580644963e-05,
      "loss": 1.5965,
      "step": 2980
    },
    {
      "epoch": 1.0301984469370147,
      "grad_norm": 2.5492920875549316,
      "learning_rate": 3.680508715808382e-05,
      "loss": 1.6429,
      "step": 2985
    },
    {
      "epoch": 1.0319240724762726,
      "grad_norm": 2.604522228240967,
      "learning_rate": 3.6765238172653974e-05,
      "loss": 1.7127,
      "step": 2990
    },
    {
      "epoch": 1.0336496980155305,
      "grad_norm": 2.4437973499298096,
      "learning_rate": 3.672535075452735e-05,
      "loss": 1.7474,
      "step": 2995
    },
    {
      "epoch": 1.0353753235547887,
      "grad_norm": 2.0030667781829834,
      "learning_rate": 3.668542503400145e-05,
      "loss": 1.4639,
      "step": 3000
    },
    {
      "epoch": 1.0353753235547887,
      "eval_loss": 1.8689180612564087,
      "eval_runtime": 303.2645,
      "eval_samples_per_second": 16.988,
      "eval_steps_per_second": 8.494,
      "step": 3000
    },
    {
      "epoch": 1.0371009490940466,
      "grad_norm": 2.4526991844177246,
      "learning_rate": 3.6645461141498894e-05,
      "loss": 1.4992,
      "step": 3005
    },
    {
      "epoch": 1.0388265746333045,
      "grad_norm": 2.1932785511016846,
      "learning_rate": 3.660545920756699e-05,
      "loss": 1.625,
      "step": 3010
    },
    {
      "epoch": 1.0405522001725624,
      "grad_norm": 2.4215822219848633,
      "learning_rate": 3.656541936287731e-05,
      "loss": 1.8569,
      "step": 3015
    },
    {
      "epoch": 1.0422778257118206,
      "grad_norm": 2.9305949211120605,
      "learning_rate": 3.6525341738225256e-05,
      "loss": 1.5268,
      "step": 3020
    },
    {
      "epoch": 1.0440034512510785,
      "grad_norm": 2.4744842052459717,
      "learning_rate": 3.648522646452968e-05,
      "loss": 1.6616,
      "step": 3025
    },
    {
      "epoch": 1.0457290767903364,
      "grad_norm": 2.3835484981536865,
      "learning_rate": 3.6445073672832376e-05,
      "loss": 1.6995,
      "step": 3030
    },
    {
      "epoch": 1.0474547023295946,
      "grad_norm": 2.1643497943878174,
      "learning_rate": 3.6404883494297726e-05,
      "loss": 1.5241,
      "step": 3035
    },
    {
      "epoch": 1.0491803278688525,
      "grad_norm": 2.2248311042785645,
      "learning_rate": 3.6364656060212225e-05,
      "loss": 1.4337,
      "step": 3040
    },
    {
      "epoch": 1.0509059534081104,
      "grad_norm": 2.935838222503662,
      "learning_rate": 3.632439150198408e-05,
      "loss": 1.6571,
      "step": 3045
    },
    {
      "epoch": 1.0526315789473684,
      "grad_norm": 2.2989158630371094,
      "learning_rate": 3.628408995114275e-05,
      "loss": 1.835,
      "step": 3050
    },
    {
      "epoch": 1.0543572044866265,
      "grad_norm": 2.362377882003784,
      "learning_rate": 3.6243751539338566e-05,
      "loss": 1.6627,
      "step": 3055
    },
    {
      "epoch": 1.0560828300258844,
      "grad_norm": 2.243824005126953,
      "learning_rate": 3.620337639834223e-05,
      "loss": 1.4129,
      "step": 3060
    },
    {
      "epoch": 1.0578084555651424,
      "grad_norm": 2.324777603149414,
      "learning_rate": 3.616296466004446e-05,
      "loss": 1.8261,
      "step": 3065
    },
    {
      "epoch": 1.0595340811044003,
      "grad_norm": 2.915379047393799,
      "learning_rate": 3.6122516456455514e-05,
      "loss": 1.7533,
      "step": 3070
    },
    {
      "epoch": 1.0612597066436584,
      "grad_norm": 1.8510632514953613,
      "learning_rate": 3.6082031919704765e-05,
      "loss": 1.5281,
      "step": 3075
    },
    {
      "epoch": 1.0629853321829164,
      "grad_norm": 2.059597969055176,
      "learning_rate": 3.604151118204025e-05,
      "loss": 1.6153,
      "step": 3080
    },
    {
      "epoch": 1.0647109577221743,
      "grad_norm": 2.3676867485046387,
      "learning_rate": 3.600095437582831e-05,
      "loss": 1.7719,
      "step": 3085
    },
    {
      "epoch": 1.0664365832614322,
      "grad_norm": 2.38106107711792,
      "learning_rate": 3.5960361633553074e-05,
      "loss": 1.8393,
      "step": 3090
    },
    {
      "epoch": 1.0681622088006903,
      "grad_norm": 1.759565830230713,
      "learning_rate": 3.591973308781607e-05,
      "loss": 1.5416,
      "step": 3095
    },
    {
      "epoch": 1.0698878343399483,
      "grad_norm": 2.593092203140259,
      "learning_rate": 3.587906887133577e-05,
      "loss": 1.6678,
      "step": 3100
    },
    {
      "epoch": 1.0698878343399483,
      "eval_loss": 1.8614122867584229,
      "eval_runtime": 302.4644,
      "eval_samples_per_second": 17.033,
      "eval_steps_per_second": 8.517,
      "step": 3100
    },
    {
      "epoch": 1.0716134598792062,
      "grad_norm": 2.3836324214935303,
      "learning_rate": 3.583836911694721e-05,
      "loss": 1.4811,
      "step": 3105
    },
    {
      "epoch": 1.0733390854184641,
      "grad_norm": 2.356658935546875,
      "learning_rate": 3.5797633957601465e-05,
      "loss": 1.5877,
      "step": 3110
    },
    {
      "epoch": 1.0750647109577223,
      "grad_norm": 1.996821641921997,
      "learning_rate": 3.575686352636529e-05,
      "loss": 1.644,
      "step": 3115
    },
    {
      "epoch": 1.0767903364969802,
      "grad_norm": 2.541971206665039,
      "learning_rate": 3.571605795642066e-05,
      "loss": 1.6025,
      "step": 3120
    },
    {
      "epoch": 1.0785159620362381,
      "grad_norm": 2.2934553623199463,
      "learning_rate": 3.567521738106433e-05,
      "loss": 1.4799,
      "step": 3125
    },
    {
      "epoch": 1.080241587575496,
      "grad_norm": 2.4249050617218018,
      "learning_rate": 3.563434193370741e-05,
      "loss": 1.7575,
      "step": 3130
    },
    {
      "epoch": 1.0819672131147542,
      "grad_norm": 2.746209144592285,
      "learning_rate": 3.5593431747874915e-05,
      "loss": 1.7012,
      "step": 3135
    },
    {
      "epoch": 1.0836928386540121,
      "grad_norm": 2.704127311706543,
      "learning_rate": 3.555248695720535e-05,
      "loss": 1.661,
      "step": 3140
    },
    {
      "epoch": 1.08541846419327,
      "grad_norm": 2.2903859615325928,
      "learning_rate": 3.5511507695450255e-05,
      "loss": 1.5674,
      "step": 3145
    },
    {
      "epoch": 1.087144089732528,
      "grad_norm": 2.180577278137207,
      "learning_rate": 3.5470494096473775e-05,
      "loss": 1.7397,
      "step": 3150
    },
    {
      "epoch": 1.088869715271786,
      "grad_norm": 2.4582693576812744,
      "learning_rate": 3.542944629425221e-05,
      "loss": 1.6861,
      "step": 3155
    },
    {
      "epoch": 1.090595340811044,
      "grad_norm": 2.7091267108917236,
      "learning_rate": 3.538836442287361e-05,
      "loss": 1.683,
      "step": 3160
    },
    {
      "epoch": 1.092320966350302,
      "grad_norm": 2.1817197799682617,
      "learning_rate": 3.5347248616537316e-05,
      "loss": 1.4574,
      "step": 3165
    },
    {
      "epoch": 1.0940465918895599,
      "grad_norm": 2.781985282897949,
      "learning_rate": 3.5306099009553496e-05,
      "loss": 1.4857,
      "step": 3170
    },
    {
      "epoch": 1.095772217428818,
      "grad_norm": 2.102964162826538,
      "learning_rate": 3.526491573634276e-05,
      "loss": 1.5837,
      "step": 3175
    },
    {
      "epoch": 1.097497842968076,
      "grad_norm": 2.1615302562713623,
      "learning_rate": 3.5223698931435675e-05,
      "loss": 1.6527,
      "step": 3180
    },
    {
      "epoch": 1.0992234685073339,
      "grad_norm": 2.623114585876465,
      "learning_rate": 3.518244872947236e-05,
      "loss": 1.8804,
      "step": 3185
    },
    {
      "epoch": 1.1009490940465918,
      "grad_norm": 2.3053011894226074,
      "learning_rate": 3.514116526520202e-05,
      "loss": 1.6321,
      "step": 3190
    },
    {
      "epoch": 1.10267471958585,
      "grad_norm": 2.9947361946105957,
      "learning_rate": 3.509984867348251e-05,
      "loss": 1.752,
      "step": 3195
    },
    {
      "epoch": 1.1044003451251079,
      "grad_norm": 2.6661458015441895,
      "learning_rate": 3.50584990892799e-05,
      "loss": 1.5044,
      "step": 3200
    },
    {
      "epoch": 1.1044003451251079,
      "eval_loss": 1.8534648418426514,
      "eval_runtime": 302.6269,
      "eval_samples_per_second": 17.024,
      "eval_steps_per_second": 8.512,
      "step": 3200
    },
    {
      "epoch": 1.1061259706643658,
      "grad_norm": 2.391956090927124,
      "learning_rate": 3.501711664766806e-05,
      "loss": 1.6299,
      "step": 3205
    },
    {
      "epoch": 1.1078515962036237,
      "grad_norm": 2.595496892929077,
      "learning_rate": 3.4975701483828175e-05,
      "loss": 1.5704,
      "step": 3210
    },
    {
      "epoch": 1.1095772217428819,
      "grad_norm": 2.428750514984131,
      "learning_rate": 3.49342537330483e-05,
      "loss": 1.5801,
      "step": 3215
    },
    {
      "epoch": 1.1113028472821398,
      "grad_norm": 2.648366928100586,
      "learning_rate": 3.489277353072298e-05,
      "loss": 1.7531,
      "step": 3220
    },
    {
      "epoch": 1.1130284728213977,
      "grad_norm": 2.388091564178467,
      "learning_rate": 3.485126101235273e-05,
      "loss": 1.5373,
      "step": 3225
    },
    {
      "epoch": 1.1147540983606556,
      "grad_norm": 2.959707736968994,
      "learning_rate": 3.480971631354367e-05,
      "loss": 1.6557,
      "step": 3230
    },
    {
      "epoch": 1.1164797238999138,
      "grad_norm": 2.6728017330169678,
      "learning_rate": 3.4768139570007e-05,
      "loss": 1.5573,
      "step": 3235
    },
    {
      "epoch": 1.1182053494391717,
      "grad_norm": 2.993605375289917,
      "learning_rate": 3.4726530917558614e-05,
      "loss": 1.5935,
      "step": 3240
    },
    {
      "epoch": 1.1199309749784296,
      "grad_norm": 1.99238121509552,
      "learning_rate": 3.468489049211866e-05,
      "loss": 1.5243,
      "step": 3245
    },
    {
      "epoch": 1.1216566005176876,
      "grad_norm": 2.628077268600464,
      "learning_rate": 3.464321842971105e-05,
      "loss": 1.555,
      "step": 3250
    },
    {
      "epoch": 1.1233822260569457,
      "grad_norm": 2.63280987739563,
      "learning_rate": 3.460151486646307e-05,
      "loss": 1.5634,
      "step": 3255
    },
    {
      "epoch": 1.1251078515962036,
      "grad_norm": 2.9389944076538086,
      "learning_rate": 3.455977993860486e-05,
      "loss": 1.4843,
      "step": 3260
    },
    {
      "epoch": 1.1268334771354616,
      "grad_norm": 2.559810161590576,
      "learning_rate": 3.451801378246907e-05,
      "loss": 1.7854,
      "step": 3265
    },
    {
      "epoch": 1.1285591026747195,
      "grad_norm": 2.167203903198242,
      "learning_rate": 3.447621653449033e-05,
      "loss": 1.6619,
      "step": 3270
    },
    {
      "epoch": 1.1302847282139776,
      "grad_norm": 2.416879653930664,
      "learning_rate": 3.443438833120484e-05,
      "loss": 1.6062,
      "step": 3275
    },
    {
      "epoch": 1.1320103537532356,
      "grad_norm": 2.418300151824951,
      "learning_rate": 3.439252930924994e-05,
      "loss": 1.613,
      "step": 3280
    },
    {
      "epoch": 1.1337359792924935,
      "grad_norm": 3.109010696411133,
      "learning_rate": 3.435063960536361e-05,
      "loss": 1.56,
      "step": 3285
    },
    {
      "epoch": 1.1354616048317516,
      "grad_norm": 2.361459493637085,
      "learning_rate": 3.4308719356384075e-05,
      "loss": 1.5538,
      "step": 3290
    },
    {
      "epoch": 1.1371872303710095,
      "grad_norm": 2.8539047241210938,
      "learning_rate": 3.426676869924934e-05,
      "loss": 1.6157,
      "step": 3295
    },
    {
      "epoch": 1.1389128559102675,
      "grad_norm": 2.4676990509033203,
      "learning_rate": 3.422478777099674e-05,
      "loss": 1.7035,
      "step": 3300
    },
    {
      "epoch": 1.1389128559102675,
      "eval_loss": 1.8503129482269287,
      "eval_runtime": 302.6385,
      "eval_samples_per_second": 17.024,
      "eval_steps_per_second": 8.512,
      "step": 3300
    },
    {
      "epoch": 1.1406384814495254,
      "grad_norm": 3.0828700065612793,
      "learning_rate": 3.41827767087625e-05,
      "loss": 1.7343,
      "step": 3305
    },
    {
      "epoch": 1.1423641069887833,
      "grad_norm": 2.469508171081543,
      "learning_rate": 3.4140735649781274e-05,
      "loss": 1.5216,
      "step": 3310
    },
    {
      "epoch": 1.1440897325280415,
      "grad_norm": 1.7305721044540405,
      "learning_rate": 3.4098664731385703e-05,
      "loss": 1.7536,
      "step": 3315
    },
    {
      "epoch": 1.1458153580672994,
      "grad_norm": 3.386140823364258,
      "learning_rate": 3.405656409100597e-05,
      "loss": 1.6519,
      "step": 3320
    },
    {
      "epoch": 1.1475409836065573,
      "grad_norm": 2.3354978561401367,
      "learning_rate": 3.4014433866169354e-05,
      "loss": 1.5974,
      "step": 3325
    },
    {
      "epoch": 1.1492666091458155,
      "grad_norm": 2.5100488662719727,
      "learning_rate": 3.397227419449977e-05,
      "loss": 1.6622,
      "step": 3330
    },
    {
      "epoch": 1.1509922346850734,
      "grad_norm": 2.3297994136810303,
      "learning_rate": 3.3930085213717324e-05,
      "loss": 1.5698,
      "step": 3335
    },
    {
      "epoch": 1.1527178602243313,
      "grad_norm": 2.6312661170959473,
      "learning_rate": 3.388786706163787e-05,
      "loss": 1.7524,
      "step": 3340
    },
    {
      "epoch": 1.1544434857635892,
      "grad_norm": 1.9261573553085327,
      "learning_rate": 3.384561987617255e-05,
      "loss": 1.5639,
      "step": 3345
    },
    {
      "epoch": 1.1561691113028472,
      "grad_norm": 2.383986711502075,
      "learning_rate": 3.380334379532735e-05,
      "loss": 1.5012,
      "step": 3350
    },
    {
      "epoch": 1.1578947368421053,
      "grad_norm": 2.5122551918029785,
      "learning_rate": 3.376103895720264e-05,
      "loss": 1.6886,
      "step": 3355
    },
    {
      "epoch": 1.1596203623813632,
      "grad_norm": 2.2442786693573,
      "learning_rate": 3.3718705499992734e-05,
      "loss": 1.5278,
      "step": 3360
    },
    {
      "epoch": 1.1613459879206212,
      "grad_norm": 2.7572991847991943,
      "learning_rate": 3.3676343561985434e-05,
      "loss": 1.7021,
      "step": 3365
    },
    {
      "epoch": 1.1630716134598793,
      "grad_norm": 2.5837645530700684,
      "learning_rate": 3.363395328156158e-05,
      "loss": 1.6171,
      "step": 3370
    },
    {
      "epoch": 1.1647972389991372,
      "grad_norm": 2.663529634475708,
      "learning_rate": 3.359153479719459e-05,
      "loss": 1.6415,
      "step": 3375
    },
    {
      "epoch": 1.1665228645383952,
      "grad_norm": 2.0528740882873535,
      "learning_rate": 3.354908824745003e-05,
      "loss": 1.4272,
      "step": 3380
    },
    {
      "epoch": 1.168248490077653,
      "grad_norm": 3.0520613193511963,
      "learning_rate": 3.350661377098512e-05,
      "loss": 1.5583,
      "step": 3385
    },
    {
      "epoch": 1.1699741156169112,
      "grad_norm": 2.3505377769470215,
      "learning_rate": 3.346411150654832e-05,
      "loss": 1.4458,
      "step": 3390
    },
    {
      "epoch": 1.1716997411561692,
      "grad_norm": 2.5706300735473633,
      "learning_rate": 3.342158159297887e-05,
      "loss": 1.6661,
      "step": 3395
    },
    {
      "epoch": 1.173425366695427,
      "grad_norm": 2.926128625869751,
      "learning_rate": 3.337902416920632e-05,
      "loss": 1.5468,
      "step": 3400
    },
    {
      "epoch": 1.173425366695427,
      "eval_loss": 1.8458772897720337,
      "eval_runtime": 302.4207,
      "eval_samples_per_second": 17.036,
      "eval_steps_per_second": 8.518,
      "step": 3400
    },
    {
      "epoch": 1.175150992234685,
      "grad_norm": 2.7061986923217773,
      "learning_rate": 3.333643937425009e-05,
      "loss": 1.7464,
      "step": 3405
    },
    {
      "epoch": 1.1768766177739431,
      "grad_norm": 2.9702160358428955,
      "learning_rate": 3.3293827347219015e-05,
      "loss": 1.6995,
      "step": 3410
    },
    {
      "epoch": 1.178602243313201,
      "grad_norm": 3.2435121536254883,
      "learning_rate": 3.325118822731086e-05,
      "loss": 1.6856,
      "step": 3415
    },
    {
      "epoch": 1.180327868852459,
      "grad_norm": 2.399482011795044,
      "learning_rate": 3.3208522153811926e-05,
      "loss": 1.7915,
      "step": 3420
    },
    {
      "epoch": 1.182053494391717,
      "grad_norm": 3.3274829387664795,
      "learning_rate": 3.316582926609656e-05,
      "loss": 1.6551,
      "step": 3425
    },
    {
      "epoch": 1.183779119930975,
      "grad_norm": 2.554583787918091,
      "learning_rate": 3.3123109703626684e-05,
      "loss": 1.5769,
      "step": 3430
    },
    {
      "epoch": 1.185504745470233,
      "grad_norm": 2.5066490173339844,
      "learning_rate": 3.3080363605951346e-05,
      "loss": 1.6655,
      "step": 3435
    },
    {
      "epoch": 1.187230371009491,
      "grad_norm": 2.5252349376678467,
      "learning_rate": 3.3037591112706325e-05,
      "loss": 1.6876,
      "step": 3440
    },
    {
      "epoch": 1.1889559965487488,
      "grad_norm": 2.9687230587005615,
      "learning_rate": 3.299479236361357e-05,
      "loss": 1.5485,
      "step": 3445
    },
    {
      "epoch": 1.190681622088007,
      "grad_norm": 2.5800094604492188,
      "learning_rate": 3.295196749848082e-05,
      "loss": 1.5468,
      "step": 3450
    },
    {
      "epoch": 1.192407247627265,
      "grad_norm": 2.135235071182251,
      "learning_rate": 3.290911665720113e-05,
      "loss": 1.481,
      "step": 3455
    },
    {
      "epoch": 1.1941328731665228,
      "grad_norm": 2.401965856552124,
      "learning_rate": 3.2866239979752406e-05,
      "loss": 1.5224,
      "step": 3460
    },
    {
      "epoch": 1.1958584987057808,
      "grad_norm": 2.3918447494506836,
      "learning_rate": 3.2823337606196946e-05,
      "loss": 1.4852,
      "step": 3465
    },
    {
      "epoch": 1.197584124245039,
      "grad_norm": 2.913106679916382,
      "learning_rate": 3.2780409676681e-05,
      "loss": 1.7479,
      "step": 3470
    },
    {
      "epoch": 1.1993097497842968,
      "grad_norm": 2.783790349960327,
      "learning_rate": 3.273745633143428e-05,
      "loss": 1.5145,
      "step": 3475
    },
    {
      "epoch": 1.2010353753235548,
      "grad_norm": 2.7883381843566895,
      "learning_rate": 3.269447771076953e-05,
      "loss": 1.3681,
      "step": 3480
    },
    {
      "epoch": 1.2027610008628127,
      "grad_norm": 2.241171360015869,
      "learning_rate": 3.265147395508209e-05,
      "loss": 1.4684,
      "step": 3485
    },
    {
      "epoch": 1.2044866264020708,
      "grad_norm": 2.3735718727111816,
      "learning_rate": 3.2608445204849356e-05,
      "loss": 1.6797,
      "step": 3490
    },
    {
      "epoch": 1.2062122519413288,
      "grad_norm": 2.6991584300994873,
      "learning_rate": 3.25653916006304e-05,
      "loss": 1.7841,
      "step": 3495
    },
    {
      "epoch": 1.2079378774805867,
      "grad_norm": 2.7147879600524902,
      "learning_rate": 3.2522313283065476e-05,
      "loss": 1.587,
      "step": 3500
    },
    {
      "epoch": 1.2079378774805867,
      "eval_loss": 1.8354874849319458,
      "eval_runtime": 302.6364,
      "eval_samples_per_second": 17.024,
      "eval_steps_per_second": 8.512,
      "step": 3500
    },
    {
      "epoch": 1.2096635030198446,
      "grad_norm": 2.3059136867523193,
      "learning_rate": 3.247921039287558e-05,
      "loss": 1.638,
      "step": 3505
    },
    {
      "epoch": 1.2113891285591027,
      "grad_norm": 2.6951961517333984,
      "learning_rate": 3.243608307086197e-05,
      "loss": 1.7114,
      "step": 3510
    },
    {
      "epoch": 1.2131147540983607,
      "grad_norm": 3.1136536598205566,
      "learning_rate": 3.2392931457905684e-05,
      "loss": 1.4603,
      "step": 3515
    },
    {
      "epoch": 1.2148403796376186,
      "grad_norm": 2.4714975357055664,
      "learning_rate": 3.234975569496717e-05,
      "loss": 1.5752,
      "step": 3520
    },
    {
      "epoch": 1.2165660051768765,
      "grad_norm": 2.559502363204956,
      "learning_rate": 3.230655592308572e-05,
      "loss": 1.6436,
      "step": 3525
    },
    {
      "epoch": 1.2182916307161347,
      "grad_norm": 3.0360350608825684,
      "learning_rate": 3.226333228337906e-05,
      "loss": 1.7136,
      "step": 3530
    },
    {
      "epoch": 1.2200172562553926,
      "grad_norm": 3.2622392177581787,
      "learning_rate": 3.222008491704289e-05,
      "loss": 1.5953,
      "step": 3535
    },
    {
      "epoch": 1.2217428817946505,
      "grad_norm": 2.4611873626708984,
      "learning_rate": 3.217681396535043e-05,
      "loss": 1.5091,
      "step": 3540
    },
    {
      "epoch": 1.2234685073339087,
      "grad_norm": 2.080913543701172,
      "learning_rate": 3.2133519569651916e-05,
      "loss": 1.4829,
      "step": 3545
    },
    {
      "epoch": 1.2251941328731666,
      "grad_norm": 2.293351888656616,
      "learning_rate": 3.209020187137419e-05,
      "loss": 1.5893,
      "step": 3550
    },
    {
      "epoch": 1.2269197584124245,
      "grad_norm": 2.907471179962158,
      "learning_rate": 3.2046861012020204e-05,
      "loss": 1.636,
      "step": 3555
    },
    {
      "epoch": 1.2286453839516824,
      "grad_norm": 2.6033003330230713,
      "learning_rate": 3.2003497133168566e-05,
      "loss": 1.5176,
      "step": 3560
    },
    {
      "epoch": 1.2303710094909404,
      "grad_norm": 2.774204969406128,
      "learning_rate": 3.1960110376473104e-05,
      "loss": 1.5923,
      "step": 3565
    },
    {
      "epoch": 1.2320966350301985,
      "grad_norm": 1.9356929063796997,
      "learning_rate": 3.1916700883662344e-05,
      "loss": 1.7818,
      "step": 3570
    },
    {
      "epoch": 1.2338222605694564,
      "grad_norm": 2.5775365829467773,
      "learning_rate": 3.187326879653909e-05,
      "loss": 1.6905,
      "step": 3575
    },
    {
      "epoch": 1.2355478861087144,
      "grad_norm": 2.275009870529175,
      "learning_rate": 3.182981425697999e-05,
      "loss": 1.6911,
      "step": 3580
    },
    {
      "epoch": 1.2372735116479725,
      "grad_norm": 2.5771286487579346,
      "learning_rate": 3.178633740693498e-05,
      "loss": 1.7496,
      "step": 3585
    },
    {
      "epoch": 1.2389991371872304,
      "grad_norm": 3.103921413421631,
      "learning_rate": 3.174283838842693e-05,
      "loss": 1.7336,
      "step": 3590
    },
    {
      "epoch": 1.2407247627264884,
      "grad_norm": 2.661102533340454,
      "learning_rate": 3.169931734355109e-05,
      "loss": 1.465,
      "step": 3595
    },
    {
      "epoch": 1.2424503882657463,
      "grad_norm": 2.526475667953491,
      "learning_rate": 3.165577441447467e-05,
      "loss": 1.6104,
      "step": 3600
    },
    {
      "epoch": 1.2424503882657463,
      "eval_loss": 1.8404145240783691,
      "eval_runtime": 302.5372,
      "eval_samples_per_second": 17.029,
      "eval_steps_per_second": 8.515,
      "step": 3600
    },
    {
      "epoch": 1.2441760138050042,
      "grad_norm": 2.8817496299743652,
      "learning_rate": 3.161220974343638e-05,
      "loss": 1.6578,
      "step": 3605
    },
    {
      "epoch": 1.2459016393442623,
      "grad_norm": 3.231663942337036,
      "learning_rate": 3.156862347274593e-05,
      "loss": 1.6152,
      "step": 3610
    },
    {
      "epoch": 1.2476272648835203,
      "grad_norm": 2.6306912899017334,
      "learning_rate": 3.152501574478361e-05,
      "loss": 1.552,
      "step": 3615
    },
    {
      "epoch": 1.2493528904227782,
      "grad_norm": 2.7223029136657715,
      "learning_rate": 3.1481386701999793e-05,
      "loss": 1.549,
      "step": 3620
    },
    {
      "epoch": 1.2510785159620363,
      "grad_norm": 2.635183572769165,
      "learning_rate": 3.143773648691449e-05,
      "loss": 1.6619,
      "step": 3625
    },
    {
      "epoch": 1.2528041415012943,
      "grad_norm": 2.489539384841919,
      "learning_rate": 3.139406524211685e-05,
      "loss": 1.4934,
      "step": 3630
    },
    {
      "epoch": 1.2545297670405522,
      "grad_norm": 2.6930994987487793,
      "learning_rate": 3.135037311026473e-05,
      "loss": 1.5786,
      "step": 3635
    },
    {
      "epoch": 1.2562553925798101,
      "grad_norm": 2.1502325534820557,
      "learning_rate": 3.130666023408425e-05,
      "loss": 1.6549,
      "step": 3640
    },
    {
      "epoch": 1.257981018119068,
      "grad_norm": 2.125990152359009,
      "learning_rate": 3.1262926756369244e-05,
      "loss": 1.5668,
      "step": 3645
    },
    {
      "epoch": 1.2597066436583262,
      "grad_norm": 2.6824703216552734,
      "learning_rate": 3.121917281998086e-05,
      "loss": 1.6708,
      "step": 3650
    },
    {
      "epoch": 1.261432269197584,
      "grad_norm": 2.4917285442352295,
      "learning_rate": 3.1175398567847094e-05,
      "loss": 1.5536,
      "step": 3655
    },
    {
      "epoch": 1.263157894736842,
      "grad_norm": 2.713000535964966,
      "learning_rate": 3.1131604142962285e-05,
      "loss": 1.7482,
      "step": 3660
    },
    {
      "epoch": 1.2648835202761002,
      "grad_norm": 2.463974714279175,
      "learning_rate": 3.1087789688386686e-05,
      "loss": 1.4481,
      "step": 3665
    },
    {
      "epoch": 1.266609145815358,
      "grad_norm": 2.7343642711639404,
      "learning_rate": 3.104395534724597e-05,
      "loss": 1.439,
      "step": 3670
    },
    {
      "epoch": 1.268334771354616,
      "grad_norm": 2.3167724609375,
      "learning_rate": 3.1000101262730766e-05,
      "loss": 1.7446,
      "step": 3675
    },
    {
      "epoch": 1.270060396893874,
      "grad_norm": 2.4304184913635254,
      "learning_rate": 3.0956227578096206e-05,
      "loss": 1.7079,
      "step": 3680
    },
    {
      "epoch": 1.2717860224331319,
      "grad_norm": 2.6014251708984375,
      "learning_rate": 3.091233443666146e-05,
      "loss": 1.6566,
      "step": 3685
    },
    {
      "epoch": 1.27351164797239,
      "grad_norm": 2.523888349533081,
      "learning_rate": 3.086842198180923e-05,
      "loss": 1.5546,
      "step": 3690
    },
    {
      "epoch": 1.275237273511648,
      "grad_norm": 2.335949659347534,
      "learning_rate": 3.082449035698532e-05,
      "loss": 1.6433,
      "step": 3695
    },
    {
      "epoch": 1.2769628990509059,
      "grad_norm": 2.7264323234558105,
      "learning_rate": 3.078053970569816e-05,
      "loss": 1.7821,
      "step": 3700
    },
    {
      "epoch": 1.2769628990509059,
      "eval_loss": 1.8303983211517334,
      "eval_runtime": 302.4542,
      "eval_samples_per_second": 17.034,
      "eval_steps_per_second": 8.517,
      "step": 3700
    },
    {
      "epoch": 1.278688524590164,
      "grad_norm": 2.534867286682129,
      "learning_rate": 3.073657017151834e-05,
      "loss": 1.5101,
      "step": 3705
    },
    {
      "epoch": 1.280414150129422,
      "grad_norm": 2.5650687217712402,
      "learning_rate": 3.06925818980781e-05,
      "loss": 1.6443,
      "step": 3710
    },
    {
      "epoch": 1.2821397756686799,
      "grad_norm": 2.5614218711853027,
      "learning_rate": 3.064857502907094e-05,
      "loss": 1.749,
      "step": 3715
    },
    {
      "epoch": 1.2838654012079378,
      "grad_norm": 2.144200563430786,
      "learning_rate": 3.060454970825106e-05,
      "loss": 1.5354,
      "step": 3720
    },
    {
      "epoch": 1.2855910267471957,
      "grad_norm": 3.0578482151031494,
      "learning_rate": 3.056050607943297e-05,
      "loss": 1.6307,
      "step": 3725
    },
    {
      "epoch": 1.2873166522864539,
      "grad_norm": 2.5677239894866943,
      "learning_rate": 3.051644428649097e-05,
      "loss": 1.5355,
      "step": 3730
    },
    {
      "epoch": 1.2890422778257118,
      "grad_norm": 2.7002861499786377,
      "learning_rate": 3.047236447335869e-05,
      "loss": 1.5563,
      "step": 3735
    },
    {
      "epoch": 1.2907679033649697,
      "grad_norm": 2.9166619777679443,
      "learning_rate": 3.0428266784028646e-05,
      "loss": 1.7514,
      "step": 3740
    },
    {
      "epoch": 1.2924935289042279,
      "grad_norm": 3.073655128479004,
      "learning_rate": 3.0384151362551727e-05,
      "loss": 1.7233,
      "step": 3745
    },
    {
      "epoch": 1.2942191544434858,
      "grad_norm": 2.667172431945801,
      "learning_rate": 3.0340018353036746e-05,
      "loss": 1.6305,
      "step": 3750
    },
    {
      "epoch": 1.2959447799827437,
      "grad_norm": 3.1633174419403076,
      "learning_rate": 3.0295867899650006e-05,
      "loss": 1.4687,
      "step": 3755
    },
    {
      "epoch": 1.2976704055220019,
      "grad_norm": 2.3711423873901367,
      "learning_rate": 3.0251700146614743e-05,
      "loss": 1.5322,
      "step": 3760
    },
    {
      "epoch": 1.2993960310612598,
      "grad_norm": 2.8708572387695312,
      "learning_rate": 3.0207515238210736e-05,
      "loss": 1.5623,
      "step": 3765
    },
    {
      "epoch": 1.3011216566005177,
      "grad_norm": 2.806950807571411,
      "learning_rate": 3.016331331877379e-05,
      "loss": 1.5408,
      "step": 3770
    },
    {
      "epoch": 1.3028472821397756,
      "grad_norm": 2.6912899017333984,
      "learning_rate": 3.011909453269528e-05,
      "loss": 1.7226,
      "step": 3775
    },
    {
      "epoch": 1.3045729076790336,
      "grad_norm": 2.284327507019043,
      "learning_rate": 3.0074859024421698e-05,
      "loss": 1.6527,
      "step": 3780
    },
    {
      "epoch": 1.3062985332182917,
      "grad_norm": 2.571472644805908,
      "learning_rate": 3.0030606938454138e-05,
      "loss": 1.356,
      "step": 3785
    },
    {
      "epoch": 1.3080241587575496,
      "grad_norm": 2.381218910217285,
      "learning_rate": 2.9986338419347854e-05,
      "loss": 1.5758,
      "step": 3790
    },
    {
      "epoch": 1.3097497842968076,
      "grad_norm": 2.690441131591797,
      "learning_rate": 2.9942053611711772e-05,
      "loss": 1.7612,
      "step": 3795
    },
    {
      "epoch": 1.3114754098360657,
      "grad_norm": 2.636366605758667,
      "learning_rate": 2.9897752660208056e-05,
      "loss": 1.487,
      "step": 3800
    },
    {
      "epoch": 1.3114754098360657,
      "eval_loss": 1.8235148191452026,
      "eval_runtime": 302.4032,
      "eval_samples_per_second": 17.037,
      "eval_steps_per_second": 8.518,
      "step": 3800
    },
    {
      "epoch": 1.3132010353753236,
      "grad_norm": 3.194591760635376,
      "learning_rate": 2.9853435709551575e-05,
      "loss": 1.6571,
      "step": 3805
    },
    {
      "epoch": 1.3149266609145815,
      "grad_norm": 2.3243467807769775,
      "learning_rate": 2.9809102904509467e-05,
      "loss": 1.7139,
      "step": 3810
    },
    {
      "epoch": 1.3166522864538395,
      "grad_norm": 2.477782726287842,
      "learning_rate": 2.9764754389900678e-05,
      "loss": 1.4831,
      "step": 3815
    },
    {
      "epoch": 1.3183779119930974,
      "grad_norm": 2.3411877155303955,
      "learning_rate": 2.972039031059546e-05,
      "loss": 1.5488,
      "step": 3820
    },
    {
      "epoch": 1.3201035375323555,
      "grad_norm": 2.6082239151000977,
      "learning_rate": 2.9676010811514897e-05,
      "loss": 1.6824,
      "step": 3825
    },
    {
      "epoch": 1.3218291630716135,
      "grad_norm": 3.0119781494140625,
      "learning_rate": 2.963161603763046e-05,
      "loss": 1.4995,
      "step": 3830
    },
    {
      "epoch": 1.3235547886108714,
      "grad_norm": 2.218233346939087,
      "learning_rate": 2.9587206133963512e-05,
      "loss": 1.5666,
      "step": 3835
    },
    {
      "epoch": 1.3252804141501295,
      "grad_norm": 2.866321086883545,
      "learning_rate": 2.954278124558485e-05,
      "loss": 1.4783,
      "step": 3840
    },
    {
      "epoch": 1.3270060396893875,
      "grad_norm": 2.9130678176879883,
      "learning_rate": 2.9498341517614205e-05,
      "loss": 1.6805,
      "step": 3845
    },
    {
      "epoch": 1.3287316652286454,
      "grad_norm": 2.3699517250061035,
      "learning_rate": 2.945388709521978e-05,
      "loss": 1.4921,
      "step": 3850
    },
    {
      "epoch": 1.3304572907679033,
      "grad_norm": 2.2457892894744873,
      "learning_rate": 2.9409418123617804e-05,
      "loss": 1.4782,
      "step": 3855
    },
    {
      "epoch": 1.3321829163071612,
      "grad_norm": 2.529890537261963,
      "learning_rate": 2.9364934748072005e-05,
      "loss": 1.6302,
      "step": 3860
    },
    {
      "epoch": 1.3339085418464194,
      "grad_norm": 2.6939265727996826,
      "learning_rate": 2.9320437113893178e-05,
      "loss": 1.7448,
      "step": 3865
    },
    {
      "epoch": 1.3356341673856773,
      "grad_norm": 2.9499738216400146,
      "learning_rate": 2.9275925366438688e-05,
      "loss": 1.7057,
      "step": 3870
    },
    {
      "epoch": 1.3373597929249352,
      "grad_norm": 2.91133713722229,
      "learning_rate": 2.9231399651112024e-05,
      "loss": 1.5604,
      "step": 3875
    },
    {
      "epoch": 1.3390854184641934,
      "grad_norm": 2.3561508655548096,
      "learning_rate": 2.9186860113362276e-05,
      "loss": 1.6855,
      "step": 3880
    },
    {
      "epoch": 1.3408110440034513,
      "grad_norm": 2.6308178901672363,
      "learning_rate": 2.9142306898683702e-05,
      "loss": 1.4608,
      "step": 3885
    },
    {
      "epoch": 1.3425366695427092,
      "grad_norm": 2.6931135654449463,
      "learning_rate": 2.9097740152615222e-05,
      "loss": 1.547,
      "step": 3890
    },
    {
      "epoch": 1.3442622950819672,
      "grad_norm": 2.73435378074646,
      "learning_rate": 2.9053160020739977e-05,
      "loss": 1.54,
      "step": 3895
    },
    {
      "epoch": 1.345987920621225,
      "grad_norm": 3.1977310180664062,
      "learning_rate": 2.9008566648684833e-05,
      "loss": 1.7095,
      "step": 3900
    },
    {
      "epoch": 1.345987920621225,
      "eval_loss": 1.823176622390747,
      "eval_runtime": 302.5025,
      "eval_samples_per_second": 17.031,
      "eval_steps_per_second": 8.516,
      "step": 3900
    },
    {
      "epoch": 1.3477135461604832,
      "grad_norm": 2.5481677055358887,
      "learning_rate": 2.8963960182119896e-05,
      "loss": 1.5174,
      "step": 3905
    },
    {
      "epoch": 1.3494391716997411,
      "grad_norm": 2.8104076385498047,
      "learning_rate": 2.891934076675805e-05,
      "loss": 1.5891,
      "step": 3910
    },
    {
      "epoch": 1.351164797238999,
      "grad_norm": 2.6837775707244873,
      "learning_rate": 2.8874708548354478e-05,
      "loss": 1.5278,
      "step": 3915
    },
    {
      "epoch": 1.3528904227782572,
      "grad_norm": 2.595928907394409,
      "learning_rate": 2.8830063672706197e-05,
      "loss": 1.6493,
      "step": 3920
    },
    {
      "epoch": 1.3546160483175151,
      "grad_norm": 3.0147321224212646,
      "learning_rate": 2.878540628565155e-05,
      "loss": 1.778,
      "step": 3925
    },
    {
      "epoch": 1.356341673856773,
      "grad_norm": 2.2102363109588623,
      "learning_rate": 2.8740736533069763e-05,
      "loss": 1.4341,
      "step": 3930
    },
    {
      "epoch": 1.358067299396031,
      "grad_norm": 3.753051280975342,
      "learning_rate": 2.8696054560880453e-05,
      "loss": 1.5343,
      "step": 3935
    },
    {
      "epoch": 1.359792924935289,
      "grad_norm": 2.6932852268218994,
      "learning_rate": 2.8651360515043163e-05,
      "loss": 1.5372,
      "step": 3940
    },
    {
      "epoch": 1.361518550474547,
      "grad_norm": 2.610720634460449,
      "learning_rate": 2.860665454155686e-05,
      "loss": 1.5789,
      "step": 3945
    },
    {
      "epoch": 1.363244176013805,
      "grad_norm": 2.455892562866211,
      "learning_rate": 2.8561936786459477e-05,
      "loss": 1.5288,
      "step": 3950
    },
    {
      "epoch": 1.364969801553063,
      "grad_norm": 2.732286214828491,
      "learning_rate": 2.8517207395827445e-05,
      "loss": 1.6924,
      "step": 3955
    },
    {
      "epoch": 1.366695427092321,
      "grad_norm": 2.870084047317505,
      "learning_rate": 2.8472466515775203e-05,
      "loss": 1.4513,
      "step": 3960
    },
    {
      "epoch": 1.368421052631579,
      "grad_norm": 2.531329393386841,
      "learning_rate": 2.842771429245471e-05,
      "loss": 1.628,
      "step": 3965
    },
    {
      "epoch": 1.370146678170837,
      "grad_norm": 2.5453529357910156,
      "learning_rate": 2.838295087205498e-05,
      "loss": 1.6345,
      "step": 3970
    },
    {
      "epoch": 1.3718723037100948,
      "grad_norm": 2.7833175659179688,
      "learning_rate": 2.833817640080162e-05,
      "loss": 1.6602,
      "step": 3975
    },
    {
      "epoch": 1.3735979292493528,
      "grad_norm": 3.158712863922119,
      "learning_rate": 2.8293391024956316e-05,
      "loss": 1.7069,
      "step": 3980
    },
    {
      "epoch": 1.375323554788611,
      "grad_norm": 2.9444077014923096,
      "learning_rate": 2.8248594890816388e-05,
      "loss": 1.7904,
      "step": 3985
    },
    {
      "epoch": 1.3770491803278688,
      "grad_norm": 2.037445306777954,
      "learning_rate": 2.8203788144714288e-05,
      "loss": 1.3779,
      "step": 3990
    },
    {
      "epoch": 1.3787748058671268,
      "grad_norm": 2.578639030456543,
      "learning_rate": 2.8158970933017153e-05,
      "loss": 1.4361,
      "step": 3995
    },
    {
      "epoch": 1.380500431406385,
      "grad_norm": 2.9270219802856445,
      "learning_rate": 2.8114143402126293e-05,
      "loss": 1.6268,
      "step": 4000
    },
    {
      "epoch": 1.380500431406385,
      "eval_loss": 1.8147071599960327,
      "eval_runtime": 302.4699,
      "eval_samples_per_second": 17.033,
      "eval_steps_per_second": 8.517,
      "step": 4000
    },
    {
      "epoch": 1.3822260569456428,
      "grad_norm": 3.002943754196167,
      "learning_rate": 2.8069305698476727e-05,
      "loss": 1.4895,
      "step": 4005
    },
    {
      "epoch": 1.3839516824849007,
      "grad_norm": 2.2459850311279297,
      "learning_rate": 2.8024457968536694e-05,
      "loss": 1.3747,
      "step": 4010
    },
    {
      "epoch": 1.385677308024159,
      "grad_norm": 2.820657968521118,
      "learning_rate": 2.7979600358807218e-05,
      "loss": 1.7088,
      "step": 4015
    },
    {
      "epoch": 1.3874029335634168,
      "grad_norm": 2.2811107635498047,
      "learning_rate": 2.7934733015821568e-05,
      "loss": 1.5349,
      "step": 4020
    },
    {
      "epoch": 1.3891285591026747,
      "grad_norm": 2.73948335647583,
      "learning_rate": 2.7889856086144818e-05,
      "loss": 1.6052,
      "step": 4025
    },
    {
      "epoch": 1.3908541846419327,
      "grad_norm": 2.2674362659454346,
      "learning_rate": 2.7844969716373355e-05,
      "loss": 1.5674,
      "step": 4030
    },
    {
      "epoch": 1.3925798101811906,
      "grad_norm": 2.4349288940429688,
      "learning_rate": 2.7800074053134408e-05,
      "loss": 1.5367,
      "step": 4035
    },
    {
      "epoch": 1.3943054357204487,
      "grad_norm": 2.8020427227020264,
      "learning_rate": 2.7755169243085554e-05,
      "loss": 1.6455,
      "step": 4040
    },
    {
      "epoch": 1.3960310612597067,
      "grad_norm": 2.8158957958221436,
      "learning_rate": 2.7710255432914272e-05,
      "loss": 1.4673,
      "step": 4045
    },
    {
      "epoch": 1.3977566867989646,
      "grad_norm": 2.7065348625183105,
      "learning_rate": 2.766533276933741e-05,
      "loss": 1.8353,
      "step": 4050
    },
    {
      "epoch": 1.3994823123382227,
      "grad_norm": 2.558619737625122,
      "learning_rate": 2.762040139910076e-05,
      "loss": 1.6284,
      "step": 4055
    },
    {
      "epoch": 1.4012079378774807,
      "grad_norm": 2.4910833835601807,
      "learning_rate": 2.757546146897855e-05,
      "loss": 1.419,
      "step": 4060
    },
    {
      "epoch": 1.4029335634167386,
      "grad_norm": 3.015206813812256,
      "learning_rate": 2.753051312577296e-05,
      "loss": 1.6019,
      "step": 4065
    },
    {
      "epoch": 1.4046591889559965,
      "grad_norm": 2.7263991832733154,
      "learning_rate": 2.748555651631367e-05,
      "loss": 1.5257,
      "step": 4070
    },
    {
      "epoch": 1.4063848144952544,
      "grad_norm": 3.1412241458892822,
      "learning_rate": 2.7440591787457344e-05,
      "loss": 1.7099,
      "step": 4075
    },
    {
      "epoch": 1.4081104400345126,
      "grad_norm": 2.9501137733459473,
      "learning_rate": 2.739561908608719e-05,
      "loss": 1.5619,
      "step": 4080
    },
    {
      "epoch": 1.4098360655737705,
      "grad_norm": 2.342068910598755,
      "learning_rate": 2.735063855911244e-05,
      "loss": 1.6232,
      "step": 4085
    },
    {
      "epoch": 1.4115616911130284,
      "grad_norm": 2.696683883666992,
      "learning_rate": 2.7305650353467894e-05,
      "loss": 1.4343,
      "step": 4090
    },
    {
      "epoch": 1.4132873166522866,
      "grad_norm": 2.5934274196624756,
      "learning_rate": 2.7260654616113445e-05,
      "loss": 1.6581,
      "step": 4095
    },
    {
      "epoch": 1.4150129421915445,
      "grad_norm": 3.0748157501220703,
      "learning_rate": 2.7215651494033583e-05,
      "loss": 1.5578,
      "step": 4100
    },
    {
      "epoch": 1.4150129421915445,
      "eval_loss": 1.8075565099716187,
      "eval_runtime": 302.6723,
      "eval_samples_per_second": 17.022,
      "eval_steps_per_second": 8.511,
      "step": 4100
    },
    {
      "epoch": 1.4167385677308024,
      "grad_norm": 2.3166348934173584,
      "learning_rate": 2.7170641134236917e-05,
      "loss": 1.4315,
      "step": 4105
    },
    {
      "epoch": 1.4184641932700603,
      "grad_norm": 2.1397597789764404,
      "learning_rate": 2.71256236837557e-05,
      "loss": 1.4681,
      "step": 4110
    },
    {
      "epoch": 1.4201898188093183,
      "grad_norm": 3.2872941493988037,
      "learning_rate": 2.708059928964536e-05,
      "loss": 1.6477,
      "step": 4115
    },
    {
      "epoch": 1.4219154443485764,
      "grad_norm": 2.6070759296417236,
      "learning_rate": 2.7035568098983994e-05,
      "loss": 1.4782,
      "step": 4120
    },
    {
      "epoch": 1.4236410698878343,
      "grad_norm": 3.1571333408355713,
      "learning_rate": 2.6990530258871894e-05,
      "loss": 1.5575,
      "step": 4125
    },
    {
      "epoch": 1.4253666954270923,
      "grad_norm": 2.4959492683410645,
      "learning_rate": 2.6945485916431102e-05,
      "loss": 1.6112,
      "step": 4130
    },
    {
      "epoch": 1.4270923209663504,
      "grad_norm": 2.499484062194824,
      "learning_rate": 2.6900435218804866e-05,
      "loss": 1.5086,
      "step": 4135
    },
    {
      "epoch": 1.4288179465056083,
      "grad_norm": 2.782891273498535,
      "learning_rate": 2.6855378313157225e-05,
      "loss": 1.4094,
      "step": 4140
    },
    {
      "epoch": 1.4305435720448663,
      "grad_norm": 3.4361066818237305,
      "learning_rate": 2.6810315346672477e-05,
      "loss": 1.844,
      "step": 4145
    },
    {
      "epoch": 1.4322691975841242,
      "grad_norm": 2.962247133255005,
      "learning_rate": 2.6765246466554717e-05,
      "loss": 1.6021,
      "step": 4150
    },
    {
      "epoch": 1.4339948231233821,
      "grad_norm": 2.2633209228515625,
      "learning_rate": 2.6720171820027375e-05,
      "loss": 1.5554,
      "step": 4155
    },
    {
      "epoch": 1.4357204486626403,
      "grad_norm": 2.3721823692321777,
      "learning_rate": 2.667509155433271e-05,
      "loss": 1.573,
      "step": 4160
    },
    {
      "epoch": 1.4374460742018982,
      "grad_norm": 3.1025707721710205,
      "learning_rate": 2.663000581673133e-05,
      "loss": 1.6161,
      "step": 4165
    },
    {
      "epoch": 1.439171699741156,
      "grad_norm": 3.0346386432647705,
      "learning_rate": 2.658491475450171e-05,
      "loss": 1.5982,
      "step": 4170
    },
    {
      "epoch": 1.4408973252804143,
      "grad_norm": 2.7079670429229736,
      "learning_rate": 2.653981851493975e-05,
      "loss": 1.5799,
      "step": 4175
    },
    {
      "epoch": 1.4426229508196722,
      "grad_norm": 2.4726979732513428,
      "learning_rate": 2.649471724535824e-05,
      "loss": 1.5046,
      "step": 4180
    },
    {
      "epoch": 1.44434857635893,
      "grad_norm": 3.1940248012542725,
      "learning_rate": 2.6449611093086395e-05,
      "loss": 1.479,
      "step": 4185
    },
    {
      "epoch": 1.446074201898188,
      "grad_norm": 2.730158567428589,
      "learning_rate": 2.6404500205469408e-05,
      "loss": 1.6851,
      "step": 4190
    },
    {
      "epoch": 1.447799827437446,
      "grad_norm": 2.6377391815185547,
      "learning_rate": 2.6359384729867897e-05,
      "loss": 1.6525,
      "step": 4195
    },
    {
      "epoch": 1.449525452976704,
      "grad_norm": 2.343747138977051,
      "learning_rate": 2.631426481365751e-05,
      "loss": 1.6169,
      "step": 4200
    },
    {
      "epoch": 1.449525452976704,
      "eval_loss": 1.8040447235107422,
      "eval_runtime": 302.5761,
      "eval_samples_per_second": 17.027,
      "eval_steps_per_second": 8.514,
      "step": 4200
    },
    {
      "epoch": 1.451251078515962,
      "grad_norm": 2.826345443725586,
      "learning_rate": 2.6269140604228383e-05,
      "loss": 1.6106,
      "step": 4205
    },
    {
      "epoch": 1.45297670405522,
      "grad_norm": 2.803689956665039,
      "learning_rate": 2.6224012248984665e-05,
      "loss": 1.6049,
      "step": 4210
    },
    {
      "epoch": 1.454702329594478,
      "grad_norm": 3.0548717975616455,
      "learning_rate": 2.6178879895344065e-05,
      "loss": 1.6913,
      "step": 4215
    },
    {
      "epoch": 1.456427955133736,
      "grad_norm": 3.18100905418396,
      "learning_rate": 2.6133743690737362e-05,
      "loss": 1.8001,
      "step": 4220
    },
    {
      "epoch": 1.458153580672994,
      "grad_norm": 3.0066354274749756,
      "learning_rate": 2.6088603782607873e-05,
      "loss": 1.5623,
      "step": 4225
    },
    {
      "epoch": 1.4598792062122519,
      "grad_norm": 2.9405834674835205,
      "learning_rate": 2.604346031841107e-05,
      "loss": 1.5785,
      "step": 4230
    },
    {
      "epoch": 1.4616048317515098,
      "grad_norm": 2.7806413173675537,
      "learning_rate": 2.599831344561399e-05,
      "loss": 1.6677,
      "step": 4235
    },
    {
      "epoch": 1.463330457290768,
      "grad_norm": 2.2985129356384277,
      "learning_rate": 2.595316331169484e-05,
      "loss": 1.6234,
      "step": 4240
    },
    {
      "epoch": 1.4650560828300259,
      "grad_norm": 2.271012306213379,
      "learning_rate": 2.590801006414245e-05,
      "loss": 1.5872,
      "step": 4245
    },
    {
      "epoch": 1.4667817083692838,
      "grad_norm": 2.934816837310791,
      "learning_rate": 2.5862853850455852e-05,
      "loss": 1.498,
      "step": 4250
    },
    {
      "epoch": 1.468507333908542,
      "grad_norm": 2.153743267059326,
      "learning_rate": 2.581769481814375e-05,
      "loss": 1.6297,
      "step": 4255
    },
    {
      "epoch": 1.4702329594477999,
      "grad_norm": 2.426229953765869,
      "learning_rate": 2.5772533114724058e-05,
      "loss": 1.6213,
      "step": 4260
    },
    {
      "epoch": 1.4719585849870578,
      "grad_norm": 2.371919870376587,
      "learning_rate": 2.572736888772342e-05,
      "loss": 1.6611,
      "step": 4265
    },
    {
      "epoch": 1.4736842105263157,
      "grad_norm": 2.444472312927246,
      "learning_rate": 2.56822022846767e-05,
      "loss": 1.6242,
      "step": 4270
    },
    {
      "epoch": 1.4754098360655736,
      "grad_norm": 3.114858865737915,
      "learning_rate": 2.563703345312657e-05,
      "loss": 1.7035,
      "step": 4275
    },
    {
      "epoch": 1.4771354616048318,
      "grad_norm": 3.247196912765503,
      "learning_rate": 2.5591862540622937e-05,
      "loss": 1.6301,
      "step": 4280
    },
    {
      "epoch": 1.4788610871440897,
      "grad_norm": 2.32468318939209,
      "learning_rate": 2.5546689694722526e-05,
      "loss": 1.6739,
      "step": 4285
    },
    {
      "epoch": 1.4805867126833476,
      "grad_norm": 2.4454987049102783,
      "learning_rate": 2.550151506298838e-05,
      "loss": 1.5641,
      "step": 4290
    },
    {
      "epoch": 1.4823123382226058,
      "grad_norm": 2.5829808712005615,
      "learning_rate": 2.5456338792989365e-05,
      "loss": 1.656,
      "step": 4295
    },
    {
      "epoch": 1.4840379637618637,
      "grad_norm": 2.324348211288452,
      "learning_rate": 2.5411161032299712e-05,
      "loss": 1.6035,
      "step": 4300
    },
    {
      "epoch": 1.4840379637618637,
      "eval_loss": 1.796486258506775,
      "eval_runtime": 302.8349,
      "eval_samples_per_second": 17.013,
      "eval_steps_per_second": 8.506,
      "step": 4300
    },
    {
      "epoch": 1.4857635893011216,
      "grad_norm": 4.47995138168335,
      "learning_rate": 2.5365981928498505e-05,
      "loss": 1.5835,
      "step": 4305
    },
    {
      "epoch": 1.4874892148403798,
      "grad_norm": 2.1462175846099854,
      "learning_rate": 2.532080162916923e-05,
      "loss": 1.3598,
      "step": 4310
    },
    {
      "epoch": 1.4892148403796377,
      "grad_norm": 2.3172638416290283,
      "learning_rate": 2.527562028189927e-05,
      "loss": 1.4669,
      "step": 4315
    },
    {
      "epoch": 1.4909404659188956,
      "grad_norm": 2.4973316192626953,
      "learning_rate": 2.5230438034279436e-05,
      "loss": 1.6554,
      "step": 4320
    },
    {
      "epoch": 1.4926660914581535,
      "grad_norm": 3.1877572536468506,
      "learning_rate": 2.518525503390347e-05,
      "loss": 1.6183,
      "step": 4325
    },
    {
      "epoch": 1.4943917169974115,
      "grad_norm": 3.3676071166992188,
      "learning_rate": 2.51400714283676e-05,
      "loss": 1.6294,
      "step": 4330
    },
    {
      "epoch": 1.4961173425366696,
      "grad_norm": 2.6429295539855957,
      "learning_rate": 2.509488736527e-05,
      "loss": 1.6407,
      "step": 4335
    },
    {
      "epoch": 1.4978429680759275,
      "grad_norm": 2.0274276733398438,
      "learning_rate": 2.5049702992210343e-05,
      "loss": 1.3848,
      "step": 4340
    },
    {
      "epoch": 1.4995685936151855,
      "grad_norm": 2.9797279834747314,
      "learning_rate": 2.5004518456789334e-05,
      "loss": 1.6543,
      "step": 4345
    },
    {
      "epoch": 1.5012942191544436,
      "grad_norm": 2.7083895206451416,
      "learning_rate": 2.4959333906608183e-05,
      "loss": 1.5556,
      "step": 4350
    },
    {
      "epoch": 1.5030198446937013,
      "grad_norm": 2.690778970718384,
      "learning_rate": 2.491414948926819e-05,
      "loss": 1.6478,
      "step": 4355
    },
    {
      "epoch": 1.5047454702329595,
      "grad_norm": 3.257462739944458,
      "learning_rate": 2.4868965352370154e-05,
      "loss": 1.7343,
      "step": 4360
    },
    {
      "epoch": 1.5064710957722174,
      "grad_norm": 2.396777868270874,
      "learning_rate": 2.4823781643514026e-05,
      "loss": 1.55,
      "step": 4365
    },
    {
      "epoch": 1.5081967213114753,
      "grad_norm": 3.1725728511810303,
      "learning_rate": 2.4778598510298314e-05,
      "loss": 1.5387,
      "step": 4370
    },
    {
      "epoch": 1.5099223468507335,
      "grad_norm": 3.166405439376831,
      "learning_rate": 2.4733416100319675e-05,
      "loss": 1.6335,
      "step": 4375
    },
    {
      "epoch": 1.5116479723899914,
      "grad_norm": 2.854830265045166,
      "learning_rate": 2.4688234561172373e-05,
      "loss": 1.619,
      "step": 4380
    },
    {
      "epoch": 1.5133735979292493,
      "grad_norm": 2.5748960971832275,
      "learning_rate": 2.4643054040447855e-05,
      "loss": 1.8241,
      "step": 4385
    },
    {
      "epoch": 1.5150992234685075,
      "grad_norm": 2.619704484939575,
      "learning_rate": 2.4597874685734224e-05,
      "loss": 1.4704,
      "step": 4390
    },
    {
      "epoch": 1.5168248490077652,
      "grad_norm": 2.8050734996795654,
      "learning_rate": 2.455269664461579e-05,
      "loss": 1.7106,
      "step": 4395
    },
    {
      "epoch": 1.5185504745470233,
      "grad_norm": 3.0166847705841064,
      "learning_rate": 2.450752006467254e-05,
      "loss": 1.5906,
      "step": 4400
    },
    {
      "epoch": 1.5185504745470233,
      "eval_loss": 1.7916959524154663,
      "eval_runtime": 302.6205,
      "eval_samples_per_second": 17.025,
      "eval_steps_per_second": 8.512,
      "step": 4400
    },
    {
      "epoch": 1.5202761000862812,
      "grad_norm": 3.7011213302612305,
      "learning_rate": 2.4462345093479733e-05,
      "loss": 1.6952,
      "step": 4405
    },
    {
      "epoch": 1.5220017256255391,
      "grad_norm": 3.559443235397339,
      "learning_rate": 2.441717187860734e-05,
      "loss": 1.613,
      "step": 4410
    },
    {
      "epoch": 1.5237273511647973,
      "grad_norm": 2.5309834480285645,
      "learning_rate": 2.43720005676196e-05,
      "loss": 1.5232,
      "step": 4415
    },
    {
      "epoch": 1.5254529767040552,
      "grad_norm": 3.1854634284973145,
      "learning_rate": 2.4326831308074554e-05,
      "loss": 1.6271,
      "step": 4420
    },
    {
      "epoch": 1.5271786022433131,
      "grad_norm": 2.9208462238311768,
      "learning_rate": 2.42816642475235e-05,
      "loss": 1.6823,
      "step": 4425
    },
    {
      "epoch": 1.5289042277825713,
      "grad_norm": 2.6076149940490723,
      "learning_rate": 2.42364995335106e-05,
      "loss": 1.6422,
      "step": 4430
    },
    {
      "epoch": 1.5306298533218292,
      "grad_norm": 3.3202757835388184,
      "learning_rate": 2.4191337313572313e-05,
      "loss": 1.7892,
      "step": 4435
    },
    {
      "epoch": 1.5323554788610871,
      "grad_norm": 2.6988534927368164,
      "learning_rate": 2.4146177735236978e-05,
      "loss": 1.6334,
      "step": 4440
    },
    {
      "epoch": 1.5340811044003453,
      "grad_norm": 2.9294941425323486,
      "learning_rate": 2.4101020946024278e-05,
      "loss": 1.5241,
      "step": 4445
    },
    {
      "epoch": 1.535806729939603,
      "grad_norm": 3.574214458465576,
      "learning_rate": 2.405586709344482e-05,
      "loss": 1.8001,
      "step": 4450
    },
    {
      "epoch": 1.5375323554788611,
      "grad_norm": 2.3569178581237793,
      "learning_rate": 2.4010716324999573e-05,
      "loss": 1.639,
      "step": 4455
    },
    {
      "epoch": 1.539257981018119,
      "grad_norm": 2.629648208618164,
      "learning_rate": 2.3965568788179474e-05,
      "loss": 1.4352,
      "step": 4460
    },
    {
      "epoch": 1.540983606557377,
      "grad_norm": 2.4655776023864746,
      "learning_rate": 2.3920424630464883e-05,
      "loss": 1.7281,
      "step": 4465
    },
    {
      "epoch": 1.5427092320966351,
      "grad_norm": 2.6323330402374268,
      "learning_rate": 2.3875283999325123e-05,
      "loss": 1.5333,
      "step": 4470
    },
    {
      "epoch": 1.544434857635893,
      "grad_norm": 3.201667070388794,
      "learning_rate": 2.3830147042218002e-05,
      "loss": 1.5145,
      "step": 4475
    },
    {
      "epoch": 1.546160483175151,
      "grad_norm": 3.2218780517578125,
      "learning_rate": 2.378501390658931e-05,
      "loss": 1.5271,
      "step": 4480
    },
    {
      "epoch": 1.5478861087144091,
      "grad_norm": 3.121234655380249,
      "learning_rate": 2.373988473987238e-05,
      "loss": 1.5743,
      "step": 4485
    },
    {
      "epoch": 1.5496117342536668,
      "grad_norm": 3.1088688373565674,
      "learning_rate": 2.3694759689487553e-05,
      "loss": 1.7567,
      "step": 4490
    },
    {
      "epoch": 1.551337359792925,
      "grad_norm": 2.9364073276519775,
      "learning_rate": 2.3649638902841758e-05,
      "loss": 1.6999,
      "step": 4495
    },
    {
      "epoch": 1.553062985332183,
      "grad_norm": 3.4985337257385254,
      "learning_rate": 2.3604522527327948e-05,
      "loss": 1.6043,
      "step": 4500
    },
    {
      "epoch": 1.553062985332183,
      "eval_loss": 1.786272644996643,
      "eval_runtime": 302.8395,
      "eval_samples_per_second": 17.012,
      "eval_steps_per_second": 8.506,
      "step": 4500
    },
    {
      "epoch": 1.5547886108714408,
      "grad_norm": 2.835439920425415,
      "learning_rate": 2.3559410710324717e-05,
      "loss": 1.7165,
      "step": 4505
    },
    {
      "epoch": 1.556514236410699,
      "grad_norm": 2.7241580486297607,
      "learning_rate": 2.3514303599195726e-05,
      "loss": 1.4922,
      "step": 4510
    },
    {
      "epoch": 1.558239861949957,
      "grad_norm": 2.9211654663085938,
      "learning_rate": 2.34692013412893e-05,
      "loss": 1.5641,
      "step": 4515
    },
    {
      "epoch": 1.5599654874892148,
      "grad_norm": 2.6883702278137207,
      "learning_rate": 2.342410408393787e-05,
      "loss": 1.5996,
      "step": 4520
    },
    {
      "epoch": 1.561691113028473,
      "grad_norm": 2.349846601486206,
      "learning_rate": 2.3379011974457566e-05,
      "loss": 1.5926,
      "step": 4525
    },
    {
      "epoch": 1.5634167385677307,
      "grad_norm": 2.876394271850586,
      "learning_rate": 2.3333925160147695e-05,
      "loss": 1.6674,
      "step": 4530
    },
    {
      "epoch": 1.5651423641069888,
      "grad_norm": 2.3586480617523193,
      "learning_rate": 2.328884378829025e-05,
      "loss": 1.5451,
      "step": 4535
    },
    {
      "epoch": 1.5668679896462467,
      "grad_norm": 2.826767683029175,
      "learning_rate": 2.324376800614947e-05,
      "loss": 1.5137,
      "step": 4540
    },
    {
      "epoch": 1.5685936151855047,
      "grad_norm": 2.4419543743133545,
      "learning_rate": 2.3198697960971314e-05,
      "loss": 1.5101,
      "step": 4545
    },
    {
      "epoch": 1.5703192407247628,
      "grad_norm": 2.8422138690948486,
      "learning_rate": 2.3153633799983016e-05,
      "loss": 1.7445,
      "step": 4550
    },
    {
      "epoch": 1.5720448662640207,
      "grad_norm": 3.189359426498413,
      "learning_rate": 2.3108575670392564e-05,
      "loss": 1.4767,
      "step": 4555
    },
    {
      "epoch": 1.5737704918032787,
      "grad_norm": 2.6708149909973145,
      "learning_rate": 2.306352371938827e-05,
      "loss": 1.5893,
      "step": 4560
    },
    {
      "epoch": 1.5754961173425368,
      "grad_norm": 2.5370004177093506,
      "learning_rate": 2.3018478094138252e-05,
      "loss": 1.6746,
      "step": 4565
    },
    {
      "epoch": 1.5772217428817945,
      "grad_norm": 2.7858946323394775,
      "learning_rate": 2.2973438941789977e-05,
      "loss": 1.535,
      "step": 4570
    },
    {
      "epoch": 1.5789473684210527,
      "grad_norm": 2.9644360542297363,
      "learning_rate": 2.2928406409469732e-05,
      "loss": 1.7309,
      "step": 4575
    },
    {
      "epoch": 1.5806729939603106,
      "grad_norm": 2.5612568855285645,
      "learning_rate": 2.2883380644282215e-05,
      "loss": 1.4671,
      "step": 4580
    },
    {
      "epoch": 1.5823986194995685,
      "grad_norm": 2.656277894973755,
      "learning_rate": 2.2838361793310013e-05,
      "loss": 1.5991,
      "step": 4585
    },
    {
      "epoch": 1.5841242450388267,
      "grad_norm": 2.557091474533081,
      "learning_rate": 2.2793350003613105e-05,
      "loss": 1.4322,
      "step": 4590
    },
    {
      "epoch": 1.5858498705780846,
      "grad_norm": 2.949769973754883,
      "learning_rate": 2.274834542222843e-05,
      "loss": 1.5494,
      "step": 4595
    },
    {
      "epoch": 1.5875754961173425,
      "grad_norm": 2.9435057640075684,
      "learning_rate": 2.270334819616936e-05,
      "loss": 1.5725,
      "step": 4600
    },
    {
      "epoch": 1.5875754961173425,
      "eval_loss": 1.7855629920959473,
      "eval_runtime": 302.6738,
      "eval_samples_per_second": 17.022,
      "eval_steps_per_second": 8.511,
      "step": 4600
    },
    {
      "epoch": 1.5893011216566006,
      "grad_norm": 2.878763437271118,
      "learning_rate": 2.2658358472425252e-05,
      "loss": 1.6844,
      "step": 4605
    },
    {
      "epoch": 1.5910267471958583,
      "grad_norm": 2.49957013130188,
      "learning_rate": 2.2613376397960944e-05,
      "loss": 1.5499,
      "step": 4610
    },
    {
      "epoch": 1.5927523727351165,
      "grad_norm": 2.600358486175537,
      "learning_rate": 2.256840211971631e-05,
      "loss": 1.319,
      "step": 4615
    },
    {
      "epoch": 1.5944779982743744,
      "grad_norm": 3.2401208877563477,
      "learning_rate": 2.2523435784605722e-05,
      "loss": 1.6526,
      "step": 4620
    },
    {
      "epoch": 1.5962036238136323,
      "grad_norm": 3.0242552757263184,
      "learning_rate": 2.2478477539517638e-05,
      "loss": 1.6511,
      "step": 4625
    },
    {
      "epoch": 1.5979292493528905,
      "grad_norm": 2.7568740844726562,
      "learning_rate": 2.2433527531314062e-05,
      "loss": 1.4597,
      "step": 4630
    },
    {
      "epoch": 1.5996548748921484,
      "grad_norm": 2.346224784851074,
      "learning_rate": 2.238858590683012e-05,
      "loss": 1.4277,
      "step": 4635
    },
    {
      "epoch": 1.6013805004314063,
      "grad_norm": 2.495762825012207,
      "learning_rate": 2.2343652812873514e-05,
      "loss": 1.6106,
      "step": 4640
    },
    {
      "epoch": 1.6031061259706645,
      "grad_norm": 3.0647497177124023,
      "learning_rate": 2.2298728396224107e-05,
      "loss": 1.6317,
      "step": 4645
    },
    {
      "epoch": 1.6048317515099222,
      "grad_norm": 2.8127071857452393,
      "learning_rate": 2.2253812803633432e-05,
      "loss": 1.7425,
      "step": 4650
    },
    {
      "epoch": 1.6065573770491803,
      "grad_norm": 2.9239673614501953,
      "learning_rate": 2.2208906181824146e-05,
      "loss": 1.6648,
      "step": 4655
    },
    {
      "epoch": 1.6082830025884383,
      "grad_norm": 3.0016210079193115,
      "learning_rate": 2.2164008677489652e-05,
      "loss": 1.5851,
      "step": 4660
    },
    {
      "epoch": 1.6100086281276962,
      "grad_norm": 2.7299728393554688,
      "learning_rate": 2.211912043729353e-05,
      "loss": 1.6085,
      "step": 4665
    },
    {
      "epoch": 1.6117342536669543,
      "grad_norm": 2.549569606781006,
      "learning_rate": 2.207424160786914e-05,
      "loss": 1.5516,
      "step": 4670
    },
    {
      "epoch": 1.6134598792062123,
      "grad_norm": 2.444023847579956,
      "learning_rate": 2.2029372335819053e-05,
      "loss": 1.656,
      "step": 4675
    },
    {
      "epoch": 1.6151855047454702,
      "grad_norm": 3.3334596157073975,
      "learning_rate": 2.1984512767714653e-05,
      "loss": 1.6932,
      "step": 4680
    },
    {
      "epoch": 1.6169111302847283,
      "grad_norm": 2.8519015312194824,
      "learning_rate": 2.1939663050095617e-05,
      "loss": 1.5364,
      "step": 4685
    },
    {
      "epoch": 1.618636755823986,
      "grad_norm": 2.7880842685699463,
      "learning_rate": 2.1894823329469445e-05,
      "loss": 1.6513,
      "step": 4690
    },
    {
      "epoch": 1.6203623813632442,
      "grad_norm": 3.2346854209899902,
      "learning_rate": 2.1849993752310964e-05,
      "loss": 1.7621,
      "step": 4695
    },
    {
      "epoch": 1.6220880069025023,
      "grad_norm": 2.75567364692688,
      "learning_rate": 2.180517446506189e-05,
      "loss": 1.5248,
      "step": 4700
    },
    {
      "epoch": 1.6220880069025023,
      "eval_loss": 1.7764285802841187,
      "eval_runtime": 302.4252,
      "eval_samples_per_second": 17.036,
      "eval_steps_per_second": 8.518,
      "step": 4700
    },
    {
      "epoch": 1.62381363244176,
      "grad_norm": 2.7099618911743164,
      "learning_rate": 2.176036561413032e-05,
      "loss": 1.4587,
      "step": 4705
    },
    {
      "epoch": 1.6255392579810182,
      "grad_norm": 2.6374387741088867,
      "learning_rate": 2.1715567345890236e-05,
      "loss": 1.562,
      "step": 4710
    },
    {
      "epoch": 1.627264883520276,
      "grad_norm": 2.9928672313690186,
      "learning_rate": 2.16707798066811e-05,
      "loss": 1.6419,
      "step": 4715
    },
    {
      "epoch": 1.628990509059534,
      "grad_norm": 3.1589770317077637,
      "learning_rate": 2.1626003142807262e-05,
      "loss": 1.515,
      "step": 4720
    },
    {
      "epoch": 1.6307161345987922,
      "grad_norm": 3.3148791790008545,
      "learning_rate": 2.15812375005376e-05,
      "loss": 1.5382,
      "step": 4725
    },
    {
      "epoch": 1.63244176013805,
      "grad_norm": 2.7362284660339355,
      "learning_rate": 2.1536483026104952e-05,
      "loss": 1.6605,
      "step": 4730
    },
    {
      "epoch": 1.634167385677308,
      "grad_norm": 2.97182035446167,
      "learning_rate": 2.1491739865705717e-05,
      "loss": 1.5807,
      "step": 4735
    },
    {
      "epoch": 1.6358930112165662,
      "grad_norm": 2.446636915206909,
      "learning_rate": 2.144700816549928e-05,
      "loss": 1.4693,
      "step": 4740
    },
    {
      "epoch": 1.6376186367558239,
      "grad_norm": 3.0101678371429443,
      "learning_rate": 2.1402288071607637e-05,
      "loss": 1.6447,
      "step": 4745
    },
    {
      "epoch": 1.639344262295082,
      "grad_norm": 2.8496921062469482,
      "learning_rate": 2.135757973011484e-05,
      "loss": 1.6602,
      "step": 4750
    },
    {
      "epoch": 1.64106988783434,
      "grad_norm": 2.4810409545898438,
      "learning_rate": 2.1312883287066575e-05,
      "loss": 1.6683,
      "step": 4755
    },
    {
      "epoch": 1.6427955133735979,
      "grad_norm": 3.7405261993408203,
      "learning_rate": 2.1268198888469635e-05,
      "loss": 1.7128,
      "step": 4760
    },
    {
      "epoch": 1.644521138912856,
      "grad_norm": 2.587904453277588,
      "learning_rate": 2.1223526680291485e-05,
      "loss": 1.4841,
      "step": 4765
    },
    {
      "epoch": 1.646246764452114,
      "grad_norm": 3.272174119949341,
      "learning_rate": 2.117886680845978e-05,
      "loss": 1.5661,
      "step": 4770
    },
    {
      "epoch": 1.6479723899913719,
      "grad_norm": 2.9792580604553223,
      "learning_rate": 2.1134219418861835e-05,
      "loss": 1.6832,
      "step": 4775
    },
    {
      "epoch": 1.64969801553063,
      "grad_norm": 2.101884365081787,
      "learning_rate": 2.1089584657344237e-05,
      "loss": 1.6255,
      "step": 4780
    },
    {
      "epoch": 1.6514236410698877,
      "grad_norm": 2.517096519470215,
      "learning_rate": 2.104496266971228e-05,
      "loss": 1.6331,
      "step": 4785
    },
    {
      "epoch": 1.6531492666091459,
      "grad_norm": 2.5811166763305664,
      "learning_rate": 2.1000353601729578e-05,
      "loss": 1.5364,
      "step": 4790
    },
    {
      "epoch": 1.6548748921484038,
      "grad_norm": 2.8266289234161377,
      "learning_rate": 2.0955757599117485e-05,
      "loss": 1.5208,
      "step": 4795
    },
    {
      "epoch": 1.6566005176876617,
      "grad_norm": 3.1955456733703613,
      "learning_rate": 2.0911174807554727e-05,
      "loss": 1.5631,
      "step": 4800
    },
    {
      "epoch": 1.6566005176876617,
      "eval_loss": 1.7730567455291748,
      "eval_runtime": 302.4827,
      "eval_samples_per_second": 17.032,
      "eval_steps_per_second": 8.516,
      "step": 4800
    },
    {
      "epoch": 1.6583261432269198,
      "grad_norm": 2.6685032844543457,
      "learning_rate": 2.0866605372676836e-05,
      "loss": 1.6779,
      "step": 4805
    },
    {
      "epoch": 1.6600517687661778,
      "grad_norm": 2.60956072807312,
      "learning_rate": 2.0822049440075753e-05,
      "loss": 1.6235,
      "step": 4810
    },
    {
      "epoch": 1.6617773943054357,
      "grad_norm": 2.9430012702941895,
      "learning_rate": 2.0777507155299257e-05,
      "loss": 1.54,
      "step": 4815
    },
    {
      "epoch": 1.6635030198446938,
      "grad_norm": 2.899322986602783,
      "learning_rate": 2.073297866385059e-05,
      "loss": 1.555,
      "step": 4820
    },
    {
      "epoch": 1.6652286453839515,
      "grad_norm": 2.0765624046325684,
      "learning_rate": 2.0688464111187926e-05,
      "loss": 1.4143,
      "step": 4825
    },
    {
      "epoch": 1.6669542709232097,
      "grad_norm": 2.7009851932525635,
      "learning_rate": 2.064396364272389e-05,
      "loss": 1.4494,
      "step": 4830
    },
    {
      "epoch": 1.6686798964624676,
      "grad_norm": 3.3387746810913086,
      "learning_rate": 2.0599477403825128e-05,
      "loss": 1.6786,
      "step": 4835
    },
    {
      "epoch": 1.6704055220017255,
      "grad_norm": 2.552109956741333,
      "learning_rate": 2.055500553981177e-05,
      "loss": 1.5767,
      "step": 4840
    },
    {
      "epoch": 1.6721311475409837,
      "grad_norm": 2.5673987865448,
      "learning_rate": 2.0510548195957014e-05,
      "loss": 1.6063,
      "step": 4845
    },
    {
      "epoch": 1.6738567730802416,
      "grad_norm": 2.741851329803467,
      "learning_rate": 2.0466105517486608e-05,
      "loss": 1.7939,
      "step": 4850
    },
    {
      "epoch": 1.6755823986194995,
      "grad_norm": 2.3568341732025146,
      "learning_rate": 2.042167764957842e-05,
      "loss": 1.6076,
      "step": 4855
    },
    {
      "epoch": 1.6773080241587577,
      "grad_norm": 2.703005313873291,
      "learning_rate": 2.0377264737361883e-05,
      "loss": 1.6637,
      "step": 4860
    },
    {
      "epoch": 1.6790336496980154,
      "grad_norm": 2.9574244022369385,
      "learning_rate": 2.033286692591766e-05,
      "loss": 1.4911,
      "step": 4865
    },
    {
      "epoch": 1.6807592752372735,
      "grad_norm": 2.74649977684021,
      "learning_rate": 2.0288484360277e-05,
      "loss": 1.4803,
      "step": 4870
    },
    {
      "epoch": 1.6824849007765315,
      "grad_norm": 2.8327202796936035,
      "learning_rate": 2.02441171854214e-05,
      "loss": 1.5552,
      "step": 4875
    },
    {
      "epoch": 1.6842105263157894,
      "grad_norm": 2.76155161857605,
      "learning_rate": 2.019976554628207e-05,
      "loss": 1.5839,
      "step": 4880
    },
    {
      "epoch": 1.6859361518550475,
      "grad_norm": 2.488708019256592,
      "learning_rate": 2.0155429587739465e-05,
      "loss": 1.5595,
      "step": 4885
    },
    {
      "epoch": 1.6876617773943055,
      "grad_norm": 3.269577741622925,
      "learning_rate": 2.0111109454622826e-05,
      "loss": 1.4406,
      "step": 4890
    },
    {
      "epoch": 1.6893874029335634,
      "grad_norm": 3.1695713996887207,
      "learning_rate": 2.006680529170968e-05,
      "loss": 1.4581,
      "step": 4895
    },
    {
      "epoch": 1.6911130284728215,
      "grad_norm": 2.7144291400909424,
      "learning_rate": 2.0022517243725413e-05,
      "loss": 1.3422,
      "step": 4900
    },
    {
      "epoch": 1.6911130284728215,
      "eval_loss": 1.7706739902496338,
      "eval_runtime": 302.7419,
      "eval_samples_per_second": 17.018,
      "eval_steps_per_second": 8.509,
      "step": 4900
    },
    {
      "epoch": 1.6928386540120792,
      "grad_norm": 3.1177256107330322,
      "learning_rate": 1.9978245455342738e-05,
      "loss": 1.6199,
      "step": 4905
    },
    {
      "epoch": 1.6945642795513374,
      "grad_norm": 3.0874950885772705,
      "learning_rate": 1.9933990071181293e-05,
      "loss": 1.4618,
      "step": 4910
    },
    {
      "epoch": 1.6962899050905953,
      "grad_norm": 2.987313747406006,
      "learning_rate": 1.9889751235807084e-05,
      "loss": 1.421,
      "step": 4915
    },
    {
      "epoch": 1.6980155306298532,
      "grad_norm": 2.7320711612701416,
      "learning_rate": 1.9845529093732094e-05,
      "loss": 1.5439,
      "step": 4920
    },
    {
      "epoch": 1.6997411561691114,
      "grad_norm": 2.7806451320648193,
      "learning_rate": 1.9801323789413754e-05,
      "loss": 1.5895,
      "step": 4925
    },
    {
      "epoch": 1.7014667817083693,
      "grad_norm": 2.8656885623931885,
      "learning_rate": 1.975713546725451e-05,
      "loss": 1.5493,
      "step": 4930
    },
    {
      "epoch": 1.7031924072476272,
      "grad_norm": 2.9163594245910645,
      "learning_rate": 1.9712964271601315e-05,
      "loss": 1.4848,
      "step": 4935
    },
    {
      "epoch": 1.7049180327868854,
      "grad_norm": 3.00356125831604,
      "learning_rate": 1.9668810346745186e-05,
      "loss": 1.6864,
      "step": 4940
    },
    {
      "epoch": 1.706643658326143,
      "grad_norm": 3.020921468734741,
      "learning_rate": 1.9624673836920727e-05,
      "loss": 1.5397,
      "step": 4945
    },
    {
      "epoch": 1.7083692838654012,
      "grad_norm": 2.59334397315979,
      "learning_rate": 1.9580554886305644e-05,
      "loss": 1.5402,
      "step": 4950
    },
    {
      "epoch": 1.7100949094046591,
      "grad_norm": 2.7662386894226074,
      "learning_rate": 1.9536453639020297e-05,
      "loss": 1.5474,
      "step": 4955
    },
    {
      "epoch": 1.711820534943917,
      "grad_norm": 3.4281649589538574,
      "learning_rate": 1.949237023912718e-05,
      "loss": 1.7291,
      "step": 4960
    },
    {
      "epoch": 1.7135461604831752,
      "grad_norm": 2.30112886428833,
      "learning_rate": 1.944830483063055e-05,
      "loss": 1.6061,
      "step": 4965
    },
    {
      "epoch": 1.7152717860224331,
      "grad_norm": 2.504115104675293,
      "learning_rate": 1.9404257557475824e-05,
      "loss": 1.5232,
      "step": 4970
    },
    {
      "epoch": 1.716997411561691,
      "grad_norm": 3.462043285369873,
      "learning_rate": 1.9360228563549226e-05,
      "loss": 1.4085,
      "step": 4975
    },
    {
      "epoch": 1.7187230371009492,
      "grad_norm": 2.992333173751831,
      "learning_rate": 1.9316217992677243e-05,
      "loss": 1.6112,
      "step": 4980
    },
    {
      "epoch": 1.7204486626402071,
      "grad_norm": 2.92852520942688,
      "learning_rate": 1.92722259886262e-05,
      "loss": 1.4714,
      "step": 4985
    },
    {
      "epoch": 1.722174288179465,
      "grad_norm": 4.034793376922607,
      "learning_rate": 1.922825269510174e-05,
      "loss": 1.5794,
      "step": 4990
    },
    {
      "epoch": 1.7238999137187232,
      "grad_norm": 2.5228071212768555,
      "learning_rate": 1.9184298255748422e-05,
      "loss": 1.3401,
      "step": 4995
    },
    {
      "epoch": 1.725625539257981,
      "grad_norm": 2.7445240020751953,
      "learning_rate": 1.9140362814149197e-05,
      "loss": 1.6494,
      "step": 5000
    },
    {
      "epoch": 1.725625539257981,
      "eval_loss": 1.7658652067184448,
      "eval_runtime": 302.7094,
      "eval_samples_per_second": 17.02,
      "eval_steps_per_second": 8.51,
      "step": 5000
    },
    {
      "epoch": 1.727351164797239,
      "grad_norm": 3.1380693912506104,
      "learning_rate": 1.909644651382495e-05,
      "loss": 1.5537,
      "step": 5005
    },
    {
      "epoch": 1.729076790336497,
      "grad_norm": 3.178893566131592,
      "learning_rate": 1.9052549498234068e-05,
      "loss": 1.5513,
      "step": 5010
    },
    {
      "epoch": 1.730802415875755,
      "grad_norm": 3.130248546600342,
      "learning_rate": 1.9008671910771896e-05,
      "loss": 1.5017,
      "step": 5015
    },
    {
      "epoch": 1.732528041415013,
      "grad_norm": 2.164966583251953,
      "learning_rate": 1.896481389477036e-05,
      "loss": 1.5809,
      "step": 5020
    },
    {
      "epoch": 1.734253666954271,
      "grad_norm": 2.9903342723846436,
      "learning_rate": 1.8920975593497418e-05,
      "loss": 1.508,
      "step": 5025
    },
    {
      "epoch": 1.735979292493529,
      "grad_norm": 2.516021966934204,
      "learning_rate": 1.887715715015666e-05,
      "loss": 1.5438,
      "step": 5030
    },
    {
      "epoch": 1.737704918032787,
      "grad_norm": 3.0033607482910156,
      "learning_rate": 1.8833358707886773e-05,
      "loss": 1.5712,
      "step": 5035
    },
    {
      "epoch": 1.7394305435720447,
      "grad_norm": 2.8732690811157227,
      "learning_rate": 1.878958040976114e-05,
      "loss": 1.5853,
      "step": 5040
    },
    {
      "epoch": 1.7411561691113029,
      "grad_norm": 2.995323896408081,
      "learning_rate": 1.874582239878731e-05,
      "loss": 1.5206,
      "step": 5045
    },
    {
      "epoch": 1.7428817946505608,
      "grad_norm": 2.3957955837249756,
      "learning_rate": 1.8702084817906592e-05,
      "loss": 1.5501,
      "step": 5050
    },
    {
      "epoch": 1.7446074201898187,
      "grad_norm": 2.539332389831543,
      "learning_rate": 1.865836780999353e-05,
      "loss": 1.5398,
      "step": 5055
    },
    {
      "epoch": 1.7463330457290769,
      "grad_norm": 3.1254403591156006,
      "learning_rate": 1.8614671517855477e-05,
      "loss": 1.6691,
      "step": 5060
    },
    {
      "epoch": 1.7480586712683348,
      "grad_norm": 2.611813545227051,
      "learning_rate": 1.857099608423213e-05,
      "loss": 1.5563,
      "step": 5065
    },
    {
      "epoch": 1.7497842968075927,
      "grad_norm": 3.6152660846710205,
      "learning_rate": 1.852734165179501e-05,
      "loss": 1.576,
      "step": 5070
    },
    {
      "epoch": 1.7515099223468509,
      "grad_norm": 2.8588223457336426,
      "learning_rate": 1.848370836314708e-05,
      "loss": 1.596,
      "step": 5075
    },
    {
      "epoch": 1.7532355478861086,
      "grad_norm": 2.5252840518951416,
      "learning_rate": 1.844009636082219e-05,
      "loss": 1.6654,
      "step": 5080
    },
    {
      "epoch": 1.7549611734253667,
      "grad_norm": 2.6486775875091553,
      "learning_rate": 1.8396505787284703e-05,
      "loss": 1.6658,
      "step": 5085
    },
    {
      "epoch": 1.7566867989646247,
      "grad_norm": 3.1325552463531494,
      "learning_rate": 1.8352936784928932e-05,
      "loss": 1.6899,
      "step": 5090
    },
    {
      "epoch": 1.7584124245038826,
      "grad_norm": 2.902170419692993,
      "learning_rate": 1.8309389496078765e-05,
      "loss": 1.5537,
      "step": 5095
    },
    {
      "epoch": 1.7601380500431407,
      "grad_norm": 2.983657121658325,
      "learning_rate": 1.826586406298713e-05,
      "loss": 1.7179,
      "step": 5100
    },
    {
      "epoch": 1.7601380500431407,
      "eval_loss": 1.7606011629104614,
      "eval_runtime": 302.5664,
      "eval_samples_per_second": 17.028,
      "eval_steps_per_second": 8.514,
      "step": 5100
    },
    {
      "epoch": 1.7618636755823986,
      "grad_norm": 3.0186216831207275,
      "learning_rate": 1.82223606278356e-05,
      "loss": 1.5693,
      "step": 5105
    },
    {
      "epoch": 1.7635893011216566,
      "grad_norm": 3.2466962337493896,
      "learning_rate": 1.8178879332733833e-05,
      "loss": 1.6393,
      "step": 5110
    },
    {
      "epoch": 1.7653149266609147,
      "grad_norm": 5.01107120513916,
      "learning_rate": 1.813542031971921e-05,
      "loss": 1.5266,
      "step": 5115
    },
    {
      "epoch": 1.7670405522001724,
      "grad_norm": 2.8034045696258545,
      "learning_rate": 1.8091983730756312e-05,
      "loss": 1.6732,
      "step": 5120
    },
    {
      "epoch": 1.7687661777394306,
      "grad_norm": 2.5894951820373535,
      "learning_rate": 1.8048569707736452e-05,
      "loss": 1.677,
      "step": 5125
    },
    {
      "epoch": 1.7704918032786885,
      "grad_norm": 2.8734240531921387,
      "learning_rate": 1.800517839247726e-05,
      "loss": 1.6006,
      "step": 5130
    },
    {
      "epoch": 1.7722174288179464,
      "grad_norm": 2.637089967727661,
      "learning_rate": 1.7961809926722155e-05,
      "loss": 1.7213,
      "step": 5135
    },
    {
      "epoch": 1.7739430543572046,
      "grad_norm": 2.6307551860809326,
      "learning_rate": 1.7918464452139942e-05,
      "loss": 1.4524,
      "step": 5140
    },
    {
      "epoch": 1.7756686798964625,
      "grad_norm": 2.5717267990112305,
      "learning_rate": 1.7875142110324296e-05,
      "loss": 1.7883,
      "step": 5145
    },
    {
      "epoch": 1.7773943054357204,
      "grad_norm": 3.125915050506592,
      "learning_rate": 1.783184304279336e-05,
      "loss": 1.6021,
      "step": 5150
    },
    {
      "epoch": 1.7791199309749786,
      "grad_norm": 2.816962957382202,
      "learning_rate": 1.7788567390989206e-05,
      "loss": 1.669,
      "step": 5155
    },
    {
      "epoch": 1.7808455565142363,
      "grad_norm": 3.340441942214966,
      "learning_rate": 1.7745315296277453e-05,
      "loss": 1.6747,
      "step": 5160
    },
    {
      "epoch": 1.7825711820534944,
      "grad_norm": 3.5722663402557373,
      "learning_rate": 1.770208689994675e-05,
      "loss": 1.68,
      "step": 5165
    },
    {
      "epoch": 1.7842968075927523,
      "grad_norm": 2.6036345958709717,
      "learning_rate": 1.7658882343208337e-05,
      "loss": 1.4474,
      "step": 5170
    },
    {
      "epoch": 1.7860224331320103,
      "grad_norm": 2.605799436569214,
      "learning_rate": 1.7615701767195563e-05,
      "loss": 1.5827,
      "step": 5175
    },
    {
      "epoch": 1.7877480586712684,
      "grad_norm": 3.853703737258911,
      "learning_rate": 1.7572545312963473e-05,
      "loss": 1.7162,
      "step": 5180
    },
    {
      "epoch": 1.7894736842105263,
      "grad_norm": 2.5015246868133545,
      "learning_rate": 1.75294131214883e-05,
      "loss": 1.7603,
      "step": 5185
    },
    {
      "epoch": 1.7911993097497843,
      "grad_norm": 3.1374688148498535,
      "learning_rate": 1.7486305333666992e-05,
      "loss": 1.7398,
      "step": 5190
    },
    {
      "epoch": 1.7929249352890424,
      "grad_norm": 3.020641565322876,
      "learning_rate": 1.7443222090316825e-05,
      "loss": 1.6979,
      "step": 5195
    },
    {
      "epoch": 1.7946505608283,
      "grad_norm": 2.8536908626556396,
      "learning_rate": 1.740016353217486e-05,
      "loss": 1.6765,
      "step": 5200
    },
    {
      "epoch": 1.7946505608283,
      "eval_loss": 1.7568014860153198,
      "eval_runtime": 302.559,
      "eval_samples_per_second": 17.028,
      "eval_steps_per_second": 8.514,
      "step": 5200
    },
    {
      "epoch": 1.7963761863675582,
      "grad_norm": 3.4018681049346924,
      "learning_rate": 1.735712979989756e-05,
      "loss": 1.7017,
      "step": 5205
    },
    {
      "epoch": 1.7981018119068162,
      "grad_norm": 3.222557306289673,
      "learning_rate": 1.7314121034060237e-05,
      "loss": 1.6367,
      "step": 5210
    },
    {
      "epoch": 1.799827437446074,
      "grad_norm": 2.686681032180786,
      "learning_rate": 1.72711373751567e-05,
      "loss": 1.494,
      "step": 5215
    },
    {
      "epoch": 1.8015530629853322,
      "grad_norm": 2.8158938884735107,
      "learning_rate": 1.7228178963598703e-05,
      "loss": 1.4853,
      "step": 5220
    },
    {
      "epoch": 1.8032786885245902,
      "grad_norm": 2.612778663635254,
      "learning_rate": 1.7185245939715566e-05,
      "loss": 1.5978,
      "step": 5225
    },
    {
      "epoch": 1.805004314063848,
      "grad_norm": 2.6768109798431396,
      "learning_rate": 1.7142338443753626e-05,
      "loss": 1.4449,
      "step": 5230
    },
    {
      "epoch": 1.8067299396031062,
      "grad_norm": 3.0095038414001465,
      "learning_rate": 1.7099456615875875e-05,
      "loss": 1.5634,
      "step": 5235
    },
    {
      "epoch": 1.808455565142364,
      "grad_norm": 2.699615478515625,
      "learning_rate": 1.705660059616144e-05,
      "loss": 1.7108,
      "step": 5240
    },
    {
      "epoch": 1.810181190681622,
      "grad_norm": 2.6740245819091797,
      "learning_rate": 1.7013770524605134e-05,
      "loss": 1.5851,
      "step": 5245
    },
    {
      "epoch": 1.8119068162208802,
      "grad_norm": 2.700972318649292,
      "learning_rate": 1.697096654111703e-05,
      "loss": 1.5847,
      "step": 5250
    },
    {
      "epoch": 1.813632441760138,
      "grad_norm": 3.9315731525421143,
      "learning_rate": 1.6928188785521945e-05,
      "loss": 1.697,
      "step": 5255
    },
    {
      "epoch": 1.815358067299396,
      "grad_norm": 2.1900980472564697,
      "learning_rate": 1.6885437397559055e-05,
      "loss": 1.4408,
      "step": 5260
    },
    {
      "epoch": 1.817083692838654,
      "grad_norm": 3.088047742843628,
      "learning_rate": 1.6842712516881375e-05,
      "loss": 1.458,
      "step": 5265
    },
    {
      "epoch": 1.818809318377912,
      "grad_norm": 2.5663204193115234,
      "learning_rate": 1.6800014283055358e-05,
      "loss": 1.6636,
      "step": 5270
    },
    {
      "epoch": 1.82053494391717,
      "grad_norm": 3.026125431060791,
      "learning_rate": 1.675734283556038e-05,
      "loss": 1.4486,
      "step": 5275
    },
    {
      "epoch": 1.822260569456428,
      "grad_norm": 2.4376437664031982,
      "learning_rate": 1.6714698313788352e-05,
      "loss": 1.5637,
      "step": 5280
    },
    {
      "epoch": 1.823986194995686,
      "grad_norm": 2.612245798110962,
      "learning_rate": 1.667208085704319e-05,
      "loss": 1.5229,
      "step": 5285
    },
    {
      "epoch": 1.825711820534944,
      "grad_norm": 2.7418012619018555,
      "learning_rate": 1.662949060454043e-05,
      "loss": 1.4714,
      "step": 5290
    },
    {
      "epoch": 1.8274374460742018,
      "grad_norm": 2.7969017028808594,
      "learning_rate": 1.658692769540672e-05,
      "loss": 1.6004,
      "step": 5295
    },
    {
      "epoch": 1.82916307161346,
      "grad_norm": 2.2497763633728027,
      "learning_rate": 1.65443922686794e-05,
      "loss": 1.6113,
      "step": 5300
    },
    {
      "epoch": 1.82916307161346,
      "eval_loss": 1.7510493993759155,
      "eval_runtime": 302.7748,
      "eval_samples_per_second": 17.016,
      "eval_steps_per_second": 8.508,
      "step": 5300
    },
    {
      "epoch": 1.8308886971527178,
      "grad_norm": 2.586627721786499,
      "learning_rate": 1.6501884463306048e-05,
      "loss": 1.6502,
      "step": 5305
    },
    {
      "epoch": 1.8326143226919758,
      "grad_norm": 2.362501382827759,
      "learning_rate": 1.645940441814397e-05,
      "loss": 1.4004,
      "step": 5310
    },
    {
      "epoch": 1.834339948231234,
      "grad_norm": 2.6304190158843994,
      "learning_rate": 1.641695227195983e-05,
      "loss": 1.4761,
      "step": 5315
    },
    {
      "epoch": 1.8360655737704918,
      "grad_norm": 2.3573217391967773,
      "learning_rate": 1.6374528163429143e-05,
      "loss": 1.3688,
      "step": 5320
    },
    {
      "epoch": 1.8377911993097498,
      "grad_norm": 3.4139699935913086,
      "learning_rate": 1.6332132231135837e-05,
      "loss": 1.6384,
      "step": 5325
    },
    {
      "epoch": 1.839516824849008,
      "grad_norm": 2.5102715492248535,
      "learning_rate": 1.628976461357179e-05,
      "loss": 1.4626,
      "step": 5330
    },
    {
      "epoch": 1.8412424503882656,
      "grad_norm": 3.139512062072754,
      "learning_rate": 1.62474254491364e-05,
      "loss": 1.6656,
      "step": 5335
    },
    {
      "epoch": 1.8429680759275238,
      "grad_norm": 2.573110580444336,
      "learning_rate": 1.62051148761361e-05,
      "loss": 1.5867,
      "step": 5340
    },
    {
      "epoch": 1.8446937014667817,
      "grad_norm": 2.975707530975342,
      "learning_rate": 1.6162833032783964e-05,
      "loss": 1.6817,
      "step": 5345
    },
    {
      "epoch": 1.8464193270060396,
      "grad_norm": 2.8368160724639893,
      "learning_rate": 1.6120580057199157e-05,
      "loss": 1.4657,
      "step": 5350
    },
    {
      "epoch": 1.8481449525452978,
      "grad_norm": 3.103113889694214,
      "learning_rate": 1.6078356087406595e-05,
      "loss": 1.5683,
      "step": 5355
    },
    {
      "epoch": 1.8498705780845557,
      "grad_norm": 3.2139620780944824,
      "learning_rate": 1.603616126133643e-05,
      "loss": 1.4067,
      "step": 5360
    },
    {
      "epoch": 1.8515962036238136,
      "grad_norm": 3.045511484146118,
      "learning_rate": 1.599399571682359e-05,
      "loss": 1.5182,
      "step": 5365
    },
    {
      "epoch": 1.8533218291630718,
      "grad_norm": 3.0531575679779053,
      "learning_rate": 1.5951859591607387e-05,
      "loss": 1.3746,
      "step": 5370
    },
    {
      "epoch": 1.8550474547023295,
      "grad_norm": 2.3515474796295166,
      "learning_rate": 1.590975302333099e-05,
      "loss": 1.6049,
      "step": 5375
    },
    {
      "epoch": 1.8567730802415876,
      "grad_norm": 3.5721921920776367,
      "learning_rate": 1.5867676149541063e-05,
      "loss": 1.6796,
      "step": 5380
    },
    {
      "epoch": 1.8584987057808455,
      "grad_norm": 2.5588455200195312,
      "learning_rate": 1.5825629107687224e-05,
      "loss": 1.5805,
      "step": 5385
    },
    {
      "epoch": 1.8602243313201035,
      "grad_norm": 2.424907922744751,
      "learning_rate": 1.5783612035121674e-05,
      "loss": 1.7086,
      "step": 5390
    },
    {
      "epoch": 1.8619499568593616,
      "grad_norm": 3.1782596111297607,
      "learning_rate": 1.5741625069098688e-05,
      "loss": 1.5143,
      "step": 5395
    },
    {
      "epoch": 1.8636755823986195,
      "grad_norm": 2.870239734649658,
      "learning_rate": 1.5699668346774232e-05,
      "loss": 1.619,
      "step": 5400
    },
    {
      "epoch": 1.8636755823986195,
      "eval_loss": 1.7484798431396484,
      "eval_runtime": 302.7106,
      "eval_samples_per_second": 17.02,
      "eval_steps_per_second": 8.51,
      "step": 5400
    },
    {
      "epoch": 1.8654012079378774,
      "grad_norm": 3.02476167678833,
      "learning_rate": 1.5657742005205433e-05,
      "loss": 1.5411,
      "step": 5405
    },
    {
      "epoch": 1.8671268334771356,
      "grad_norm": 2.7984063625335693,
      "learning_rate": 1.56158461813502e-05,
      "loss": 1.6282,
      "step": 5410
    },
    {
      "epoch": 1.8688524590163933,
      "grad_norm": 2.7914490699768066,
      "learning_rate": 1.557398101206676e-05,
      "loss": 1.4287,
      "step": 5415
    },
    {
      "epoch": 1.8705780845556514,
      "grad_norm": 3.309894323348999,
      "learning_rate": 1.553214663411317e-05,
      "loss": 1.535,
      "step": 5420
    },
    {
      "epoch": 1.8723037100949094,
      "grad_norm": 2.472893476486206,
      "learning_rate": 1.549034318414695e-05,
      "loss": 1.6161,
      "step": 5425
    },
    {
      "epoch": 1.8740293356341673,
      "grad_norm": 3.640540361404419,
      "learning_rate": 1.5448570798724535e-05,
      "loss": 1.5055,
      "step": 5430
    },
    {
      "epoch": 1.8757549611734254,
      "grad_norm": 2.967108964920044,
      "learning_rate": 1.5406829614300934e-05,
      "loss": 1.4418,
      "step": 5435
    },
    {
      "epoch": 1.8774805867126834,
      "grad_norm": 2.672037124633789,
      "learning_rate": 1.5365119767229198e-05,
      "loss": 1.4942,
      "step": 5440
    },
    {
      "epoch": 1.8792062122519413,
      "grad_norm": 2.9304771423339844,
      "learning_rate": 1.5323441393760042e-05,
      "loss": 1.463,
      "step": 5445
    },
    {
      "epoch": 1.8809318377911994,
      "grad_norm": 3.0559585094451904,
      "learning_rate": 1.5281794630041328e-05,
      "loss": 1.5791,
      "step": 5450
    },
    {
      "epoch": 1.8826574633304571,
      "grad_norm": 2.922791004180908,
      "learning_rate": 1.5240179612117696e-05,
      "loss": 1.4234,
      "step": 5455
    },
    {
      "epoch": 1.8843830888697153,
      "grad_norm": 2.7406787872314453,
      "learning_rate": 1.5198596475930069e-05,
      "loss": 1.563,
      "step": 5460
    },
    {
      "epoch": 1.8861087144089732,
      "grad_norm": 2.7450168132781982,
      "learning_rate": 1.5157045357315235e-05,
      "loss": 1.7527,
      "step": 5465
    },
    {
      "epoch": 1.8878343399482311,
      "grad_norm": 3.7505910396575928,
      "learning_rate": 1.5115526392005363e-05,
      "loss": 1.717,
      "step": 5470
    },
    {
      "epoch": 1.8895599654874893,
      "grad_norm": 2.6113760471343994,
      "learning_rate": 1.5074039715627627e-05,
      "loss": 1.4937,
      "step": 5475
    },
    {
      "epoch": 1.8912855910267472,
      "grad_norm": 2.304337978363037,
      "learning_rate": 1.5032585463703708e-05,
      "loss": 1.4229,
      "step": 5480
    },
    {
      "epoch": 1.8930112165660051,
      "grad_norm": 2.5453827381134033,
      "learning_rate": 1.4991163771649358e-05,
      "loss": 1.6266,
      "step": 5485
    },
    {
      "epoch": 1.8947368421052633,
      "grad_norm": 3.216003656387329,
      "learning_rate": 1.4949774774773989e-05,
      "loss": 1.6235,
      "step": 5490
    },
    {
      "epoch": 1.896462467644521,
      "grad_norm": 2.9396772384643555,
      "learning_rate": 1.4908418608280186e-05,
      "loss": 1.5106,
      "step": 5495
    },
    {
      "epoch": 1.8981880931837791,
      "grad_norm": 2.8401811122894287,
      "learning_rate": 1.4867095407263324e-05,
      "loss": 1.5246,
      "step": 5500
    },
    {
      "epoch": 1.8981880931837791,
      "eval_loss": 1.7455360889434814,
      "eval_runtime": 304.849,
      "eval_samples_per_second": 16.9,
      "eval_steps_per_second": 8.45,
      "step": 5500
    },
    {
      "epoch": 1.899913718723037,
      "grad_norm": 2.7802388668060303,
      "learning_rate": 1.4825805306711043e-05,
      "loss": 1.5513,
      "step": 5505
    },
    {
      "epoch": 1.901639344262295,
      "grad_norm": 2.8599400520324707,
      "learning_rate": 1.4784548441502902e-05,
      "loss": 1.5631,
      "step": 5510
    },
    {
      "epoch": 1.9033649698015531,
      "grad_norm": 2.6766517162323,
      "learning_rate": 1.4743324946409864e-05,
      "loss": 1.4545,
      "step": 5515
    },
    {
      "epoch": 1.905090595340811,
      "grad_norm": 3.199012041091919,
      "learning_rate": 1.4702134956093905e-05,
      "loss": 1.6733,
      "step": 5520
    },
    {
      "epoch": 1.906816220880069,
      "grad_norm": 2.8172643184661865,
      "learning_rate": 1.4660978605107528e-05,
      "loss": 1.6161,
      "step": 5525
    },
    {
      "epoch": 1.9085418464193271,
      "grad_norm": 3.50565505027771,
      "learning_rate": 1.4619856027893364e-05,
      "loss": 1.5435,
      "step": 5530
    },
    {
      "epoch": 1.910267471958585,
      "grad_norm": 2.5108911991119385,
      "learning_rate": 1.4578767358783729e-05,
      "loss": 1.569,
      "step": 5535
    },
    {
      "epoch": 1.911993097497843,
      "grad_norm": 2.6479861736297607,
      "learning_rate": 1.4537712732000148e-05,
      "loss": 1.5386,
      "step": 5540
    },
    {
      "epoch": 1.913718723037101,
      "grad_norm": 3.052103281021118,
      "learning_rate": 1.4496692281652967e-05,
      "loss": 1.6246,
      "step": 5545
    },
    {
      "epoch": 1.9154443485763588,
      "grad_norm": 2.6809332370758057,
      "learning_rate": 1.445570614174086e-05,
      "loss": 1.7033,
      "step": 5550
    },
    {
      "epoch": 1.917169974115617,
      "grad_norm": 2.7767608165740967,
      "learning_rate": 1.4414754446150463e-05,
      "loss": 1.5987,
      "step": 5555
    },
    {
      "epoch": 1.9188955996548749,
      "grad_norm": 3.1486785411834717,
      "learning_rate": 1.437383732865585e-05,
      "loss": 1.6121,
      "step": 5560
    },
    {
      "epoch": 1.9206212251941328,
      "grad_norm": 3.3245131969451904,
      "learning_rate": 1.4332954922918174e-05,
      "loss": 1.5625,
      "step": 5565
    },
    {
      "epoch": 1.922346850733391,
      "grad_norm": 2.555710792541504,
      "learning_rate": 1.4292107362485182e-05,
      "loss": 1.5696,
      "step": 5570
    },
    {
      "epoch": 1.9240724762726489,
      "grad_norm": 3.0416693687438965,
      "learning_rate": 1.4251294780790814e-05,
      "loss": 1.5375,
      "step": 5575
    },
    {
      "epoch": 1.9257981018119068,
      "grad_norm": 3.7060699462890625,
      "learning_rate": 1.421051731115471e-05,
      "loss": 1.4787,
      "step": 5580
    },
    {
      "epoch": 1.927523727351165,
      "grad_norm": 3.0065243244171143,
      "learning_rate": 1.416977508678185e-05,
      "loss": 1.6336,
      "step": 5585
    },
    {
      "epoch": 1.9292493528904227,
      "grad_norm": 2.5693912506103516,
      "learning_rate": 1.412906824076205e-05,
      "loss": 1.4733,
      "step": 5590
    },
    {
      "epoch": 1.9309749784296808,
      "grad_norm": 2.933281660079956,
      "learning_rate": 1.4088396906069578e-05,
      "loss": 1.4804,
      "step": 5595
    },
    {
      "epoch": 1.9327006039689387,
      "grad_norm": 2.5541977882385254,
      "learning_rate": 1.4047761215562698e-05,
      "loss": 1.4021,
      "step": 5600
    },
    {
      "epoch": 1.9327006039689387,
      "eval_loss": 1.740545630455017,
      "eval_runtime": 302.9034,
      "eval_samples_per_second": 17.009,
      "eval_steps_per_second": 8.504,
      "step": 5600
    },
    {
      "epoch": 1.9344262295081966,
      "grad_norm": 3.630831241607666,
      "learning_rate": 1.4007161301983235e-05,
      "loss": 1.548,
      "step": 5605
    },
    {
      "epoch": 1.9361518550474548,
      "grad_norm": 2.3484392166137695,
      "learning_rate": 1.3966597297956146e-05,
      "loss": 1.4105,
      "step": 5610
    },
    {
      "epoch": 1.9378774805867127,
      "grad_norm": 3.3102633953094482,
      "learning_rate": 1.3926069335989067e-05,
      "loss": 1.6461,
      "step": 5615
    },
    {
      "epoch": 1.9396031061259706,
      "grad_norm": 2.7500133514404297,
      "learning_rate": 1.3885577548471928e-05,
      "loss": 1.5583,
      "step": 5620
    },
    {
      "epoch": 1.9413287316652288,
      "grad_norm": 3.0275049209594727,
      "learning_rate": 1.3845122067676463e-05,
      "loss": 1.5725,
      "step": 5625
    },
    {
      "epoch": 1.9430543572044865,
      "grad_norm": 3.1371095180511475,
      "learning_rate": 1.3804703025755827e-05,
      "loss": 1.6508,
      "step": 5630
    },
    {
      "epoch": 1.9447799827437446,
      "grad_norm": 3.0074820518493652,
      "learning_rate": 1.3764320554744118e-05,
      "loss": 1.4518,
      "step": 5635
    },
    {
      "epoch": 1.9465056082830026,
      "grad_norm": 2.897449016571045,
      "learning_rate": 1.3723974786555992e-05,
      "loss": 1.5115,
      "step": 5640
    },
    {
      "epoch": 1.9482312338222605,
      "grad_norm": 2.6466448307037354,
      "learning_rate": 1.3683665852986205e-05,
      "loss": 1.4718,
      "step": 5645
    },
    {
      "epoch": 1.9499568593615186,
      "grad_norm": 2.5645945072174072,
      "learning_rate": 1.3643393885709188e-05,
      "loss": 1.7192,
      "step": 5650
    },
    {
      "epoch": 1.9516824849007766,
      "grad_norm": 2.671315908432007,
      "learning_rate": 1.3603159016278625e-05,
      "loss": 1.5601,
      "step": 5655
    },
    {
      "epoch": 1.9534081104400345,
      "grad_norm": 3.0719852447509766,
      "learning_rate": 1.3562961376126981e-05,
      "loss": 1.5995,
      "step": 5660
    },
    {
      "epoch": 1.9551337359792926,
      "grad_norm": 3.292687177658081,
      "learning_rate": 1.3522801096565152e-05,
      "loss": 1.5884,
      "step": 5665
    },
    {
      "epoch": 1.9568593615185503,
      "grad_norm": 3.0056357383728027,
      "learning_rate": 1.3482678308781948e-05,
      "loss": 1.3273,
      "step": 5670
    },
    {
      "epoch": 1.9585849870578085,
      "grad_norm": 2.579847574234009,
      "learning_rate": 1.3442593143843757e-05,
      "loss": 1.5392,
      "step": 5675
    },
    {
      "epoch": 1.9603106125970664,
      "grad_norm": 3.0611979961395264,
      "learning_rate": 1.3402545732694005e-05,
      "loss": 1.7643,
      "step": 5680
    },
    {
      "epoch": 1.9620362381363243,
      "grad_norm": 2.068230628967285,
      "learning_rate": 1.3362536206152849e-05,
      "loss": 1.3339,
      "step": 5685
    },
    {
      "epoch": 1.9637618636755825,
      "grad_norm": 3.067996025085449,
      "learning_rate": 1.332256469491665e-05,
      "loss": 1.618,
      "step": 5690
    },
    {
      "epoch": 1.9654874892148404,
      "grad_norm": 2.8169779777526855,
      "learning_rate": 1.3282631329557626e-05,
      "loss": 1.4599,
      "step": 5695
    },
    {
      "epoch": 1.9672131147540983,
      "grad_norm": 4.107193470001221,
      "learning_rate": 1.3242736240523335e-05,
      "loss": 1.6958,
      "step": 5700
    },
    {
      "epoch": 1.9672131147540983,
      "eval_loss": 1.7376062870025635,
      "eval_runtime": 302.7583,
      "eval_samples_per_second": 17.017,
      "eval_steps_per_second": 8.508,
      "step": 5700
    },
    {
      "epoch": 1.9689387402933565,
      "grad_norm": 2.857182502746582,
      "learning_rate": 1.3202879558136355e-05,
      "loss": 1.6825,
      "step": 5705
    },
    {
      "epoch": 1.9706643658326142,
      "grad_norm": 3.159440755844116,
      "learning_rate": 1.3163061412593753e-05,
      "loss": 1.4653,
      "step": 5710
    },
    {
      "epoch": 1.9723899913718723,
      "grad_norm": 3.3830175399780273,
      "learning_rate": 1.312328193396674e-05,
      "loss": 1.4238,
      "step": 5715
    },
    {
      "epoch": 1.9741156169111302,
      "grad_norm": 3.119508981704712,
      "learning_rate": 1.3083541252200221e-05,
      "loss": 1.6132,
      "step": 5720
    },
    {
      "epoch": 1.9758412424503882,
      "grad_norm": 3.122955083847046,
      "learning_rate": 1.3043839497112353e-05,
      "loss": 1.6232,
      "step": 5725
    },
    {
      "epoch": 1.9775668679896463,
      "grad_norm": 2.551039934158325,
      "learning_rate": 1.3004176798394143e-05,
      "loss": 1.397,
      "step": 5730
    },
    {
      "epoch": 1.9792924935289042,
      "grad_norm": 3.2765815258026123,
      "learning_rate": 1.2964553285608994e-05,
      "loss": 1.5828,
      "step": 5735
    },
    {
      "epoch": 1.9810181190681622,
      "grad_norm": 3.027974843978882,
      "learning_rate": 1.2924969088192335e-05,
      "loss": 1.3892,
      "step": 5740
    },
    {
      "epoch": 1.9827437446074203,
      "grad_norm": 2.8773632049560547,
      "learning_rate": 1.288542433545113e-05,
      "loss": 1.6072,
      "step": 5745
    },
    {
      "epoch": 1.984469370146678,
      "grad_norm": 3.3793728351593018,
      "learning_rate": 1.2845919156563529e-05,
      "loss": 1.6093,
      "step": 5750
    },
    {
      "epoch": 1.9861949956859362,
      "grad_norm": 3.126124382019043,
      "learning_rate": 1.2806453680578373e-05,
      "loss": 1.6073,
      "step": 5755
    },
    {
      "epoch": 1.987920621225194,
      "grad_norm": 2.9278318881988525,
      "learning_rate": 1.2767028036414836e-05,
      "loss": 1.6056,
      "step": 5760
    },
    {
      "epoch": 1.989646246764452,
      "grad_norm": 2.590902328491211,
      "learning_rate": 1.2727642352861969e-05,
      "loss": 1.5603,
      "step": 5765
    },
    {
      "epoch": 1.9913718723037102,
      "grad_norm": 2.9497194290161133,
      "learning_rate": 1.268829675857828e-05,
      "loss": 1.4533,
      "step": 5770
    },
    {
      "epoch": 1.993097497842968,
      "grad_norm": 3.302985668182373,
      "learning_rate": 1.2648991382091335e-05,
      "loss": 1.5024,
      "step": 5775
    },
    {
      "epoch": 1.994823123382226,
      "grad_norm": 2.9602510929107666,
      "learning_rate": 1.2609726351797291e-05,
      "loss": 1.5502,
      "step": 5780
    },
    {
      "epoch": 1.9965487489214842,
      "grad_norm": 2.977802276611328,
      "learning_rate": 1.2570501795960554e-05,
      "loss": 1.4939,
      "step": 5785
    },
    {
      "epoch": 1.9982743744607419,
      "grad_norm": 3.1901183128356934,
      "learning_rate": 1.2531317842713263e-05,
      "loss": 1.5699,
      "step": 5790
    },
    {
      "epoch": 2.0,
      "grad_norm": 4.730459690093994,
      "learning_rate": 1.2492174620054981e-05,
      "loss": 1.603,
      "step": 5795
    },
    {
      "epoch": 2.001725625539258,
      "grad_norm": 2.2386093139648438,
      "learning_rate": 1.2453072255852153e-05,
      "loss": 1.2991,
      "step": 5800
    },
    {
      "epoch": 2.001725625539258,
      "eval_loss": 1.7353301048278809,
      "eval_runtime": 302.7618,
      "eval_samples_per_second": 17.017,
      "eval_steps_per_second": 8.508,
      "step": 5800
    },
    {
      "epoch": 2.003451251078516,
      "grad_norm": 2.4578323364257812,
      "learning_rate": 1.241401087783783e-05,
      "loss": 1.2056,
      "step": 5805
    },
    {
      "epoch": 2.005176876617774,
      "grad_norm": 2.93528151512146,
      "learning_rate": 1.237499061361111e-05,
      "loss": 1.3082,
      "step": 5810
    },
    {
      "epoch": 2.006902502157032,
      "grad_norm": 2.9739742279052734,
      "learning_rate": 1.2336011590636833e-05,
      "loss": 1.318,
      "step": 5815
    },
    {
      "epoch": 2.00862812769629,
      "grad_norm": 2.6776251792907715,
      "learning_rate": 1.2297073936245082e-05,
      "loss": 1.2964,
      "step": 5820
    },
    {
      "epoch": 2.010353753235548,
      "grad_norm": 2.695183277130127,
      "learning_rate": 1.2258177777630841e-05,
      "loss": 1.2769,
      "step": 5825
    },
    {
      "epoch": 2.0120793787748057,
      "grad_norm": 2.0514984130859375,
      "learning_rate": 1.2219323241853505e-05,
      "loss": 1.2394,
      "step": 5830
    },
    {
      "epoch": 2.013805004314064,
      "grad_norm": 2.8710036277770996,
      "learning_rate": 1.2180510455836532e-05,
      "loss": 1.4214,
      "step": 5835
    },
    {
      "epoch": 2.015530629853322,
      "grad_norm": 2.6582422256469727,
      "learning_rate": 1.2141739546366987e-05,
      "loss": 1.3354,
      "step": 5840
    },
    {
      "epoch": 2.0172562553925797,
      "grad_norm": 3.301408529281616,
      "learning_rate": 1.2103010640095141e-05,
      "loss": 1.2902,
      "step": 5845
    },
    {
      "epoch": 2.018981880931838,
      "grad_norm": 2.62007212638855,
      "learning_rate": 1.2064323863534071e-05,
      "loss": 1.4419,
      "step": 5850
    },
    {
      "epoch": 2.020707506471096,
      "grad_norm": 2.926845073699951,
      "learning_rate": 1.202567934305919e-05,
      "loss": 1.4533,
      "step": 5855
    },
    {
      "epoch": 2.0224331320103537,
      "grad_norm": 3.0972306728363037,
      "learning_rate": 1.1987077204907929e-05,
      "loss": 1.3776,
      "step": 5860
    },
    {
      "epoch": 2.024158757549612,
      "grad_norm": 2.658162832260132,
      "learning_rate": 1.1948517575179224e-05,
      "loss": 1.3226,
      "step": 5865
    },
    {
      "epoch": 2.0258843830888695,
      "grad_norm": 2.8892781734466553,
      "learning_rate": 1.1910000579833192e-05,
      "loss": 1.3716,
      "step": 5870
    },
    {
      "epoch": 2.0276100086281277,
      "grad_norm": 3.1913161277770996,
      "learning_rate": 1.1871526344690641e-05,
      "loss": 1.3514,
      "step": 5875
    },
    {
      "epoch": 2.029335634167386,
      "grad_norm": 2.9531497955322266,
      "learning_rate": 1.1833094995432729e-05,
      "loss": 1.3363,
      "step": 5880
    },
    {
      "epoch": 2.0310612597066435,
      "grad_norm": 2.4168460369110107,
      "learning_rate": 1.1794706657600504e-05,
      "loss": 1.4586,
      "step": 5885
    },
    {
      "epoch": 2.0327868852459017,
      "grad_norm": 2.9351015090942383,
      "learning_rate": 1.1756361456594523e-05,
      "loss": 1.284,
      "step": 5890
    },
    {
      "epoch": 2.03451251078516,
      "grad_norm": 2.833801746368408,
      "learning_rate": 1.171805951767443e-05,
      "loss": 1.1747,
      "step": 5895
    },
    {
      "epoch": 2.0362381363244175,
      "grad_norm": 3.2054994106292725,
      "learning_rate": 1.1679800965958524e-05,
      "loss": 1.3191,
      "step": 5900
    },
    {
      "epoch": 2.0362381363244175,
      "eval_loss": 1.7638386487960815,
      "eval_runtime": 302.8399,
      "eval_samples_per_second": 17.012,
      "eval_steps_per_second": 8.506,
      "step": 5900
    },
    {
      "epoch": 2.0379637618636757,
      "grad_norm": 2.484315872192383,
      "learning_rate": 1.1641585926423412e-05,
      "loss": 1.3192,
      "step": 5905
    },
    {
      "epoch": 2.0396893874029334,
      "grad_norm": 2.5062918663024902,
      "learning_rate": 1.1603414523903525e-05,
      "loss": 1.2545,
      "step": 5910
    },
    {
      "epoch": 2.0414150129421915,
      "grad_norm": 2.940427303314209,
      "learning_rate": 1.1565286883090777e-05,
      "loss": 1.251,
      "step": 5915
    },
    {
      "epoch": 2.0431406384814497,
      "grad_norm": 2.3915274143218994,
      "learning_rate": 1.1527203128534111e-05,
      "loss": 1.2729,
      "step": 5920
    },
    {
      "epoch": 2.0448662640207074,
      "grad_norm": 2.59128737449646,
      "learning_rate": 1.1489163384639134e-05,
      "loss": 1.3133,
      "step": 5925
    },
    {
      "epoch": 2.0465918895599655,
      "grad_norm": 2.8275630474090576,
      "learning_rate": 1.1451167775667646e-05,
      "loss": 1.1557,
      "step": 5930
    },
    {
      "epoch": 2.0483175150992237,
      "grad_norm": 2.4824905395507812,
      "learning_rate": 1.1413216425737316e-05,
      "loss": 1.2544,
      "step": 5935
    },
    {
      "epoch": 2.0500431406384814,
      "grad_norm": 2.724959373474121,
      "learning_rate": 1.1375309458821199e-05,
      "loss": 1.1892,
      "step": 5940
    },
    {
      "epoch": 2.0517687661777395,
      "grad_norm": 2.4328575134277344,
      "learning_rate": 1.1337446998747392e-05,
      "loss": 1.2846,
      "step": 5945
    },
    {
      "epoch": 2.053494391716997,
      "grad_norm": 2.5160939693450928,
      "learning_rate": 1.1299629169198608e-05,
      "loss": 1.2655,
      "step": 5950
    },
    {
      "epoch": 2.0552200172562554,
      "grad_norm": 3.144073247909546,
      "learning_rate": 1.126185609371174e-05,
      "loss": 1.4232,
      "step": 5955
    },
    {
      "epoch": 2.0569456427955135,
      "grad_norm": 2.6219165325164795,
      "learning_rate": 1.1224127895677508e-05,
      "loss": 1.3452,
      "step": 5960
    },
    {
      "epoch": 2.058671268334771,
      "grad_norm": 2.403637647628784,
      "learning_rate": 1.1186444698340034e-05,
      "loss": 1.195,
      "step": 5965
    },
    {
      "epoch": 2.0603968938740294,
      "grad_norm": 2.864558458328247,
      "learning_rate": 1.1148806624796446e-05,
      "loss": 1.2425,
      "step": 5970
    },
    {
      "epoch": 2.0621225194132875,
      "grad_norm": 3.4144461154937744,
      "learning_rate": 1.1111213797996431e-05,
      "loss": 1.2935,
      "step": 5975
    },
    {
      "epoch": 2.063848144952545,
      "grad_norm": 2.899101734161377,
      "learning_rate": 1.1073666340741926e-05,
      "loss": 1.3557,
      "step": 5980
    },
    {
      "epoch": 2.0655737704918034,
      "grad_norm": 3.2540061473846436,
      "learning_rate": 1.1036164375686608e-05,
      "loss": 1.3213,
      "step": 5985
    },
    {
      "epoch": 2.067299396031061,
      "grad_norm": 3.270378828048706,
      "learning_rate": 1.09987080253356e-05,
      "loss": 1.1403,
      "step": 5990
    },
    {
      "epoch": 2.069025021570319,
      "grad_norm": 3.1041533946990967,
      "learning_rate": 1.096129741204497e-05,
      "loss": 1.2678,
      "step": 5995
    },
    {
      "epoch": 2.0707506471095773,
      "grad_norm": 3.3815457820892334,
      "learning_rate": 1.092393265802142e-05,
      "loss": 1.4244,
      "step": 6000
    },
    {
      "epoch": 2.0707506471095773,
      "eval_loss": 1.7622179985046387,
      "eval_runtime": 302.564,
      "eval_samples_per_second": 17.028,
      "eval_steps_per_second": 8.514,
      "step": 6000
    },
    {
      "epoch": 2.072476272648835,
      "grad_norm": 2.3119056224823,
      "learning_rate": 1.088661388532182e-05,
      "loss": 1.2736,
      "step": 6005
    },
    {
      "epoch": 2.074201898188093,
      "grad_norm": 2.8252041339874268,
      "learning_rate": 1.0849341215852852e-05,
      "loss": 1.2989,
      "step": 6010
    },
    {
      "epoch": 2.0759275237273513,
      "grad_norm": 2.3465843200683594,
      "learning_rate": 1.0812114771370594e-05,
      "loss": 1.1559,
      "step": 6015
    },
    {
      "epoch": 2.077653149266609,
      "grad_norm": 2.759913444519043,
      "learning_rate": 1.0774934673480105e-05,
      "loss": 1.3089,
      "step": 6020
    },
    {
      "epoch": 2.079378774805867,
      "grad_norm": 2.6685805320739746,
      "learning_rate": 1.0737801043635079e-05,
      "loss": 1.2807,
      "step": 6025
    },
    {
      "epoch": 2.081104400345125,
      "grad_norm": 2.773902654647827,
      "learning_rate": 1.070071400313738e-05,
      "loss": 1.3651,
      "step": 6030
    },
    {
      "epoch": 2.082830025884383,
      "grad_norm": 2.557523012161255,
      "learning_rate": 1.0663673673136706e-05,
      "loss": 1.3162,
      "step": 6035
    },
    {
      "epoch": 2.084555651423641,
      "grad_norm": 3.1752030849456787,
      "learning_rate": 1.0626680174630168e-05,
      "loss": 1.3553,
      "step": 6040
    },
    {
      "epoch": 2.086281276962899,
      "grad_norm": 2.9074339866638184,
      "learning_rate": 1.0589733628461897e-05,
      "loss": 1.2501,
      "step": 6045
    },
    {
      "epoch": 2.088006902502157,
      "grad_norm": 2.2894399166107178,
      "learning_rate": 1.0552834155322625e-05,
      "loss": 1.0742,
      "step": 6050
    },
    {
      "epoch": 2.089732528041415,
      "grad_norm": 2.340026378631592,
      "learning_rate": 1.0515981875749345e-05,
      "loss": 1.3905,
      "step": 6055
    },
    {
      "epoch": 2.091458153580673,
      "grad_norm": 3.7347891330718994,
      "learning_rate": 1.0479176910124861e-05,
      "loss": 1.4442,
      "step": 6060
    },
    {
      "epoch": 2.093183779119931,
      "grad_norm": 3.296314239501953,
      "learning_rate": 1.0442419378677431e-05,
      "loss": 1.2988,
      "step": 6065
    },
    {
      "epoch": 2.094909404659189,
      "grad_norm": 2.8004791736602783,
      "learning_rate": 1.0405709401480377e-05,
      "loss": 1.3158,
      "step": 6070
    },
    {
      "epoch": 2.096635030198447,
      "grad_norm": 2.7599775791168213,
      "learning_rate": 1.0369047098451644e-05,
      "loss": 1.366,
      "step": 6075
    },
    {
      "epoch": 2.098360655737705,
      "grad_norm": 2.4877076148986816,
      "learning_rate": 1.0332432589353475e-05,
      "loss": 1.4161,
      "step": 6080
    },
    {
      "epoch": 2.1000862812769627,
      "grad_norm": 2.922769546508789,
      "learning_rate": 1.029586599379197e-05,
      "loss": 1.3209,
      "step": 6085
    },
    {
      "epoch": 2.101811906816221,
      "grad_norm": 4.2689924240112305,
      "learning_rate": 1.0259347431216734e-05,
      "loss": 1.2491,
      "step": 6090
    },
    {
      "epoch": 2.103537532355479,
      "grad_norm": 2.811990976333618,
      "learning_rate": 1.0222877020920432e-05,
      "loss": 1.3132,
      "step": 6095
    },
    {
      "epoch": 2.1052631578947367,
      "grad_norm": 3.149024724960327,
      "learning_rate": 1.0186454882038472e-05,
      "loss": 1.2649,
      "step": 6100
    },
    {
      "epoch": 2.1052631578947367,
      "eval_loss": 1.7668492794036865,
      "eval_runtime": 303.2367,
      "eval_samples_per_second": 16.99,
      "eval_steps_per_second": 8.495,
      "step": 6100
    },
    {
      "epoch": 2.106988783433995,
      "grad_norm": 3.229386568069458,
      "learning_rate": 1.0150081133548533e-05,
      "loss": 1.3358,
      "step": 6105
    },
    {
      "epoch": 2.108714408973253,
      "grad_norm": 3.0076613426208496,
      "learning_rate": 1.0113755894270269e-05,
      "loss": 1.471,
      "step": 6110
    },
    {
      "epoch": 2.1104400345125107,
      "grad_norm": 2.632561683654785,
      "learning_rate": 1.0077479282864829e-05,
      "loss": 1.475,
      "step": 6115
    },
    {
      "epoch": 2.112165660051769,
      "grad_norm": 3.5188791751861572,
      "learning_rate": 1.004125141783454e-05,
      "loss": 1.3558,
      "step": 6120
    },
    {
      "epoch": 2.1138912855910266,
      "grad_norm": 2.0564398765563965,
      "learning_rate": 1.0005072417522484e-05,
      "loss": 1.2795,
      "step": 6125
    },
    {
      "epoch": 2.1156169111302847,
      "grad_norm": 3.1895384788513184,
      "learning_rate": 9.968942400112125e-06,
      "loss": 1.3555,
      "step": 6130
    },
    {
      "epoch": 2.117342536669543,
      "grad_norm": 2.735651731491089,
      "learning_rate": 9.932861483626915e-06,
      "loss": 1.2671,
      "step": 6135
    },
    {
      "epoch": 2.1190681622088006,
      "grad_norm": 3.4709982872009277,
      "learning_rate": 9.896829785929899e-06,
      "loss": 1.328,
      "step": 6140
    },
    {
      "epoch": 2.1207937877480587,
      "grad_norm": 2.9391374588012695,
      "learning_rate": 9.860847424723363e-06,
      "loss": 1.333,
      "step": 6145
    },
    {
      "epoch": 2.122519413287317,
      "grad_norm": 2.165335178375244,
      "learning_rate": 9.824914517548408e-06,
      "loss": 1.3015,
      "step": 6150
    },
    {
      "epoch": 2.1242450388265746,
      "grad_norm": 3.1543285846710205,
      "learning_rate": 9.789031181784606e-06,
      "loss": 1.4338,
      "step": 6155
    },
    {
      "epoch": 2.1259706643658327,
      "grad_norm": 2.722301483154297,
      "learning_rate": 9.753197534649583e-06,
      "loss": 1.3127,
      "step": 6160
    },
    {
      "epoch": 2.1276962899050904,
      "grad_norm": 2.852005958557129,
      "learning_rate": 9.717413693198674e-06,
      "loss": 1.2205,
      "step": 6165
    },
    {
      "epoch": 2.1294219154443486,
      "grad_norm": 2.996551990509033,
      "learning_rate": 9.68167977432448e-06,
      "loss": 1.2192,
      "step": 6170
    },
    {
      "epoch": 2.1311475409836067,
      "grad_norm": 2.7773499488830566,
      "learning_rate": 9.645995894756557e-06,
      "loss": 1.292,
      "step": 6175
    },
    {
      "epoch": 2.1328731665228644,
      "grad_norm": 3.4746716022491455,
      "learning_rate": 9.610362171060972e-06,
      "loss": 1.3676,
      "step": 6180
    },
    {
      "epoch": 2.1345987920621226,
      "grad_norm": 3.166818141937256,
      "learning_rate": 9.57477871963998e-06,
      "loss": 1.2668,
      "step": 6185
    },
    {
      "epoch": 2.1363244176013807,
      "grad_norm": 3.707212209701538,
      "learning_rate": 9.539245656731602e-06,
      "loss": 1.2746,
      "step": 6190
    },
    {
      "epoch": 2.1380500431406384,
      "grad_norm": 3.0281999111175537,
      "learning_rate": 9.50376309840925e-06,
      "loss": 1.3675,
      "step": 6195
    },
    {
      "epoch": 2.1397756686798965,
      "grad_norm": 3.1510376930236816,
      "learning_rate": 9.468331160581365e-06,
      "loss": 1.4013,
      "step": 6200
    },
    {
      "epoch": 2.1397756686798965,
      "eval_loss": 1.7670265436172485,
      "eval_runtime": 302.6635,
      "eval_samples_per_second": 17.022,
      "eval_steps_per_second": 8.511,
      "step": 6200
    },
    {
      "epoch": 2.1415012942191542,
      "grad_norm": 3.252293109893799,
      "learning_rate": 9.43294995899103e-06,
      "loss": 1.152,
      "step": 6205
    },
    {
      "epoch": 2.1432269197584124,
      "grad_norm": 2.2143964767456055,
      "learning_rate": 9.397619609215603e-06,
      "loss": 1.2438,
      "step": 6210
    },
    {
      "epoch": 2.1449525452976705,
      "grad_norm": 3.110769271850586,
      "learning_rate": 9.362340226666294e-06,
      "loss": 1.3907,
      "step": 6215
    },
    {
      "epoch": 2.1466781708369282,
      "grad_norm": 2.5117440223693848,
      "learning_rate": 9.327111926587861e-06,
      "loss": 1.3239,
      "step": 6220
    },
    {
      "epoch": 2.1484037963761864,
      "grad_norm": 4.24735164642334,
      "learning_rate": 9.291934824058162e-06,
      "loss": 1.2518,
      "step": 6225
    },
    {
      "epoch": 2.1501294219154445,
      "grad_norm": 2.960930824279785,
      "learning_rate": 9.25680903398783e-06,
      "loss": 1.4043,
      "step": 6230
    },
    {
      "epoch": 2.1518550474547022,
      "grad_norm": 3.3274881839752197,
      "learning_rate": 9.221734671119872e-06,
      "loss": 1.4238,
      "step": 6235
    },
    {
      "epoch": 2.1535806729939604,
      "grad_norm": 2.455388307571411,
      "learning_rate": 9.186711850029323e-06,
      "loss": 1.178,
      "step": 6240
    },
    {
      "epoch": 2.155306298533218,
      "grad_norm": 2.687474012374878,
      "learning_rate": 9.151740685122803e-06,
      "loss": 1.2295,
      "step": 6245
    },
    {
      "epoch": 2.1570319240724762,
      "grad_norm": 3.129704713821411,
      "learning_rate": 9.116821290638234e-06,
      "loss": 1.3589,
      "step": 6250
    },
    {
      "epoch": 2.1587575496117344,
      "grad_norm": 3.0012001991271973,
      "learning_rate": 9.081953780644411e-06,
      "loss": 1.1478,
      "step": 6255
    },
    {
      "epoch": 2.160483175150992,
      "grad_norm": 2.8292531967163086,
      "learning_rate": 9.047138269040623e-06,
      "loss": 1.2757,
      "step": 6260
    },
    {
      "epoch": 2.1622088006902502,
      "grad_norm": 2.356199026107788,
      "learning_rate": 9.012374869556333e-06,
      "loss": 1.1934,
      "step": 6265
    },
    {
      "epoch": 2.1639344262295084,
      "grad_norm": 3.1599347591400146,
      "learning_rate": 8.977663695750738e-06,
      "loss": 1.3136,
      "step": 6270
    },
    {
      "epoch": 2.165660051768766,
      "grad_norm": 2.7733418941497803,
      "learning_rate": 8.943004861012452e-06,
      "loss": 1.3098,
      "step": 6275
    },
    {
      "epoch": 2.1673856773080242,
      "grad_norm": 2.7642838954925537,
      "learning_rate": 8.908398478559121e-06,
      "loss": 1.3978,
      "step": 6280
    },
    {
      "epoch": 2.1691113028472824,
      "grad_norm": 2.4933018684387207,
      "learning_rate": 8.873844661437044e-06,
      "loss": 1.2554,
      "step": 6285
    },
    {
      "epoch": 2.17083692838654,
      "grad_norm": 2.439387321472168,
      "learning_rate": 8.839343522520793e-06,
      "loss": 1.2677,
      "step": 6290
    },
    {
      "epoch": 2.1725625539257982,
      "grad_norm": 2.0462636947631836,
      "learning_rate": 8.80489517451289e-06,
      "loss": 1.2121,
      "step": 6295
    },
    {
      "epoch": 2.174288179465056,
      "grad_norm": 2.51237416267395,
      "learning_rate": 8.770499729943369e-06,
      "loss": 1.1802,
      "step": 6300
    },
    {
      "epoch": 2.174288179465056,
      "eval_loss": 1.763197660446167,
      "eval_runtime": 302.6,
      "eval_samples_per_second": 17.026,
      "eval_steps_per_second": 8.513,
      "step": 6300
    },
    {
      "epoch": 2.176013805004314,
      "grad_norm": 3.1545441150665283,
      "learning_rate": 8.736157301169481e-06,
      "loss": 1.3355,
      "step": 6305
    },
    {
      "epoch": 2.177739430543572,
      "grad_norm": 2.5326473712921143,
      "learning_rate": 8.701868000375296e-06,
      "loss": 1.2421,
      "step": 6310
    },
    {
      "epoch": 2.17946505608283,
      "grad_norm": 3.215494155883789,
      "learning_rate": 8.6676319395713e-06,
      "loss": 1.2504,
      "step": 6315
    },
    {
      "epoch": 2.181190681622088,
      "grad_norm": 2.5014655590057373,
      "learning_rate": 8.633449230594095e-06,
      "loss": 1.3452,
      "step": 6320
    },
    {
      "epoch": 2.1829163071613458,
      "grad_norm": 3.1138620376586914,
      "learning_rate": 8.599319985105993e-06,
      "loss": 1.4128,
      "step": 6325
    },
    {
      "epoch": 2.184641932700604,
      "grad_norm": 2.7809622287750244,
      "learning_rate": 8.565244314594666e-06,
      "loss": 1.2509,
      "step": 6330
    },
    {
      "epoch": 2.186367558239862,
      "grad_norm": 2.65628719329834,
      "learning_rate": 8.531222330372759e-06,
      "loss": 1.2036,
      "step": 6335
    },
    {
      "epoch": 2.1880931837791198,
      "grad_norm": 3.4086861610412598,
      "learning_rate": 8.497254143577565e-06,
      "loss": 1.3682,
      "step": 6340
    },
    {
      "epoch": 2.189818809318378,
      "grad_norm": 3.044257879257202,
      "learning_rate": 8.463339865170617e-06,
      "loss": 1.4754,
      "step": 6345
    },
    {
      "epoch": 2.191544434857636,
      "grad_norm": 3.9227702617645264,
      "learning_rate": 8.42947960593737e-06,
      "loss": 1.2636,
      "step": 6350
    },
    {
      "epoch": 2.1932700603968938,
      "grad_norm": 3.3141627311706543,
      "learning_rate": 8.395673476486812e-06,
      "loss": 1.2938,
      "step": 6355
    },
    {
      "epoch": 2.194995685936152,
      "grad_norm": 3.188743829727173,
      "learning_rate": 8.361921587251115e-06,
      "loss": 1.2731,
      "step": 6360
    },
    {
      "epoch": 2.19672131147541,
      "grad_norm": 2.899747133255005,
      "learning_rate": 8.328224048485247e-06,
      "loss": 1.2967,
      "step": 6365
    },
    {
      "epoch": 2.1984469370146678,
      "grad_norm": 2.885918378829956,
      "learning_rate": 8.294580970266651e-06,
      "loss": 1.2685,
      "step": 6370
    },
    {
      "epoch": 2.200172562553926,
      "grad_norm": 2.901318073272705,
      "learning_rate": 8.260992462494876e-06,
      "loss": 1.253,
      "step": 6375
    },
    {
      "epoch": 2.2018981880931836,
      "grad_norm": 3.492069721221924,
      "learning_rate": 8.22745863489118e-06,
      "loss": 1.3864,
      "step": 6380
    },
    {
      "epoch": 2.2036238136324418,
      "grad_norm": 3.102193832397461,
      "learning_rate": 8.193979596998236e-06,
      "loss": 1.3964,
      "step": 6385
    },
    {
      "epoch": 2.2053494391717,
      "grad_norm": 3.397854804992676,
      "learning_rate": 8.1605554581797e-06,
      "loss": 1.2919,
      "step": 6390
    },
    {
      "epoch": 2.2070750647109576,
      "grad_norm": 2.876600503921509,
      "learning_rate": 8.127186327619924e-06,
      "loss": 1.3272,
      "step": 6395
    },
    {
      "epoch": 2.2088006902502157,
      "grad_norm": 3.2410759925842285,
      "learning_rate": 8.093872314323561e-06,
      "loss": 1.3795,
      "step": 6400
    },
    {
      "epoch": 2.2088006902502157,
      "eval_loss": 1.7629917860031128,
      "eval_runtime": 302.6775,
      "eval_samples_per_second": 17.021,
      "eval_steps_per_second": 8.511,
      "step": 6400
    },
    {
      "epoch": 2.2105263157894735,
      "grad_norm": 3.196251392364502,
      "learning_rate": 8.060613527115221e-06,
      "loss": 1.3146,
      "step": 6405
    },
    {
      "epoch": 2.2122519413287316,
      "grad_norm": 2.944650411605835,
      "learning_rate": 8.02741007463909e-06,
      "loss": 1.4449,
      "step": 6410
    },
    {
      "epoch": 2.2139775668679897,
      "grad_norm": 2.9971578121185303,
      "learning_rate": 7.994262065358617e-06,
      "loss": 1.4397,
      "step": 6415
    },
    {
      "epoch": 2.2157031924072474,
      "grad_norm": 3.157585382461548,
      "learning_rate": 7.961169607556124e-06,
      "loss": 1.4975,
      "step": 6420
    },
    {
      "epoch": 2.2174288179465056,
      "grad_norm": 2.7568399906158447,
      "learning_rate": 7.928132809332475e-06,
      "loss": 1.3811,
      "step": 6425
    },
    {
      "epoch": 2.2191544434857637,
      "grad_norm": 2.702613592147827,
      "learning_rate": 7.895151778606727e-06,
      "loss": 1.3034,
      "step": 6430
    },
    {
      "epoch": 2.2208800690250214,
      "grad_norm": 2.463597059249878,
      "learning_rate": 7.862226623115724e-06,
      "loss": 1.4248,
      "step": 6435
    },
    {
      "epoch": 2.2226056945642796,
      "grad_norm": 2.9728667736053467,
      "learning_rate": 7.829357450413846e-06,
      "loss": 1.2489,
      "step": 6440
    },
    {
      "epoch": 2.2243313201035377,
      "grad_norm": 2.8808813095092773,
      "learning_rate": 7.796544367872544e-06,
      "loss": 1.2108,
      "step": 6445
    },
    {
      "epoch": 2.2260569456427954,
      "grad_norm": 2.9584853649139404,
      "learning_rate": 7.763787482680076e-06,
      "loss": 1.3631,
      "step": 6450
    },
    {
      "epoch": 2.2277825711820536,
      "grad_norm": 3.509110927581787,
      "learning_rate": 7.731086901841103e-06,
      "loss": 1.2872,
      "step": 6455
    },
    {
      "epoch": 2.2295081967213113,
      "grad_norm": 2.850428581237793,
      "learning_rate": 7.698442732176381e-06,
      "loss": 1.2874,
      "step": 6460
    },
    {
      "epoch": 2.2312338222605694,
      "grad_norm": 2.6531941890716553,
      "learning_rate": 7.665855080322365e-06,
      "loss": 1.3524,
      "step": 6465
    },
    {
      "epoch": 2.2329594477998276,
      "grad_norm": 2.568528890609741,
      "learning_rate": 7.63332405273091e-06,
      "loss": 1.3036,
      "step": 6470
    },
    {
      "epoch": 2.2346850733390853,
      "grad_norm": 2.786864757537842,
      "learning_rate": 7.600849755668899e-06,
      "loss": 1.2787,
      "step": 6475
    },
    {
      "epoch": 2.2364106988783434,
      "grad_norm": 3.168595552444458,
      "learning_rate": 7.568432295217887e-06,
      "loss": 1.269,
      "step": 6480
    },
    {
      "epoch": 2.2381363244176016,
      "grad_norm": 3.0936927795410156,
      "learning_rate": 7.536071777273773e-06,
      "loss": 1.2828,
      "step": 6485
    },
    {
      "epoch": 2.2398619499568593,
      "grad_norm": 3.1754140853881836,
      "learning_rate": 7.503768307546436e-06,
      "loss": 1.2132,
      "step": 6490
    },
    {
      "epoch": 2.2415875754961174,
      "grad_norm": 2.8986895084381104,
      "learning_rate": 7.47152199155941e-06,
      "loss": 1.3082,
      "step": 6495
    },
    {
      "epoch": 2.243313201035375,
      "grad_norm": 3.444489002227783,
      "learning_rate": 7.439332934649518e-06,
      "loss": 1.4736,
      "step": 6500
    },
    {
      "epoch": 2.243313201035375,
      "eval_loss": 1.7610725164413452,
      "eval_runtime": 302.6115,
      "eval_samples_per_second": 17.025,
      "eval_steps_per_second": 8.513,
      "step": 6500
    },
    {
      "epoch": 2.2450388265746333,
      "grad_norm": 3.2297439575195312,
      "learning_rate": 7.407201241966555e-06,
      "loss": 1.3964,
      "step": 6505
    },
    {
      "epoch": 2.2467644521138914,
      "grad_norm": 3.5758259296417236,
      "learning_rate": 7.375127018472902e-06,
      "loss": 1.4189,
      "step": 6510
    },
    {
      "epoch": 2.248490077653149,
      "grad_norm": 2.7353947162628174,
      "learning_rate": 7.343110368943238e-06,
      "loss": 1.3561,
      "step": 6515
    },
    {
      "epoch": 2.2502157031924073,
      "grad_norm": 3.1620986461639404,
      "learning_rate": 7.311151397964152e-06,
      "loss": 1.2093,
      "step": 6520
    },
    {
      "epoch": 2.2519413287316654,
      "grad_norm": 2.2921841144561768,
      "learning_rate": 7.279250209933835e-06,
      "loss": 1.3462,
      "step": 6525
    },
    {
      "epoch": 2.253666954270923,
      "grad_norm": 2.525920867919922,
      "learning_rate": 7.2474069090616935e-06,
      "loss": 1.2976,
      "step": 6530
    },
    {
      "epoch": 2.2553925798101813,
      "grad_norm": 3.4152631759643555,
      "learning_rate": 7.215621599368066e-06,
      "loss": 1.3102,
      "step": 6535
    },
    {
      "epoch": 2.257118205349439,
      "grad_norm": 2.9213640689849854,
      "learning_rate": 7.183894384683834e-06,
      "loss": 1.4505,
      "step": 6540
    },
    {
      "epoch": 2.258843830888697,
      "grad_norm": 3.9083523750305176,
      "learning_rate": 7.15222536865012e-06,
      "loss": 1.4601,
      "step": 6545
    },
    {
      "epoch": 2.2605694564279553,
      "grad_norm": 3.009716510772705,
      "learning_rate": 7.120614654717925e-06,
      "loss": 1.3867,
      "step": 6550
    },
    {
      "epoch": 2.262295081967213,
      "grad_norm": 2.9255857467651367,
      "learning_rate": 7.089062346147804e-06,
      "loss": 1.333,
      "step": 6555
    },
    {
      "epoch": 2.264020707506471,
      "grad_norm": 3.1315758228302,
      "learning_rate": 7.057568546009527e-06,
      "loss": 1.2876,
      "step": 6560
    },
    {
      "epoch": 2.2657463330457293,
      "grad_norm": 2.8057031631469727,
      "learning_rate": 7.026133357181713e-06,
      "loss": 1.3404,
      "step": 6565
    },
    {
      "epoch": 2.267471958584987,
      "grad_norm": 2.477199077606201,
      "learning_rate": 6.994756882351558e-06,
      "loss": 1.1199,
      "step": 6570
    },
    {
      "epoch": 2.269197584124245,
      "grad_norm": 3.0173256397247314,
      "learning_rate": 6.96343922401442e-06,
      "loss": 1.3772,
      "step": 6575
    },
    {
      "epoch": 2.2709232096635033,
      "grad_norm": 3.4539804458618164,
      "learning_rate": 6.932180484473564e-06,
      "loss": 1.364,
      "step": 6580
    },
    {
      "epoch": 2.272648835202761,
      "grad_norm": 2.583418607711792,
      "learning_rate": 6.9009807658397545e-06,
      "loss": 1.2207,
      "step": 6585
    },
    {
      "epoch": 2.274374460742019,
      "grad_norm": 2.4050259590148926,
      "learning_rate": 6.869840170030983e-06,
      "loss": 1.3443,
      "step": 6590
    },
    {
      "epoch": 2.276100086281277,
      "grad_norm": 3.057166337966919,
      "learning_rate": 6.838758798772096e-06,
      "loss": 1.3035,
      "step": 6595
    },
    {
      "epoch": 2.277825711820535,
      "grad_norm": 2.658280611038208,
      "learning_rate": 6.807736753594479e-06,
      "loss": 1.3391,
      "step": 6600
    },
    {
      "epoch": 2.277825711820535,
      "eval_loss": 1.7592670917510986,
      "eval_runtime": 302.6385,
      "eval_samples_per_second": 17.024,
      "eval_steps_per_second": 8.512,
      "step": 6600
    },
    {
      "epoch": 2.279551337359793,
      "grad_norm": 2.9020533561706543,
      "learning_rate": 6.776774135835726e-06,
      "loss": 1.2669,
      "step": 6605
    },
    {
      "epoch": 2.281276962899051,
      "grad_norm": 2.907957077026367,
      "learning_rate": 6.745871046639285e-06,
      "loss": 1.2784,
      "step": 6610
    },
    {
      "epoch": 2.283002588438309,
      "grad_norm": 2.7620790004730225,
      "learning_rate": 6.71502758695417e-06,
      "loss": 1.5057,
      "step": 6615
    },
    {
      "epoch": 2.2847282139775666,
      "grad_norm": 3.071145534515381,
      "learning_rate": 6.684243857534581e-06,
      "loss": 1.2924,
      "step": 6620
    },
    {
      "epoch": 2.286453839516825,
      "grad_norm": 2.539260149002075,
      "learning_rate": 6.653519958939633e-06,
      "loss": 1.1713,
      "step": 6625
    },
    {
      "epoch": 2.288179465056083,
      "grad_norm": 3.2862389087677,
      "learning_rate": 6.622855991532964e-06,
      "loss": 1.3296,
      "step": 6630
    },
    {
      "epoch": 2.2899050905953406,
      "grad_norm": 3.0245580673217773,
      "learning_rate": 6.592252055482459e-06,
      "loss": 1.3248,
      "step": 6635
    },
    {
      "epoch": 2.291630716134599,
      "grad_norm": 3.4618029594421387,
      "learning_rate": 6.561708250759899e-06,
      "loss": 1.3718,
      "step": 6640
    },
    {
      "epoch": 2.293356341673857,
      "grad_norm": 2.847799777984619,
      "learning_rate": 6.53122467714064e-06,
      "loss": 1.4218,
      "step": 6645
    },
    {
      "epoch": 2.2950819672131146,
      "grad_norm": 2.7515475749969482,
      "learning_rate": 6.500801434203274e-06,
      "loss": 1.1745,
      "step": 6650
    },
    {
      "epoch": 2.296807592752373,
      "grad_norm": 2.7085564136505127,
      "learning_rate": 6.470438621329331e-06,
      "loss": 1.3761,
      "step": 6655
    },
    {
      "epoch": 2.298533218291631,
      "grad_norm": 2.8055412769317627,
      "learning_rate": 6.4401363377029175e-06,
      "loss": 1.2593,
      "step": 6660
    },
    {
      "epoch": 2.3002588438308886,
      "grad_norm": 3.9160890579223633,
      "learning_rate": 6.409894682310433e-06,
      "loss": 1.3751,
      "step": 6665
    },
    {
      "epoch": 2.301984469370147,
      "grad_norm": 2.3594248294830322,
      "learning_rate": 6.37971375394022e-06,
      "loss": 1.148,
      "step": 6670
    },
    {
      "epoch": 2.3037100949094045,
      "grad_norm": 3.2951877117156982,
      "learning_rate": 6.349593651182245e-06,
      "loss": 1.2846,
      "step": 6675
    },
    {
      "epoch": 2.3054357204486626,
      "grad_norm": 2.6915416717529297,
      "learning_rate": 6.3195344724277905e-06,
      "loss": 1.3336,
      "step": 6680
    },
    {
      "epoch": 2.3071613459879208,
      "grad_norm": 2.257741689682007,
      "learning_rate": 6.289536315869102e-06,
      "loss": 1.3786,
      "step": 6685
    },
    {
      "epoch": 2.3088869715271785,
      "grad_norm": 2.8810107707977295,
      "learning_rate": 6.259599279499115e-06,
      "loss": 1.2793,
      "step": 6690
    },
    {
      "epoch": 2.3106125970664366,
      "grad_norm": 2.746114492416382,
      "learning_rate": 6.229723461111076e-06,
      "loss": 1.3445,
      "step": 6695
    },
    {
      "epoch": 2.3123382226056943,
      "grad_norm": 2.7155401706695557,
      "learning_rate": 6.199908958298286e-06,
      "loss": 1.2583,
      "step": 6700
    },
    {
      "epoch": 2.3123382226056943,
      "eval_loss": 1.7568755149841309,
      "eval_runtime": 302.5478,
      "eval_samples_per_second": 17.029,
      "eval_steps_per_second": 8.514,
      "step": 6700
    },
    {
      "epoch": 2.3140638481449525,
      "grad_norm": 3.168567419052124,
      "learning_rate": 6.170155868453728e-06,
      "loss": 1.4123,
      "step": 6705
    },
    {
      "epoch": 2.3157894736842106,
      "grad_norm": 3.0435850620269775,
      "learning_rate": 6.140464288769782e-06,
      "loss": 1.1382,
      "step": 6710
    },
    {
      "epoch": 2.3175150992234683,
      "grad_norm": 3.220287322998047,
      "learning_rate": 6.110834316237898e-06,
      "loss": 1.2608,
      "step": 6715
    },
    {
      "epoch": 2.3192407247627265,
      "grad_norm": 3.3617639541625977,
      "learning_rate": 6.0812660476482765e-06,
      "loss": 1.4207,
      "step": 6720
    },
    {
      "epoch": 2.3209663503019846,
      "grad_norm": 3.205530881881714,
      "learning_rate": 6.051759579589558e-06,
      "loss": 1.2987,
      "step": 6725
    },
    {
      "epoch": 2.3226919758412423,
      "grad_norm": 3.047154188156128,
      "learning_rate": 6.022315008448484e-06,
      "loss": 1.199,
      "step": 6730
    },
    {
      "epoch": 2.3244176013805005,
      "grad_norm": 2.9677634239196777,
      "learning_rate": 5.992932430409631e-06,
      "loss": 1.2921,
      "step": 6735
    },
    {
      "epoch": 2.3261432269197586,
      "grad_norm": 2.794769525527954,
      "learning_rate": 5.963611941455039e-06,
      "loss": 1.2968,
      "step": 6740
    },
    {
      "epoch": 2.3278688524590163,
      "grad_norm": 2.900275230407715,
      "learning_rate": 5.934353637363949e-06,
      "loss": 1.2634,
      "step": 6745
    },
    {
      "epoch": 2.3295944779982745,
      "grad_norm": 2.9639644622802734,
      "learning_rate": 5.9051576137124395e-06,
      "loss": 1.1659,
      "step": 6750
    },
    {
      "epoch": 2.331320103537532,
      "grad_norm": 2.8806235790252686,
      "learning_rate": 5.876023965873184e-06,
      "loss": 1.3485,
      "step": 6755
    },
    {
      "epoch": 2.3330457290767903,
      "grad_norm": 3.523508310317993,
      "learning_rate": 5.846952789015053e-06,
      "loss": 1.3643,
      "step": 6760
    },
    {
      "epoch": 2.3347713546160485,
      "grad_norm": 2.9036762714385986,
      "learning_rate": 5.8179441781028795e-06,
      "loss": 1.3301,
      "step": 6765
    },
    {
      "epoch": 2.336496980155306,
      "grad_norm": 2.8591535091400146,
      "learning_rate": 5.788998227897091e-06,
      "loss": 1.2424,
      "step": 6770
    },
    {
      "epoch": 2.3382226056945643,
      "grad_norm": 2.8326377868652344,
      "learning_rate": 5.760115032953453e-06,
      "loss": 1.3927,
      "step": 6775
    },
    {
      "epoch": 2.3399482312338225,
      "grad_norm": 3.483572006225586,
      "learning_rate": 5.731294687622698e-06,
      "loss": 1.2235,
      "step": 6780
    },
    {
      "epoch": 2.34167385677308,
      "grad_norm": 2.413844108581543,
      "learning_rate": 5.702537286050286e-06,
      "loss": 1.3006,
      "step": 6785
    },
    {
      "epoch": 2.3433994823123383,
      "grad_norm": 2.8375496864318848,
      "learning_rate": 5.673842922176045e-06,
      "loss": 1.2445,
      "step": 6790
    },
    {
      "epoch": 2.3451251078515964,
      "grad_norm": 3.127248525619507,
      "learning_rate": 5.645211689733884e-06,
      "loss": 1.3485,
      "step": 6795
    },
    {
      "epoch": 2.346850733390854,
      "grad_norm": 2.7990612983703613,
      "learning_rate": 5.616643682251499e-06,
      "loss": 1.3043,
      "step": 6800
    },
    {
      "epoch": 2.346850733390854,
      "eval_loss": 1.7578045129776,
      "eval_runtime": 302.5241,
      "eval_samples_per_second": 17.03,
      "eval_steps_per_second": 8.515,
      "step": 6800
    },
    {
      "epoch": 2.3485763589301123,
      "grad_norm": 3.505307912826538,
      "learning_rate": 5.5881389930500234e-06,
      "loss": 1.2405,
      "step": 6805
    },
    {
      "epoch": 2.35030198446937,
      "grad_norm": 3.087031126022339,
      "learning_rate": 5.559697715243786e-06,
      "loss": 1.3088,
      "step": 6810
    },
    {
      "epoch": 2.352027610008628,
      "grad_norm": 3.711280107498169,
      "learning_rate": 5.531319941739943e-06,
      "loss": 1.5762,
      "step": 6815
    },
    {
      "epoch": 2.3537532355478863,
      "grad_norm": 2.815025806427002,
      "learning_rate": 5.503005765238237e-06,
      "loss": 1.2801,
      "step": 6820
    },
    {
      "epoch": 2.355478861087144,
      "grad_norm": 2.990159511566162,
      "learning_rate": 5.474755278230631e-06,
      "loss": 1.5134,
      "step": 6825
    },
    {
      "epoch": 2.357204486626402,
      "grad_norm": 2.6352221965789795,
      "learning_rate": 5.446568573001062e-06,
      "loss": 1.3006,
      "step": 6830
    },
    {
      "epoch": 2.35893011216566,
      "grad_norm": 3.5249013900756836,
      "learning_rate": 5.418445741625108e-06,
      "loss": 1.1904,
      "step": 6835
    },
    {
      "epoch": 2.360655737704918,
      "grad_norm": 2.952860116958618,
      "learning_rate": 5.390386875969694e-06,
      "loss": 1.371,
      "step": 6840
    },
    {
      "epoch": 2.362381363244176,
      "grad_norm": 2.9059512615203857,
      "learning_rate": 5.362392067692798e-06,
      "loss": 1.3286,
      "step": 6845
    },
    {
      "epoch": 2.364106988783434,
      "grad_norm": 3.05464243888855,
      "learning_rate": 5.334461408243124e-06,
      "loss": 1.4042,
      "step": 6850
    },
    {
      "epoch": 2.365832614322692,
      "grad_norm": 2.4021999835968018,
      "learning_rate": 5.3065949888598614e-06,
      "loss": 1.1674,
      "step": 6855
    },
    {
      "epoch": 2.36755823986195,
      "grad_norm": 2.888028383255005,
      "learning_rate": 5.278792900572313e-06,
      "loss": 1.3422,
      "step": 6860
    },
    {
      "epoch": 2.369283865401208,
      "grad_norm": 2.760518789291382,
      "learning_rate": 5.251055234199665e-06,
      "loss": 1.3124,
      "step": 6865
    },
    {
      "epoch": 2.371009490940466,
      "grad_norm": 2.4799728393554688,
      "learning_rate": 5.223382080350647e-06,
      "loss": 1.2141,
      "step": 6870
    },
    {
      "epoch": 2.372735116479724,
      "grad_norm": 3.145599603652954,
      "learning_rate": 5.195773529423262e-06,
      "loss": 1.324,
      "step": 6875
    },
    {
      "epoch": 2.374460742018982,
      "grad_norm": 2.77303409576416,
      "learning_rate": 5.168229671604455e-06,
      "loss": 1.2583,
      "step": 6880
    },
    {
      "epoch": 2.37618636755824,
      "grad_norm": 2.685598134994507,
      "learning_rate": 5.14075059686987e-06,
      "loss": 1.2732,
      "step": 6885
    },
    {
      "epoch": 2.3779119930974977,
      "grad_norm": 2.7315115928649902,
      "learning_rate": 5.113336394983506e-06,
      "loss": 1.384,
      "step": 6890
    },
    {
      "epoch": 2.379637618636756,
      "grad_norm": 3.5448591709136963,
      "learning_rate": 5.085987155497465e-06,
      "loss": 1.4746,
      "step": 6895
    },
    {
      "epoch": 2.381363244176014,
      "grad_norm": 2.4567630290985107,
      "learning_rate": 5.058702967751625e-06,
      "loss": 1.461,
      "step": 6900
    },
    {
      "epoch": 2.381363244176014,
      "eval_loss": 1.7541141510009766,
      "eval_runtime": 302.4303,
      "eval_samples_per_second": 17.035,
      "eval_steps_per_second": 8.518,
      "step": 6900
    },
    {
      "epoch": 2.3830888697152717,
      "grad_norm": 2.9483721256256104,
      "learning_rate": 5.031483920873376e-06,
      "loss": 1.37,
      "step": 6905
    },
    {
      "epoch": 2.38481449525453,
      "grad_norm": 2.703596830368042,
      "learning_rate": 5.004330103777308e-06,
      "loss": 1.1984,
      "step": 6910
    },
    {
      "epoch": 2.3865401207937875,
      "grad_norm": 3.3558242321014404,
      "learning_rate": 4.977241605164936e-06,
      "loss": 1.2762,
      "step": 6915
    },
    {
      "epoch": 2.3882657463330457,
      "grad_norm": 3.1067769527435303,
      "learning_rate": 4.95021851352441e-06,
      "loss": 1.2244,
      "step": 6920
    },
    {
      "epoch": 2.389991371872304,
      "grad_norm": 4.105026721954346,
      "learning_rate": 4.923260917130193e-06,
      "loss": 1.4067,
      "step": 6925
    },
    {
      "epoch": 2.3917169974115615,
      "grad_norm": 2.855753183364868,
      "learning_rate": 4.89636890404283e-06,
      "loss": 1.3016,
      "step": 6930
    },
    {
      "epoch": 2.3934426229508197,
      "grad_norm": 3.1800246238708496,
      "learning_rate": 4.869542562108609e-06,
      "loss": 1.3034,
      "step": 6935
    },
    {
      "epoch": 2.395168248490078,
      "grad_norm": 2.5592899322509766,
      "learning_rate": 4.842781978959307e-06,
      "loss": 1.0572,
      "step": 6940
    },
    {
      "epoch": 2.3968938740293355,
      "grad_norm": 2.555344820022583,
      "learning_rate": 4.816087242011877e-06,
      "loss": 1.2646,
      "step": 6945
    },
    {
      "epoch": 2.3986194995685937,
      "grad_norm": 2.6591153144836426,
      "learning_rate": 4.789458438468189e-06,
      "loss": 1.3061,
      "step": 6950
    },
    {
      "epoch": 2.400345125107852,
      "grad_norm": 2.6830294132232666,
      "learning_rate": 4.7628956553147335e-06,
      "loss": 1.2054,
      "step": 6955
    },
    {
      "epoch": 2.4020707506471095,
      "grad_norm": 3.1885204315185547,
      "learning_rate": 4.7363989793223315e-06,
      "loss": 1.3846,
      "step": 6960
    },
    {
      "epoch": 2.4037963761863677,
      "grad_norm": 2.9597859382629395,
      "learning_rate": 4.709968497045864e-06,
      "loss": 1.2664,
      "step": 6965
    },
    {
      "epoch": 2.4055220017256254,
      "grad_norm": 2.7988436222076416,
      "learning_rate": 4.683604294823962e-06,
      "loss": 1.4469,
      "step": 6970
    },
    {
      "epoch": 2.4072476272648835,
      "grad_norm": 3.197232484817505,
      "learning_rate": 4.657306458778768e-06,
      "loss": 1.3975,
      "step": 6975
    },
    {
      "epoch": 2.4089732528041417,
      "grad_norm": 2.924294948577881,
      "learning_rate": 4.631075074815608e-06,
      "loss": 1.2728,
      "step": 6980
    },
    {
      "epoch": 2.4106988783433994,
      "grad_norm": 3.3523831367492676,
      "learning_rate": 4.6049102286227556e-06,
      "loss": 1.3324,
      "step": 6985
    },
    {
      "epoch": 2.4124245038826575,
      "grad_norm": 2.5897819995880127,
      "learning_rate": 4.578812005671112e-06,
      "loss": 1.2067,
      "step": 6990
    },
    {
      "epoch": 2.414150129421915,
      "grad_norm": 2.7717299461364746,
      "learning_rate": 4.552780491213965e-06,
      "loss": 1.2851,
      "step": 6995
    },
    {
      "epoch": 2.4158757549611733,
      "grad_norm": 3.2045705318450928,
      "learning_rate": 4.526815770286663e-06,
      "loss": 1.2073,
      "step": 7000
    },
    {
      "epoch": 2.4158757549611733,
      "eval_loss": 1.7557440996170044,
      "eval_runtime": 302.4137,
      "eval_samples_per_second": 17.036,
      "eval_steps_per_second": 8.518,
      "step": 7000
    },
    {
      "epoch": 2.4176013805004315,
      "grad_norm": 2.6167423725128174,
      "learning_rate": 4.500917927706394e-06,
      "loss": 1.1849,
      "step": 7005
    },
    {
      "epoch": 2.419327006039689,
      "grad_norm": 2.9466798305511475,
      "learning_rate": 4.475087048071846e-06,
      "loss": 1.2176,
      "step": 7010
    },
    {
      "epoch": 2.4210526315789473,
      "grad_norm": 2.7506370544433594,
      "learning_rate": 4.449323215762996e-06,
      "loss": 1.4074,
      "step": 7015
    },
    {
      "epoch": 2.4227782571182055,
      "grad_norm": 3.067155122756958,
      "learning_rate": 4.42362651494079e-06,
      "loss": 1.4531,
      "step": 7020
    },
    {
      "epoch": 2.424503882657463,
      "grad_norm": 2.4499638080596924,
      "learning_rate": 4.397997029546866e-06,
      "loss": 1.3547,
      "step": 7025
    },
    {
      "epoch": 2.4262295081967213,
      "grad_norm": 3.0223331451416016,
      "learning_rate": 4.372434843303317e-06,
      "loss": 1.2919,
      "step": 7030
    },
    {
      "epoch": 2.4279551337359795,
      "grad_norm": 2.7845022678375244,
      "learning_rate": 4.3469400397123775e-06,
      "loss": 1.3315,
      "step": 7035
    },
    {
      "epoch": 2.429680759275237,
      "grad_norm": 2.83892822265625,
      "learning_rate": 4.321512702056185e-06,
      "loss": 1.2349,
      "step": 7040
    },
    {
      "epoch": 2.4314063848144953,
      "grad_norm": 3.1167564392089844,
      "learning_rate": 4.2961529133964676e-06,
      "loss": 1.3976,
      "step": 7045
    },
    {
      "epoch": 2.433132010353753,
      "grad_norm": 3.6453473567962646,
      "learning_rate": 4.270860756574316e-06,
      "loss": 1.4185,
      "step": 7050
    },
    {
      "epoch": 2.434857635893011,
      "grad_norm": 2.8848118782043457,
      "learning_rate": 4.245636314209875e-06,
      "loss": 1.254,
      "step": 7055
    },
    {
      "epoch": 2.4365832614322693,
      "grad_norm": 3.330953598022461,
      "learning_rate": 4.220479668702115e-06,
      "loss": 1.3762,
      "step": 7060
    },
    {
      "epoch": 2.438308886971527,
      "grad_norm": 3.9912991523742676,
      "learning_rate": 4.195390902228511e-06,
      "loss": 1.3232,
      "step": 7065
    },
    {
      "epoch": 2.440034512510785,
      "grad_norm": 3.19734263420105,
      "learning_rate": 4.170370096744819e-06,
      "loss": 1.3086,
      "step": 7070
    },
    {
      "epoch": 2.4417601380500433,
      "grad_norm": 3.512059211730957,
      "learning_rate": 4.145417333984791e-06,
      "loss": 1.3966,
      "step": 7075
    },
    {
      "epoch": 2.443485763589301,
      "grad_norm": 2.4025487899780273,
      "learning_rate": 4.120532695459908e-06,
      "loss": 1.1404,
      "step": 7080
    },
    {
      "epoch": 2.445211389128559,
      "grad_norm": 2.8000619411468506,
      "learning_rate": 4.095716262459118e-06,
      "loss": 1.1944,
      "step": 7085
    },
    {
      "epoch": 2.4469370146678173,
      "grad_norm": 3.431306838989258,
      "learning_rate": 4.070968116048546e-06,
      "loss": 1.3549,
      "step": 7090
    },
    {
      "epoch": 2.448662640207075,
      "grad_norm": 3.7570319175720215,
      "learning_rate": 4.046288337071277e-06,
      "loss": 1.167,
      "step": 7095
    },
    {
      "epoch": 2.450388265746333,
      "grad_norm": 2.361166000366211,
      "learning_rate": 4.0216770061470455e-06,
      "loss": 1.2022,
      "step": 7100
    },
    {
      "epoch": 2.450388265746333,
      "eval_loss": 1.7539026737213135,
      "eval_runtime": 302.6244,
      "eval_samples_per_second": 17.024,
      "eval_steps_per_second": 8.512,
      "step": 7100
    },
    {
      "epoch": 2.452113891285591,
      "grad_norm": 2.4854576587677,
      "learning_rate": 3.9971342036719986e-06,
      "loss": 1.2889,
      "step": 7105
    },
    {
      "epoch": 2.453839516824849,
      "grad_norm": 2.196861743927002,
      "learning_rate": 3.97266000981843e-06,
      "loss": 1.0258,
      "step": 7110
    },
    {
      "epoch": 2.455565142364107,
      "grad_norm": 3.4627089500427246,
      "learning_rate": 3.948254504534513e-06,
      "loss": 1.3044,
      "step": 7115
    },
    {
      "epoch": 2.457290767903365,
      "grad_norm": 2.7826995849609375,
      "learning_rate": 3.92391776754403e-06,
      "loss": 1.2853,
      "step": 7120
    },
    {
      "epoch": 2.459016393442623,
      "grad_norm": 2.5215437412261963,
      "learning_rate": 3.89964987834614e-06,
      "loss": 1.3218,
      "step": 7125
    },
    {
      "epoch": 2.4607420189818807,
      "grad_norm": 3.185840606689453,
      "learning_rate": 3.875450916215082e-06,
      "loss": 1.0975,
      "step": 7130
    },
    {
      "epoch": 2.462467644521139,
      "grad_norm": 2.7256929874420166,
      "learning_rate": 3.851320960199953e-06,
      "loss": 1.2768,
      "step": 7135
    },
    {
      "epoch": 2.464193270060397,
      "grad_norm": 3.4965391159057617,
      "learning_rate": 3.827260089124429e-06,
      "loss": 1.3364,
      "step": 7140
    },
    {
      "epoch": 2.4659188955996547,
      "grad_norm": 3.327136754989624,
      "learning_rate": 3.8032683815865e-06,
      "loss": 1.3947,
      "step": 7145
    },
    {
      "epoch": 2.467644521138913,
      "grad_norm": 2.9006848335266113,
      "learning_rate": 3.7793459159582325e-06,
      "loss": 1.4522,
      "step": 7150
    },
    {
      "epoch": 2.469370146678171,
      "grad_norm": 2.227642774581909,
      "learning_rate": 3.7554927703855077e-06,
      "loss": 1.1788,
      "step": 7155
    },
    {
      "epoch": 2.4710957722174287,
      "grad_norm": 3.074267864227295,
      "learning_rate": 3.731709022787766e-06,
      "loss": 1.2568,
      "step": 7160
    },
    {
      "epoch": 2.472821397756687,
      "grad_norm": 2.752800226211548,
      "learning_rate": 3.7079947508577324e-06,
      "loss": 1.3532,
      "step": 7165
    },
    {
      "epoch": 2.474547023295945,
      "grad_norm": 3.0866730213165283,
      "learning_rate": 3.684350032061207e-06,
      "loss": 1.3308,
      "step": 7170
    },
    {
      "epoch": 2.4762726488352027,
      "grad_norm": 2.926483154296875,
      "learning_rate": 3.660774943636755e-06,
      "loss": 1.3422,
      "step": 7175
    },
    {
      "epoch": 2.477998274374461,
      "grad_norm": 2.420468330383301,
      "learning_rate": 3.6372695625955105e-06,
      "loss": 1.2079,
      "step": 7180
    },
    {
      "epoch": 2.4797238999137186,
      "grad_norm": 3.175722122192383,
      "learning_rate": 3.6138339657208893e-06,
      "loss": 1.4211,
      "step": 7185
    },
    {
      "epoch": 2.4814495254529767,
      "grad_norm": 2.6822574138641357,
      "learning_rate": 3.5904682295683557e-06,
      "loss": 1.2888,
      "step": 7190
    },
    {
      "epoch": 2.483175150992235,
      "grad_norm": 3.2723264694213867,
      "learning_rate": 3.567172430465143e-06,
      "loss": 1.4087,
      "step": 7195
    },
    {
      "epoch": 2.4849007765314925,
      "grad_norm": 3.3419175148010254,
      "learning_rate": 3.543946644510049e-06,
      "loss": 1.5023,
      "step": 7200
    },
    {
      "epoch": 2.4849007765314925,
      "eval_loss": 1.7515358924865723,
      "eval_runtime": 302.6443,
      "eval_samples_per_second": 17.023,
      "eval_steps_per_second": 8.512,
      "step": 7200
    },
    {
      "epoch": 2.4866264020707507,
      "grad_norm": 3.195610761642456,
      "learning_rate": 3.52079094757316e-06,
      "loss": 1.3555,
      "step": 7205
    },
    {
      "epoch": 2.4883520276100084,
      "grad_norm": 3.2312426567077637,
      "learning_rate": 3.497705415295596e-06,
      "loss": 1.2951,
      "step": 7210
    },
    {
      "epoch": 2.4900776531492665,
      "grad_norm": 2.976147413253784,
      "learning_rate": 3.474690123089286e-06,
      "loss": 1.292,
      "step": 7215
    },
    {
      "epoch": 2.4918032786885247,
      "grad_norm": 3.1007347106933594,
      "learning_rate": 3.4517451461367035e-06,
      "loss": 1.3826,
      "step": 7220
    },
    {
      "epoch": 2.4935289042277824,
      "grad_norm": 3.0486738681793213,
      "learning_rate": 3.4288705593906316e-06,
      "loss": 1.3728,
      "step": 7225
    },
    {
      "epoch": 2.4952545297670405,
      "grad_norm": 3.337486743927002,
      "learning_rate": 3.406066437573918e-06,
      "loss": 1.3133,
      "step": 7230
    },
    {
      "epoch": 2.4969801553062987,
      "grad_norm": 3.1573309898376465,
      "learning_rate": 3.3833328551792247e-06,
      "loss": 1.3152,
      "step": 7235
    },
    {
      "epoch": 2.4987057808455564,
      "grad_norm": 3.0520973205566406,
      "learning_rate": 3.360669886468781e-06,
      "loss": 1.3593,
      "step": 7240
    },
    {
      "epoch": 2.5004314063848145,
      "grad_norm": 3.4809165000915527,
      "learning_rate": 3.3380776054741574e-06,
      "loss": 1.3767,
      "step": 7245
    },
    {
      "epoch": 2.5021570319240727,
      "grad_norm": 3.0355281829833984,
      "learning_rate": 3.3155560859960048e-06,
      "loss": 1.2439,
      "step": 7250
    },
    {
      "epoch": 2.5038826574633304,
      "grad_norm": 3.0720505714416504,
      "learning_rate": 3.293105401603827e-06,
      "loss": 1.5021,
      "step": 7255
    },
    {
      "epoch": 2.5056082830025885,
      "grad_norm": 3.6083626747131348,
      "learning_rate": 3.2707256256357427e-06,
      "loss": 1.4072,
      "step": 7260
    },
    {
      "epoch": 2.5073339085418462,
      "grad_norm": 3.856178045272827,
      "learning_rate": 3.248416831198217e-06,
      "loss": 1.2523,
      "step": 7265
    },
    {
      "epoch": 2.5090595340811044,
      "grad_norm": 2.7580270767211914,
      "learning_rate": 3.226179091165871e-06,
      "loss": 1.2754,
      "step": 7270
    },
    {
      "epoch": 2.5107851596203625,
      "grad_norm": 3.270756959915161,
      "learning_rate": 3.204012478181201e-06,
      "loss": 1.2516,
      "step": 7275
    },
    {
      "epoch": 2.5125107851596202,
      "grad_norm": 2.7376794815063477,
      "learning_rate": 3.181917064654369e-06,
      "loss": 1.2844,
      "step": 7280
    },
    {
      "epoch": 2.5142364106988784,
      "grad_norm": 2.9652700424194336,
      "learning_rate": 3.159892922762933e-06,
      "loss": 1.347,
      "step": 7285
    },
    {
      "epoch": 2.515962036238136,
      "grad_norm": 2.779803991317749,
      "learning_rate": 3.1379401244516603e-06,
      "loss": 1.351,
      "step": 7290
    },
    {
      "epoch": 2.5176876617773942,
      "grad_norm": 3.236680269241333,
      "learning_rate": 3.11605874143224e-06,
      "loss": 1.3914,
      "step": 7295
    },
    {
      "epoch": 2.5194132873166524,
      "grad_norm": 2.520463705062866,
      "learning_rate": 3.0942488451830925e-06,
      "loss": 1.4705,
      "step": 7300
    },
    {
      "epoch": 2.5194132873166524,
      "eval_loss": 1.7501091957092285,
      "eval_runtime": 302.7474,
      "eval_samples_per_second": 17.017,
      "eval_steps_per_second": 8.509,
      "step": 7300
    },
    {
      "epoch": 2.5211389128559105,
      "grad_norm": 2.8045527935028076,
      "learning_rate": 3.0725105069491085e-06,
      "loss": 1.3202,
      "step": 7305
    },
    {
      "epoch": 2.522864538395168,
      "grad_norm": 2.4359676837921143,
      "learning_rate": 3.0508437977414346e-06,
      "loss": 1.314,
      "step": 7310
    },
    {
      "epoch": 2.5245901639344264,
      "grad_norm": 2.974846363067627,
      "learning_rate": 3.0292487883372107e-06,
      "loss": 1.3771,
      "step": 7315
    },
    {
      "epoch": 2.526315789473684,
      "grad_norm": 2.969569683074951,
      "learning_rate": 3.0077255492793805e-06,
      "loss": 1.2347,
      "step": 7320
    },
    {
      "epoch": 2.528041415012942,
      "grad_norm": 2.4283626079559326,
      "learning_rate": 2.9862741508764366e-06,
      "loss": 1.3625,
      "step": 7325
    },
    {
      "epoch": 2.5297670405522004,
      "grad_norm": 3.7893779277801514,
      "learning_rate": 2.964894663202181e-06,
      "loss": 1.3401,
      "step": 7330
    },
    {
      "epoch": 2.531492666091458,
      "grad_norm": 3.0695033073425293,
      "learning_rate": 2.9435871560955314e-06,
      "loss": 1.2435,
      "step": 7335
    },
    {
      "epoch": 2.533218291630716,
      "grad_norm": 3.2645294666290283,
      "learning_rate": 2.92235169916025e-06,
      "loss": 1.4242,
      "step": 7340
    },
    {
      "epoch": 2.534943917169974,
      "grad_norm": 3.023790121078491,
      "learning_rate": 2.9011883617647533e-06,
      "loss": 1.3189,
      "step": 7345
    },
    {
      "epoch": 2.536669542709232,
      "grad_norm": 3.28462553024292,
      "learning_rate": 2.880097213041863e-06,
      "loss": 1.2596,
      "step": 7350
    },
    {
      "epoch": 2.53839516824849,
      "grad_norm": 3.0497729778289795,
      "learning_rate": 2.859078321888592e-06,
      "loss": 1.3655,
      "step": 7355
    },
    {
      "epoch": 2.540120793787748,
      "grad_norm": 2.8129255771636963,
      "learning_rate": 2.838131756965906e-06,
      "loss": 1.2551,
      "step": 7360
    },
    {
      "epoch": 2.541846419327006,
      "grad_norm": 2.535165548324585,
      "learning_rate": 2.8172575866985157e-06,
      "loss": 1.3895,
      "step": 7365
    },
    {
      "epoch": 2.5435720448662638,
      "grad_norm": 2.981027364730835,
      "learning_rate": 2.796455879274637e-06,
      "loss": 1.3327,
      "step": 7370
    },
    {
      "epoch": 2.545297670405522,
      "grad_norm": 3.4672014713287354,
      "learning_rate": 2.775726702645784e-06,
      "loss": 1.4037,
      "step": 7375
    },
    {
      "epoch": 2.54702329594478,
      "grad_norm": 3.9358131885528564,
      "learning_rate": 2.7550701245265404e-06,
      "loss": 1.2474,
      "step": 7380
    },
    {
      "epoch": 2.548748921484038,
      "grad_norm": 2.988952875137329,
      "learning_rate": 2.7344862123943186e-06,
      "loss": 1.3287,
      "step": 7385
    },
    {
      "epoch": 2.550474547023296,
      "grad_norm": 2.9739463329315186,
      "learning_rate": 2.713975033489197e-06,
      "loss": 1.2376,
      "step": 7390
    },
    {
      "epoch": 2.552200172562554,
      "grad_norm": 3.22648549079895,
      "learning_rate": 2.6935366548136165e-06,
      "loss": 1.3481,
      "step": 7395
    },
    {
      "epoch": 2.5539257981018118,
      "grad_norm": 2.6106183528900146,
      "learning_rate": 2.6731711431322414e-06,
      "loss": 1.2087,
      "step": 7400
    },
    {
      "epoch": 2.5539257981018118,
      "eval_loss": 1.7502509355545044,
      "eval_runtime": 302.3197,
      "eval_samples_per_second": 17.042,
      "eval_steps_per_second": 8.521,
      "step": 7400
    },
    {
      "epoch": 2.55565142364107,
      "grad_norm": 3.1428847312927246,
      "learning_rate": 2.6528785649716783e-06,
      "loss": 1.3721,
      "step": 7405
    },
    {
      "epoch": 2.557377049180328,
      "grad_norm": 3.314298391342163,
      "learning_rate": 2.632658986620315e-06,
      "loss": 1.274,
      "step": 7410
    },
    {
      "epoch": 2.5591026747195857,
      "grad_norm": 3.092146873474121,
      "learning_rate": 2.612512474128048e-06,
      "loss": 1.5671,
      "step": 7415
    },
    {
      "epoch": 2.560828300258844,
      "grad_norm": 3.5354764461517334,
      "learning_rate": 2.5924390933061175e-06,
      "loss": 1.4589,
      "step": 7420
    },
    {
      "epoch": 2.5625539257981016,
      "grad_norm": 3.185094118118286,
      "learning_rate": 2.572438909726857e-06,
      "loss": 1.2765,
      "step": 7425
    },
    {
      "epoch": 2.5642795513373597,
      "grad_norm": 3.079150915145874,
      "learning_rate": 2.5525119887234995e-06,
      "loss": 1.2137,
      "step": 7430
    },
    {
      "epoch": 2.566005176876618,
      "grad_norm": 2.974482297897339,
      "learning_rate": 2.532658395389942e-06,
      "loss": 1.3419,
      "step": 7435
    },
    {
      "epoch": 2.5677308024158756,
      "grad_norm": 3.372725009918213,
      "learning_rate": 2.512878194580559e-06,
      "loss": 1.277,
      "step": 7440
    },
    {
      "epoch": 2.5694564279551337,
      "grad_norm": 2.7342886924743652,
      "learning_rate": 2.4931714509099867e-06,
      "loss": 1.2437,
      "step": 7445
    },
    {
      "epoch": 2.5711820534943914,
      "grad_norm": 3.6042654514312744,
      "learning_rate": 2.473538228752881e-06,
      "loss": 1.5935,
      "step": 7450
    },
    {
      "epoch": 2.5729076790336496,
      "grad_norm": 3.3279306888580322,
      "learning_rate": 2.45397859224375e-06,
      "loss": 1.384,
      "step": 7455
    },
    {
      "epoch": 2.5746333045729077,
      "grad_norm": 2.598956346511841,
      "learning_rate": 2.4344926052767086e-06,
      "loss": 1.3144,
      "step": 7460
    },
    {
      "epoch": 2.576358930112166,
      "grad_norm": 3.259697198867798,
      "learning_rate": 2.4150803315052982e-06,
      "loss": 1.3253,
      "step": 7465
    },
    {
      "epoch": 2.5780845556514236,
      "grad_norm": 3.452051877975464,
      "learning_rate": 2.3957418343422662e-06,
      "loss": 1.3116,
      "step": 7470
    },
    {
      "epoch": 2.5798101811906817,
      "grad_norm": 3.331242561340332,
      "learning_rate": 2.376477176959355e-06,
      "loss": 1.2568,
      "step": 7475
    },
    {
      "epoch": 2.5815358067299394,
      "grad_norm": 2.9418179988861084,
      "learning_rate": 2.3572864222870946e-06,
      "loss": 1.2414,
      "step": 7480
    },
    {
      "epoch": 2.5832614322691976,
      "grad_norm": 2.2199816703796387,
      "learning_rate": 2.3381696330146115e-06,
      "loss": 1.2099,
      "step": 7485
    },
    {
      "epoch": 2.5849870578084557,
      "grad_norm": 3.6190080642700195,
      "learning_rate": 2.3191268715894084e-06,
      "loss": 1.4194,
      "step": 7490
    },
    {
      "epoch": 2.5867126833477134,
      "grad_norm": 3.3060758113861084,
      "learning_rate": 2.3001582002171644e-06,
      "loss": 1.3192,
      "step": 7495
    },
    {
      "epoch": 2.5884383088869716,
      "grad_norm": 3.0672593116760254,
      "learning_rate": 2.281263680861545e-06,
      "loss": 1.3631,
      "step": 7500
    },
    {
      "epoch": 2.5884383088869716,
      "eval_loss": 1.747057557106018,
      "eval_runtime": 302.5904,
      "eval_samples_per_second": 17.026,
      "eval_steps_per_second": 8.513,
      "step": 7500
    },
    {
      "epoch": 2.5901639344262293,
      "grad_norm": 2.788407802581787,
      "learning_rate": 2.2624433752439777e-06,
      "loss": 1.4558,
      "step": 7505
    },
    {
      "epoch": 2.5918895599654874,
      "grad_norm": 2.521437406539917,
      "learning_rate": 2.2436973448434677e-06,
      "loss": 1.283,
      "step": 7510
    },
    {
      "epoch": 2.5936151855047456,
      "grad_norm": 2.7777185440063477,
      "learning_rate": 2.2250256508963815e-06,
      "loss": 1.3136,
      "step": 7515
    },
    {
      "epoch": 2.5953408110440037,
      "grad_norm": 3.140158176422119,
      "learning_rate": 2.2064283543962644e-06,
      "loss": 1.2531,
      "step": 7520
    },
    {
      "epoch": 2.5970664365832614,
      "grad_norm": 3.4183804988861084,
      "learning_rate": 2.1879055160936276e-06,
      "loss": 1.1869,
      "step": 7525
    },
    {
      "epoch": 2.5987920621225196,
      "grad_norm": 2.982311248779297,
      "learning_rate": 2.169457196495761e-06,
      "loss": 1.2697,
      "step": 7530
    },
    {
      "epoch": 2.6005176876617773,
      "grad_norm": 3.373650550842285,
      "learning_rate": 2.1510834558665175e-06,
      "loss": 1.3329,
      "step": 7535
    },
    {
      "epoch": 2.6022433132010354,
      "grad_norm": 3.1898140907287598,
      "learning_rate": 2.1327843542261416e-06,
      "loss": 1.3543,
      "step": 7540
    },
    {
      "epoch": 2.6039689387402936,
      "grad_norm": 3.246615409851074,
      "learning_rate": 2.11455995135105e-06,
      "loss": 1.3322,
      "step": 7545
    },
    {
      "epoch": 2.6056945642795513,
      "grad_norm": 2.8437914848327637,
      "learning_rate": 2.096410306773655e-06,
      "loss": 1.3375,
      "step": 7550
    },
    {
      "epoch": 2.6074201898188094,
      "grad_norm": 3.394460678100586,
      "learning_rate": 2.0783354797821592e-06,
      "loss": 1.3308,
      "step": 7555
    },
    {
      "epoch": 2.609145815358067,
      "grad_norm": 3.3257551193237305,
      "learning_rate": 2.0603355294203456e-06,
      "loss": 1.3834,
      "step": 7560
    },
    {
      "epoch": 2.6108714408973253,
      "grad_norm": 3.348006248474121,
      "learning_rate": 2.042410514487436e-06,
      "loss": 1.4052,
      "step": 7565
    },
    {
      "epoch": 2.6125970664365834,
      "grad_norm": 2.828658103942871,
      "learning_rate": 2.0245604935378306e-06,
      "loss": 1.2497,
      "step": 7570
    },
    {
      "epoch": 2.614322691975841,
      "grad_norm": 2.362816572189331,
      "learning_rate": 2.006785524880983e-06,
      "loss": 1.3302,
      "step": 7575
    },
    {
      "epoch": 2.6160483175150993,
      "grad_norm": 2.489511728286743,
      "learning_rate": 1.989085666581153e-06,
      "loss": 1.1808,
      "step": 7580
    },
    {
      "epoch": 2.617773943054357,
      "grad_norm": 3.1681249141693115,
      "learning_rate": 1.9714609764572593e-06,
      "loss": 1.3244,
      "step": 7585
    },
    {
      "epoch": 2.619499568593615,
      "grad_norm": 3.365527868270874,
      "learning_rate": 1.9539115120826707e-06,
      "loss": 1.3359,
      "step": 7590
    },
    {
      "epoch": 2.6212251941328732,
      "grad_norm": 2.6712238788604736,
      "learning_rate": 1.936437330785018e-06,
      "loss": 1.3269,
      "step": 7595
    },
    {
      "epoch": 2.6229508196721314,
      "grad_norm": 3.1975677013397217,
      "learning_rate": 1.9190384896460053e-06,
      "loss": 1.302,
      "step": 7600
    },
    {
      "epoch": 2.6229508196721314,
      "eval_loss": 1.7482792139053345,
      "eval_runtime": 302.6425,
      "eval_samples_per_second": 17.023,
      "eval_steps_per_second": 8.512,
      "step": 7600
    },
    {
      "epoch": 2.624676445211389,
      "grad_norm": 3.650103807449341,
      "learning_rate": 1.9017150455012427e-06,
      "loss": 1.261,
      "step": 7605
    },
    {
      "epoch": 2.6264020707506472,
      "grad_norm": 3.9529824256896973,
      "learning_rate": 1.884467054940031e-06,
      "loss": 1.2665,
      "step": 7610
    },
    {
      "epoch": 2.628127696289905,
      "grad_norm": 3.4415838718414307,
      "learning_rate": 1.8672945743051977e-06,
      "loss": 1.1896,
      "step": 7615
    },
    {
      "epoch": 2.629853321829163,
      "grad_norm": 3.450982093811035,
      "learning_rate": 1.8501976596929093e-06,
      "loss": 1.1964,
      "step": 7620
    },
    {
      "epoch": 2.6315789473684212,
      "grad_norm": 2.7114129066467285,
      "learning_rate": 1.8331763669524882e-06,
      "loss": 1.2365,
      "step": 7625
    },
    {
      "epoch": 2.633304572907679,
      "grad_norm": 3.35370135307312,
      "learning_rate": 1.8162307516862237e-06,
      "loss": 1.3401,
      "step": 7630
    },
    {
      "epoch": 2.635030198446937,
      "grad_norm": 3.107806444168091,
      "learning_rate": 1.7993608692491864e-06,
      "loss": 1.1834,
      "step": 7635
    },
    {
      "epoch": 2.636755823986195,
      "grad_norm": 2.920462131500244,
      "learning_rate": 1.782566774749078e-06,
      "loss": 1.2451,
      "step": 7640
    },
    {
      "epoch": 2.638481449525453,
      "grad_norm": 3.2916157245635986,
      "learning_rate": 1.7658485230460016e-06,
      "loss": 1.3463,
      "step": 7645
    },
    {
      "epoch": 2.640207075064711,
      "grad_norm": 3.093376874923706,
      "learning_rate": 1.7492061687523347e-06,
      "loss": 1.3697,
      "step": 7650
    },
    {
      "epoch": 2.641932700603969,
      "grad_norm": 3.8694136142730713,
      "learning_rate": 1.7326397662325078e-06,
      "loss": 1.1409,
      "step": 7655
    },
    {
      "epoch": 2.643658326143227,
      "grad_norm": 3.0264124870300293,
      "learning_rate": 1.7161493696028541e-06,
      "loss": 1.2436,
      "step": 7660
    },
    {
      "epoch": 2.6453839516824846,
      "grad_norm": 2.766178846359253,
      "learning_rate": 1.69973503273142e-06,
      "loss": 1.2543,
      "step": 7665
    },
    {
      "epoch": 2.647109577221743,
      "grad_norm": 3.0268542766571045,
      "learning_rate": 1.6833968092377971e-06,
      "loss": 1.3228,
      "step": 7670
    },
    {
      "epoch": 2.648835202761001,
      "grad_norm": 2.7456109523773193,
      "learning_rate": 1.6671347524929437e-06,
      "loss": 1.346,
      "step": 7675
    },
    {
      "epoch": 2.650560828300259,
      "grad_norm": 3.7474372386932373,
      "learning_rate": 1.6509489156189967e-06,
      "loss": 1.2746,
      "step": 7680
    },
    {
      "epoch": 2.6522864538395168,
      "grad_norm": 3.1035244464874268,
      "learning_rate": 1.6348393514891259e-06,
      "loss": 1.4459,
      "step": 7685
    },
    {
      "epoch": 2.654012079378775,
      "grad_norm": 2.7418997287750244,
      "learning_rate": 1.6188061127273334e-06,
      "loss": 1.2391,
      "step": 7690
    },
    {
      "epoch": 2.6557377049180326,
      "grad_norm": 2.8385558128356934,
      "learning_rate": 1.6028492517083078e-06,
      "loss": 1.3026,
      "step": 7695
    },
    {
      "epoch": 2.6574633304572908,
      "grad_norm": 3.012678623199463,
      "learning_rate": 1.5869688205572219e-06,
      "loss": 1.4864,
      "step": 7700
    },
    {
      "epoch": 2.6574633304572908,
      "eval_loss": 1.7477257251739502,
      "eval_runtime": 302.649,
      "eval_samples_per_second": 17.023,
      "eval_steps_per_second": 8.512,
      "step": 7700
    },
    {
      "epoch": 2.659188955996549,
      "grad_norm": 2.467007875442505,
      "learning_rate": 1.571164871149608e-06,
      "loss": 1.3371,
      "step": 7705
    },
    {
      "epoch": 2.6609145815358066,
      "grad_norm": 2.6602132320404053,
      "learning_rate": 1.5554374551111305e-06,
      "loss": 1.2249,
      "step": 7710
    },
    {
      "epoch": 2.6626402070750648,
      "grad_norm": 2.7275383472442627,
      "learning_rate": 1.5397866238174685e-06,
      "loss": 1.2075,
      "step": 7715
    },
    {
      "epoch": 2.6643658326143225,
      "grad_norm": 3.2253663539886475,
      "learning_rate": 1.5242124283941117e-06,
      "loss": 1.4197,
      "step": 7720
    },
    {
      "epoch": 2.6660914581535806,
      "grad_norm": 2.161983013153076,
      "learning_rate": 1.5087149197162253e-06,
      "loss": 1.2607,
      "step": 7725
    },
    {
      "epoch": 2.6678170836928388,
      "grad_norm": 3.245954990386963,
      "learning_rate": 1.4932941484084472e-06,
      "loss": 1.3667,
      "step": 7730
    },
    {
      "epoch": 2.669542709232097,
      "grad_norm": 3.118037462234497,
      "learning_rate": 1.477950164844763e-06,
      "loss": 1.3041,
      "step": 7735
    },
    {
      "epoch": 2.6712683347713546,
      "grad_norm": 3.9366071224212646,
      "learning_rate": 1.462683019148306e-06,
      "loss": 1.4201,
      "step": 7740
    },
    {
      "epoch": 2.6729939603106128,
      "grad_norm": 2.651857852935791,
      "learning_rate": 1.4474927611912188e-06,
      "loss": 1.1705,
      "step": 7745
    },
    {
      "epoch": 2.6747195858498705,
      "grad_norm": 3.1198112964630127,
      "learning_rate": 1.4323794405944753e-06,
      "loss": 1.427,
      "step": 7750
    },
    {
      "epoch": 2.6764452113891286,
      "grad_norm": 3.272847890853882,
      "learning_rate": 1.41734310672772e-06,
      "loss": 1.3734,
      "step": 7755
    },
    {
      "epoch": 2.6781708369283868,
      "grad_norm": 2.9282398223876953,
      "learning_rate": 1.4023838087091178e-06,
      "loss": 1.2404,
      "step": 7760
    },
    {
      "epoch": 2.6798964624676445,
      "grad_norm": 3.303556442260742,
      "learning_rate": 1.3875015954051746e-06,
      "loss": 1.3514,
      "step": 7765
    },
    {
      "epoch": 2.6816220880069026,
      "grad_norm": 3.563194990158081,
      "learning_rate": 1.3726965154306048e-06,
      "loss": 1.2488,
      "step": 7770
    },
    {
      "epoch": 2.6833477135461603,
      "grad_norm": 3.2447662353515625,
      "learning_rate": 1.3579686171481392e-06,
      "loss": 1.4747,
      "step": 7775
    },
    {
      "epoch": 2.6850733390854185,
      "grad_norm": 2.9930331707000732,
      "learning_rate": 1.3433179486683988e-06,
      "loss": 1.2014,
      "step": 7780
    },
    {
      "epoch": 2.6867989646246766,
      "grad_norm": 3.4141058921813965,
      "learning_rate": 1.3287445578497177e-06,
      "loss": 1.3128,
      "step": 7785
    },
    {
      "epoch": 2.6885245901639343,
      "grad_norm": 3.3012917041778564,
      "learning_rate": 1.3142484922979898e-06,
      "loss": 1.3858,
      "step": 7790
    },
    {
      "epoch": 2.6902502157031924,
      "grad_norm": 3.1888387203216553,
      "learning_rate": 1.2998297993665254e-06,
      "loss": 1.1819,
      "step": 7795
    },
    {
      "epoch": 2.69197584124245,
      "grad_norm": 3.4880754947662354,
      "learning_rate": 1.2854885261558675e-06,
      "loss": 1.2736,
      "step": 7800
    },
    {
      "epoch": 2.69197584124245,
      "eval_loss": 1.7462714910507202,
      "eval_runtime": 302.873,
      "eval_samples_per_second": 17.01,
      "eval_steps_per_second": 8.505,
      "step": 7800
    },
    {
      "epoch": 2.6937014667817083,
      "grad_norm": 3.247851610183716,
      "learning_rate": 1.2712247195136833e-06,
      "loss": 1.306,
      "step": 7805
    },
    {
      "epoch": 2.6954270923209664,
      "grad_norm": 2.7619314193725586,
      "learning_rate": 1.2570384260345596e-06,
      "loss": 1.1021,
      "step": 7810
    },
    {
      "epoch": 2.6971527178602246,
      "grad_norm": 3.5503814220428467,
      "learning_rate": 1.2429296920598966e-06,
      "loss": 1.3483,
      "step": 7815
    },
    {
      "epoch": 2.6988783433994823,
      "grad_norm": 2.4623360633850098,
      "learning_rate": 1.2288985636777246e-06,
      "loss": 1.316,
      "step": 7820
    },
    {
      "epoch": 2.7006039689387404,
      "grad_norm": 2.908322811126709,
      "learning_rate": 1.2149450867225742e-06,
      "loss": 1.1992,
      "step": 7825
    },
    {
      "epoch": 2.702329594477998,
      "grad_norm": 3.219142198562622,
      "learning_rate": 1.2010693067753014e-06,
      "loss": 1.3526,
      "step": 7830
    },
    {
      "epoch": 2.7040552200172563,
      "grad_norm": 2.917315721511841,
      "learning_rate": 1.1872712691629762e-06,
      "loss": 1.3624,
      "step": 7835
    },
    {
      "epoch": 2.7057808455565144,
      "grad_norm": 3.804906129837036,
      "learning_rate": 1.1735510189586936e-06,
      "loss": 1.3922,
      "step": 7840
    },
    {
      "epoch": 2.707506471095772,
      "grad_norm": 2.9479763507843018,
      "learning_rate": 1.1599086009814602e-06,
      "loss": 1.3546,
      "step": 7845
    },
    {
      "epoch": 2.7092320966350303,
      "grad_norm": 3.0028648376464844,
      "learning_rate": 1.1463440597960202e-06,
      "loss": 1.1937,
      "step": 7850
    },
    {
      "epoch": 2.710957722174288,
      "grad_norm": 4.146368980407715,
      "learning_rate": 1.132857439712734e-06,
      "loss": 1.4664,
      "step": 7855
    },
    {
      "epoch": 2.712683347713546,
      "grad_norm": 3.342933416366577,
      "learning_rate": 1.119448784787422e-06,
      "loss": 1.2318,
      "step": 7860
    },
    {
      "epoch": 2.7144089732528043,
      "grad_norm": 3.3204121589660645,
      "learning_rate": 1.1061181388212105e-06,
      "loss": 1.3985,
      "step": 7865
    },
    {
      "epoch": 2.716134598792062,
      "grad_norm": 4.057611465454102,
      "learning_rate": 1.0928655453604163e-06,
      "loss": 1.4059,
      "step": 7870
    },
    {
      "epoch": 2.71786022433132,
      "grad_norm": 2.6833837032318115,
      "learning_rate": 1.0796910476963685e-06,
      "loss": 1.4324,
      "step": 7875
    },
    {
      "epoch": 2.719585849870578,
      "grad_norm": 3.1821188926696777,
      "learning_rate": 1.0665946888653023e-06,
      "loss": 1.397,
      "step": 7880
    },
    {
      "epoch": 2.721311475409836,
      "grad_norm": 2.9180734157562256,
      "learning_rate": 1.0535765116481856e-06,
      "loss": 1.3352,
      "step": 7885
    },
    {
      "epoch": 2.723037100949094,
      "grad_norm": 2.414605140686035,
      "learning_rate": 1.040636558570618e-06,
      "loss": 1.2945,
      "step": 7890
    },
    {
      "epoch": 2.7247627264883523,
      "grad_norm": 2.532932758331299,
      "learning_rate": 1.0277748719026453e-06,
      "loss": 1.3077,
      "step": 7895
    },
    {
      "epoch": 2.72648835202761,
      "grad_norm": 3.1564815044403076,
      "learning_rate": 1.0149914936586596e-06,
      "loss": 1.2744,
      "step": 7900
    },
    {
      "epoch": 2.72648835202761,
      "eval_loss": 1.7467494010925293,
      "eval_runtime": 302.7018,
      "eval_samples_per_second": 17.02,
      "eval_steps_per_second": 8.51,
      "step": 7900
    },
    {
      "epoch": 2.728213977566868,
      "grad_norm": 2.2809712886810303,
      "learning_rate": 1.0022864655972519e-06,
      "loss": 1.2358,
      "step": 7905
    },
    {
      "epoch": 2.729939603106126,
      "grad_norm": 3.9367291927337646,
      "learning_rate": 9.896598292210624e-07,
      "loss": 1.2473,
      "step": 7910
    },
    {
      "epoch": 2.731665228645384,
      "grad_norm": 3.2641303539276123,
      "learning_rate": 9.77111625776661e-07,
      "loss": 1.3002,
      "step": 7915
    },
    {
      "epoch": 2.733390854184642,
      "grad_norm": 3.347240924835205,
      "learning_rate": 9.646418962543979e-07,
      "loss": 1.3305,
      "step": 7920
    },
    {
      "epoch": 2.7351164797239,
      "grad_norm": 2.9637081623077393,
      "learning_rate": 9.522506813882892e-07,
      "loss": 1.2782,
      "step": 7925
    },
    {
      "epoch": 2.736842105263158,
      "grad_norm": 3.236978530883789,
      "learning_rate": 9.399380216558646e-07,
      "loss": 1.3103,
      "step": 7930
    },
    {
      "epoch": 2.7385677308024157,
      "grad_norm": 2.9278573989868164,
      "learning_rate": 9.277039572780421e-07,
      "loss": 1.2554,
      "step": 7935
    },
    {
      "epoch": 2.740293356341674,
      "grad_norm": 2.3148257732391357,
      "learning_rate": 9.155485282190041e-07,
      "loss": 1.4038,
      "step": 7940
    },
    {
      "epoch": 2.742018981880932,
      "grad_norm": 3.0308525562286377,
      "learning_rate": 9.034717741860599e-07,
      "loss": 1.3537,
      "step": 7945
    },
    {
      "epoch": 2.7437446074201897,
      "grad_norm": 3.428528070449829,
      "learning_rate": 8.914737346295082e-07,
      "loss": 1.3299,
      "step": 7950
    },
    {
      "epoch": 2.745470232959448,
      "grad_norm": 3.2575759887695312,
      "learning_rate": 8.795544487425255e-07,
      "loss": 1.37,
      "step": 7955
    },
    {
      "epoch": 2.7471958584987055,
      "grad_norm": 2.7480454444885254,
      "learning_rate": 8.677139554610248e-07,
      "loss": 1.212,
      "step": 7960
    },
    {
      "epoch": 2.7489214840379637,
      "grad_norm": 2.8567419052124023,
      "learning_rate": 8.559522934635356e-07,
      "loss": 1.2997,
      "step": 7965
    },
    {
      "epoch": 2.750647109577222,
      "grad_norm": 3.5619914531707764,
      "learning_rate": 8.44269501171066e-07,
      "loss": 1.3848,
      "step": 7970
    },
    {
      "epoch": 2.75237273511648,
      "grad_norm": 3.375089406967163,
      "learning_rate": 8.326656167469998e-07,
      "loss": 1.4069,
      "step": 7975
    },
    {
      "epoch": 2.7540983606557377,
      "grad_norm": 3.3359172344207764,
      "learning_rate": 8.21140678096946e-07,
      "loss": 1.3701,
      "step": 7980
    },
    {
      "epoch": 2.755823986194996,
      "grad_norm": 3.5320780277252197,
      "learning_rate": 8.096947228686342e-07,
      "loss": 1.2399,
      "step": 7985
    },
    {
      "epoch": 2.7575496117342535,
      "grad_norm": 2.542262315750122,
      "learning_rate": 7.983277884517832e-07,
      "loss": 1.1932,
      "step": 7990
    },
    {
      "epoch": 2.7592752372735116,
      "grad_norm": 3.369224786758423,
      "learning_rate": 7.870399119779714e-07,
      "loss": 1.3976,
      "step": 7995
    },
    {
      "epoch": 2.76100086281277,
      "grad_norm": 2.8719027042388916,
      "learning_rate": 7.758311303205368e-07,
      "loss": 1.2621,
      "step": 8000
    },
    {
      "epoch": 2.76100086281277,
      "eval_loss": 1.746012568473816,
      "eval_runtime": 303.3597,
      "eval_samples_per_second": 16.983,
      "eval_steps_per_second": 8.492,
      "step": 8000
    },
    {
      "epoch": 2.7627264883520275,
      "grad_norm": 2.8152048587799072,
      "learning_rate": 7.647014800944347e-07,
      "loss": 1.4538,
      "step": 8005
    },
    {
      "epoch": 2.7644521138912856,
      "grad_norm": 2.5778467655181885,
      "learning_rate": 7.536509976561356e-07,
      "loss": 1.1544,
      "step": 8010
    },
    {
      "epoch": 2.7661777394305433,
      "grad_norm": 3.4285616874694824,
      "learning_rate": 7.426797191034812e-07,
      "loss": 1.2585,
      "step": 8015
    },
    {
      "epoch": 2.7679033649698015,
      "grad_norm": 3.2820611000061035,
      "learning_rate": 7.317876802756086e-07,
      "loss": 1.1263,
      "step": 8020
    },
    {
      "epoch": 2.7696289905090596,
      "grad_norm": 3.254821538925171,
      "learning_rate": 7.20974916752784e-07,
      "loss": 1.3758,
      "step": 8025
    },
    {
      "epoch": 2.771354616048318,
      "grad_norm": 2.706285238265991,
      "learning_rate": 7.102414638563259e-07,
      "loss": 1.2707,
      "step": 8030
    },
    {
      "epoch": 2.7730802415875755,
      "grad_norm": 3.4045801162719727,
      "learning_rate": 6.995873566484707e-07,
      "loss": 1.3572,
      "step": 8035
    },
    {
      "epoch": 2.7748058671268336,
      "grad_norm": 3.807835102081299,
      "learning_rate": 6.890126299322508e-07,
      "loss": 1.4091,
      "step": 8040
    },
    {
      "epoch": 2.7765314926660913,
      "grad_norm": 3.0362889766693115,
      "learning_rate": 6.785173182514121e-07,
      "loss": 1.2528,
      "step": 8045
    },
    {
      "epoch": 2.7782571182053495,
      "grad_norm": 3.258208751678467,
      "learning_rate": 6.681014558902604e-07,
      "loss": 1.3164,
      "step": 8050
    },
    {
      "epoch": 2.7799827437446076,
      "grad_norm": 2.539872646331787,
      "learning_rate": 6.577650768735815e-07,
      "loss": 1.3143,
      "step": 8055
    },
    {
      "epoch": 2.7817083692838653,
      "grad_norm": 2.865149736404419,
      "learning_rate": 6.475082149665162e-07,
      "loss": 1.4393,
      "step": 8060
    },
    {
      "epoch": 2.7834339948231235,
      "grad_norm": 2.701226234436035,
      "learning_rate": 6.373309036744574e-07,
      "loss": 1.2664,
      "step": 8065
    },
    {
      "epoch": 2.785159620362381,
      "grad_norm": 3.251150369644165,
      "learning_rate": 6.272331762429223e-07,
      "loss": 1.3012,
      "step": 8070
    },
    {
      "epoch": 2.7868852459016393,
      "grad_norm": 2.82242488861084,
      "learning_rate": 6.172150656574698e-07,
      "loss": 1.2957,
      "step": 8075
    },
    {
      "epoch": 2.7886108714408975,
      "grad_norm": 3.5091917514801025,
      "learning_rate": 6.072766046435724e-07,
      "loss": 1.3757,
      "step": 8080
    },
    {
      "epoch": 2.790336496980155,
      "grad_norm": 2.684217691421509,
      "learning_rate": 5.974178256665158e-07,
      "loss": 1.2442,
      "step": 8085
    },
    {
      "epoch": 2.7920621225194133,
      "grad_norm": 2.7491936683654785,
      "learning_rate": 5.876387609313055e-07,
      "loss": 1.2303,
      "step": 8090
    },
    {
      "epoch": 2.793787748058671,
      "grad_norm": 2.6238412857055664,
      "learning_rate": 5.779394423825357e-07,
      "loss": 1.3245,
      "step": 8095
    },
    {
      "epoch": 2.795513373597929,
      "grad_norm": 3.2744176387786865,
      "learning_rate": 5.683199017043062e-07,
      "loss": 1.2879,
      "step": 8100
    },
    {
      "epoch": 2.795513373597929,
      "eval_loss": 1.745915174484253,
      "eval_runtime": 302.7377,
      "eval_samples_per_second": 17.018,
      "eval_steps_per_second": 8.509,
      "step": 8100
    },
    {
      "epoch": 2.7972389991371873,
      "grad_norm": 2.648557424545288,
      "learning_rate": 5.587801703201085e-07,
      "loss": 1.2728,
      "step": 8105
    },
    {
      "epoch": 2.7989646246764455,
      "grad_norm": 2.298471450805664,
      "learning_rate": 5.493202793927372e-07,
      "loss": 1.3562,
      "step": 8110
    },
    {
      "epoch": 2.800690250215703,
      "grad_norm": 3.3269402980804443,
      "learning_rate": 5.39940259824162e-07,
      "loss": 1.2211,
      "step": 8115
    },
    {
      "epoch": 2.8024158757549613,
      "grad_norm": 3.1972427368164062,
      "learning_rate": 5.306401422554508e-07,
      "loss": 1.2047,
      "step": 8120
    },
    {
      "epoch": 2.804141501294219,
      "grad_norm": 2.7325594425201416,
      "learning_rate": 5.2141995706666e-07,
      "loss": 1.1948,
      "step": 8125
    },
    {
      "epoch": 2.805867126833477,
      "grad_norm": 2.571769952774048,
      "learning_rate": 5.122797343767388e-07,
      "loss": 1.2811,
      "step": 8130
    },
    {
      "epoch": 2.8075927523727353,
      "grad_norm": 3.425572395324707,
      "learning_rate": 5.032195040434229e-07,
      "loss": 1.284,
      "step": 8135
    },
    {
      "epoch": 2.809318377911993,
      "grad_norm": 2.8867316246032715,
      "learning_rate": 4.942392956631487e-07,
      "loss": 1.2711,
      "step": 8140
    },
    {
      "epoch": 2.811044003451251,
      "grad_norm": 2.6326496601104736,
      "learning_rate": 4.853391385709422e-07,
      "loss": 1.2147,
      "step": 8145
    },
    {
      "epoch": 2.812769628990509,
      "grad_norm": 2.886241912841797,
      "learning_rate": 4.7651906184033865e-07,
      "loss": 1.2844,
      "step": 8150
    },
    {
      "epoch": 2.814495254529767,
      "grad_norm": 2.79862117767334,
      "learning_rate": 4.677790942832799e-07,
      "loss": 1.1862,
      "step": 8155
    },
    {
      "epoch": 2.816220880069025,
      "grad_norm": 3.250882148742676,
      "learning_rate": 4.59119264450017e-07,
      "loss": 1.3558,
      "step": 8160
    },
    {
      "epoch": 2.817946505608283,
      "grad_norm": 2.4910459518432617,
      "learning_rate": 4.505396006290241e-07,
      "loss": 1.3858,
      "step": 8165
    },
    {
      "epoch": 2.819672131147541,
      "grad_norm": 2.6133551597595215,
      "learning_rate": 4.420401308468963e-07,
      "loss": 1.1028,
      "step": 8170
    },
    {
      "epoch": 2.8213977566867987,
      "grad_norm": 2.268930196762085,
      "learning_rate": 4.336208828682714e-07,
      "loss": 1.2,
      "step": 8175
    },
    {
      "epoch": 2.823123382226057,
      "grad_norm": 3.2949979305267334,
      "learning_rate": 4.252818841957301e-07,
      "loss": 1.3576,
      "step": 8180
    },
    {
      "epoch": 2.824849007765315,
      "grad_norm": 3.377671718597412,
      "learning_rate": 4.1702316206970736e-07,
      "loss": 1.3558,
      "step": 8185
    },
    {
      "epoch": 2.826574633304573,
      "grad_norm": 2.6844632625579834,
      "learning_rate": 4.0884474346840064e-07,
      "loss": 1.4184,
      "step": 8190
    },
    {
      "epoch": 2.828300258843831,
      "grad_norm": 2.829542875289917,
      "learning_rate": 4.007466551076977e-07,
      "loss": 1.2986,
      "step": 8195
    },
    {
      "epoch": 2.830025884383089,
      "grad_norm": 3.1532418727874756,
      "learning_rate": 3.927289234410603e-07,
      "loss": 1.3778,
      "step": 8200
    },
    {
      "epoch": 2.830025884383089,
      "eval_loss": 1.74588942527771,
      "eval_runtime": 302.5786,
      "eval_samples_per_second": 17.027,
      "eval_steps_per_second": 8.513,
      "step": 8200
    },
    {
      "epoch": 2.8317515099223467,
      "grad_norm": 4.112152576446533,
      "learning_rate": 3.847915746594627e-07,
      "loss": 1.2945,
      "step": 8205
    },
    {
      "epoch": 2.833477135461605,
      "grad_norm": 2.8770530223846436,
      "learning_rate": 3.769346346913061e-07,
      "loss": 1.3216,
      "step": 8210
    },
    {
      "epoch": 2.835202761000863,
      "grad_norm": 3.1246297359466553,
      "learning_rate": 3.6915812920230995e-07,
      "loss": 1.3189,
      "step": 8215
    },
    {
      "epoch": 2.8369283865401207,
      "grad_norm": 2.667628049850464,
      "learning_rate": 3.614620835954513e-07,
      "loss": 1.2554,
      "step": 8220
    },
    {
      "epoch": 2.838654012079379,
      "grad_norm": 3.313997745513916,
      "learning_rate": 3.538465230108784e-07,
      "loss": 1.1747,
      "step": 8225
    },
    {
      "epoch": 2.8403796376186365,
      "grad_norm": 3.2244760990142822,
      "learning_rate": 3.4631147232582205e-07,
      "loss": 1.2989,
      "step": 8230
    },
    {
      "epoch": 2.8421052631578947,
      "grad_norm": 3.180757522583008,
      "learning_rate": 3.3885695615450964e-07,
      "loss": 1.2198,
      "step": 8235
    },
    {
      "epoch": 2.843830888697153,
      "grad_norm": 3.154954433441162,
      "learning_rate": 3.3148299884810383e-07,
      "loss": 1.3091,
      "step": 8240
    },
    {
      "epoch": 2.8455565142364105,
      "grad_norm": 2.940537929534912,
      "learning_rate": 3.241896244946002e-07,
      "loss": 1.3292,
      "step": 8245
    },
    {
      "epoch": 2.8472821397756687,
      "grad_norm": 2.992236852645874,
      "learning_rate": 3.1697685691876864e-07,
      "loss": 1.2538,
      "step": 8250
    },
    {
      "epoch": 2.8490077653149264,
      "grad_norm": 2.8604748249053955,
      "learning_rate": 3.098447196820592e-07,
      "loss": 1.2452,
      "step": 8255
    },
    {
      "epoch": 2.8507333908541845,
      "grad_norm": 3.142718553543091,
      "learning_rate": 3.0279323608254083e-07,
      "loss": 1.3837,
      "step": 8260
    },
    {
      "epoch": 2.8524590163934427,
      "grad_norm": 2.569880485534668,
      "learning_rate": 2.958224291548045e-07,
      "loss": 1.1868,
      "step": 8265
    },
    {
      "epoch": 2.854184641932701,
      "grad_norm": 4.132168769836426,
      "learning_rate": 2.8893232166991024e-07,
      "loss": 1.3161,
      "step": 8270
    },
    {
      "epoch": 2.8559102674719585,
      "grad_norm": 3.385103464126587,
      "learning_rate": 2.8212293613530393e-07,
      "loss": 1.3549,
      "step": 8275
    },
    {
      "epoch": 2.8576358930112167,
      "grad_norm": 3.2801458835601807,
      "learning_rate": 2.7539429479473135e-07,
      "loss": 1.2818,
      "step": 8280
    },
    {
      "epoch": 2.8593615185504744,
      "grad_norm": 3.151599407196045,
      "learning_rate": 2.68746419628188e-07,
      "loss": 1.256,
      "step": 8285
    },
    {
      "epoch": 2.8610871440897325,
      "grad_norm": 2.82962965965271,
      "learning_rate": 2.621793323518307e-07,
      "loss": 1.2708,
      "step": 8290
    },
    {
      "epoch": 2.8628127696289907,
      "grad_norm": 2.844480276107788,
      "learning_rate": 2.556930544179104e-07,
      "loss": 1.1172,
      "step": 8295
    },
    {
      "epoch": 2.8645383951682484,
      "grad_norm": 2.8322527408599854,
      "learning_rate": 2.4928760701471157e-07,
      "loss": 1.1425,
      "step": 8300
    },
    {
      "epoch": 2.8645383951682484,
      "eval_loss": 1.7461506128311157,
      "eval_runtime": 302.7293,
      "eval_samples_per_second": 17.019,
      "eval_steps_per_second": 8.509,
      "step": 8300
    },
    {
      "epoch": 2.8662640207075065,
      "grad_norm": 2.707888603210449,
      "learning_rate": 2.429630110664688e-07,
      "loss": 1.23,
      "step": 8305
    },
    {
      "epoch": 2.8679896462467642,
      "grad_norm": 3.161686420440674,
      "learning_rate": 2.3671928723330294e-07,
      "loss": 1.3739,
      "step": 8310
    },
    {
      "epoch": 2.8697152717860224,
      "grad_norm": 3.106483221054077,
      "learning_rate": 2.3055645591116282e-07,
      "loss": 1.2505,
      "step": 8315
    },
    {
      "epoch": 2.8714408973252805,
      "grad_norm": 2.910766124725342,
      "learning_rate": 2.2447453723174193e-07,
      "loss": 1.273,
      "step": 8320
    },
    {
      "epoch": 2.8731665228645387,
      "grad_norm": 3.430920124053955,
      "learning_rate": 2.1847355106242862e-07,
      "loss": 1.2715,
      "step": 8325
    },
    {
      "epoch": 2.8748921484037964,
      "grad_norm": 2.6497888565063477,
      "learning_rate": 2.125535170062365e-07,
      "loss": 1.2123,
      "step": 8330
    },
    {
      "epoch": 2.8766177739430545,
      "grad_norm": 2.8372020721435547,
      "learning_rate": 2.0671445440172966e-07,
      "loss": 1.2431,
      "step": 8335
    },
    {
      "epoch": 2.878343399482312,
      "grad_norm": 2.6407997608184814,
      "learning_rate": 2.009563823229782e-07,
      "loss": 1.2402,
      "step": 8340
    },
    {
      "epoch": 2.8800690250215704,
      "grad_norm": 1.946526288986206,
      "learning_rate": 1.9527931957947777e-07,
      "loss": 1.1735,
      "step": 8345
    },
    {
      "epoch": 2.8817946505608285,
      "grad_norm": 2.206918954849243,
      "learning_rate": 1.8968328471610232e-07,
      "loss": 1.1333,
      "step": 8350
    },
    {
      "epoch": 2.883520276100086,
      "grad_norm": 2.3508994579315186,
      "learning_rate": 1.8416829601303197e-07,
      "loss": 1.2308,
      "step": 8355
    },
    {
      "epoch": 2.8852459016393444,
      "grad_norm": 2.513192653656006,
      "learning_rate": 1.7873437148570594e-07,
      "loss": 1.2521,
      "step": 8360
    },
    {
      "epoch": 2.886971527178602,
      "grad_norm": 2.954352378845215,
      "learning_rate": 1.7338152888475013e-07,
      "loss": 1.2035,
      "step": 8365
    },
    {
      "epoch": 2.88869715271786,
      "grad_norm": 3.331364870071411,
      "learning_rate": 1.6810978569593016e-07,
      "loss": 1.2397,
      "step": 8370
    },
    {
      "epoch": 2.8904227782571184,
      "grad_norm": 3.235987901687622,
      "learning_rate": 1.629191591400875e-07,
      "loss": 1.2364,
      "step": 8375
    },
    {
      "epoch": 2.892148403796376,
      "grad_norm": 2.830028533935547,
      "learning_rate": 1.5780966617308656e-07,
      "loss": 1.324,
      "step": 8380
    },
    {
      "epoch": 2.893874029335634,
      "grad_norm": 2.929511308670044,
      "learning_rate": 1.527813234857567e-07,
      "loss": 1.2859,
      "step": 8385
    },
    {
      "epoch": 2.895599654874892,
      "grad_norm": 2.8447911739349365,
      "learning_rate": 1.4783414750383918e-07,
      "loss": 1.2228,
      "step": 8390
    },
    {
      "epoch": 2.89732528041415,
      "grad_norm": 2.6943857669830322,
      "learning_rate": 1.429681543879402e-07,
      "loss": 1.3099,
      "step": 8395
    },
    {
      "epoch": 2.899050905953408,
      "grad_norm": 2.633354902267456,
      "learning_rate": 1.381833600334559e-07,
      "loss": 1.2659,
      "step": 8400
    },
    {
      "epoch": 2.899050905953408,
      "eval_loss": 1.7460445165634155,
      "eval_runtime": 302.7142,
      "eval_samples_per_second": 17.019,
      "eval_steps_per_second": 8.51,
      "step": 8400
    },
    {
      "epoch": 2.9007765314926663,
      "grad_norm": 2.687330722808838,
      "learning_rate": 1.3347978007055562e-07,
      "loss": 1.267,
      "step": 8405
    },
    {
      "epoch": 2.902502157031924,
      "grad_norm": 2.281433343887329,
      "learning_rate": 1.2885742986409322e-07,
      "loss": 1.2542,
      "step": 8410
    },
    {
      "epoch": 2.904227782571182,
      "grad_norm": 2.22752046585083,
      "learning_rate": 1.2431632451359032e-07,
      "loss": 1.4975,
      "step": 8415
    },
    {
      "epoch": 2.90595340811044,
      "grad_norm": 3.264296293258667,
      "learning_rate": 1.1985647885315866e-07,
      "loss": 1.2743,
      "step": 8420
    },
    {
      "epoch": 2.907679033649698,
      "grad_norm": 3.1514434814453125,
      "learning_rate": 1.1547790745147502e-07,
      "loss": 1.1793,
      "step": 8425
    },
    {
      "epoch": 2.909404659188956,
      "grad_norm": 3.0354130268096924,
      "learning_rate": 1.1118062461171475e-07,
      "loss": 1.1817,
      "step": 8430
    },
    {
      "epoch": 2.911130284728214,
      "grad_norm": 2.903738260269165,
      "learning_rate": 1.0696464437152109e-07,
      "loss": 1.195,
      "step": 8435
    },
    {
      "epoch": 2.912855910267472,
      "grad_norm": 2.8032896518707275,
      "learning_rate": 1.0282998050294146e-07,
      "loss": 1.3073,
      "step": 8440
    },
    {
      "epoch": 2.9145815358067297,
      "grad_norm": 2.947875738143921,
      "learning_rate": 9.877664651240238e-08,
      "loss": 1.2365,
      "step": 8445
    },
    {
      "epoch": 2.916307161345988,
      "grad_norm": 2.5088953971862793,
      "learning_rate": 9.480465564065399e-08,
      "loss": 1.3162,
      "step": 8450
    },
    {
      "epoch": 2.918032786885246,
      "grad_norm": 2.810068368911743,
      "learning_rate": 9.091402086272293e-08,
      "loss": 1.3502,
      "step": 8455
    },
    {
      "epoch": 2.9197584124245037,
      "grad_norm": 3.4656906127929688,
      "learning_rate": 8.710475488788173e-08,
      "loss": 1.3522,
      "step": 8460
    },
    {
      "epoch": 2.921484037963762,
      "grad_norm": 2.85813307762146,
      "learning_rate": 8.337687015959605e-08,
      "loss": 1.351,
      "step": 8465
    },
    {
      "epoch": 2.9232096635030196,
      "grad_norm": 3.2341206073760986,
      "learning_rate": 7.973037885549428e-08,
      "loss": 1.2393,
      "step": 8470
    },
    {
      "epoch": 2.9249352890422777,
      "grad_norm": 3.335425615310669,
      "learning_rate": 7.616529288731467e-08,
      "loss": 1.3281,
      "step": 8475
    },
    {
      "epoch": 2.926660914581536,
      "grad_norm": 2.7112321853637695,
      "learning_rate": 7.268162390088595e-08,
      "loss": 1.3449,
      "step": 8480
    },
    {
      "epoch": 2.928386540120794,
      "grad_norm": 3.0933942794799805,
      "learning_rate": 6.927938327606354e-08,
      "loss": 1.3053,
      "step": 8485
    },
    {
      "epoch": 2.9301121656600517,
      "grad_norm": 3.0589547157287598,
      "learning_rate": 6.595858212671835e-08,
      "loss": 1.3599,
      "step": 8490
    },
    {
      "epoch": 2.93183779119931,
      "grad_norm": 2.684643030166626,
      "learning_rate": 6.271923130068136e-08,
      "loss": 1.3304,
      "step": 8495
    },
    {
      "epoch": 2.9335634167385676,
      "grad_norm": 2.46567964553833,
      "learning_rate": 5.956134137971858e-08,
      "loss": 1.2662,
      "step": 8500
    },
    {
      "epoch": 2.9335634167385676,
      "eval_loss": 1.7460384368896484,
      "eval_runtime": 302.6279,
      "eval_samples_per_second": 17.024,
      "eval_steps_per_second": 8.512,
      "step": 8500
    },
    {
      "epoch": 2.9352890422778257,
      "grad_norm": 3.061760425567627,
      "learning_rate": 5.648492267949501e-08,
      "loss": 1.2588,
      "step": 8505
    },
    {
      "epoch": 2.937014667817084,
      "grad_norm": 2.8344345092773438,
      "learning_rate": 5.348998524953019e-08,
      "loss": 1.3009,
      "step": 8510
    },
    {
      "epoch": 2.9387402933563416,
      "grad_norm": 3.0412487983703613,
      "learning_rate": 5.057653887318714e-08,
      "loss": 1.3549,
      "step": 8515
    },
    {
      "epoch": 2.9404659188955997,
      "grad_norm": 2.789970874786377,
      "learning_rate": 4.774459306761958e-08,
      "loss": 1.3736,
      "step": 8520
    },
    {
      "epoch": 2.9421915444348574,
      "grad_norm": 3.357017993927002,
      "learning_rate": 4.499415708374977e-08,
      "loss": 1.2908,
      "step": 8525
    },
    {
      "epoch": 2.9439171699741156,
      "grad_norm": 3.0482895374298096,
      "learning_rate": 4.232523990624071e-08,
      "loss": 1.2974,
      "step": 8530
    },
    {
      "epoch": 2.9456427955133737,
      "grad_norm": 3.509882688522339,
      "learning_rate": 3.973785025345733e-08,
      "loss": 1.304,
      "step": 8535
    },
    {
      "epoch": 2.9473684210526314,
      "grad_norm": 3.2232120037078857,
      "learning_rate": 3.72319965774498e-08,
      "loss": 1.2502,
      "step": 8540
    },
    {
      "epoch": 2.9490940465918896,
      "grad_norm": 3.25995135307312,
      "learning_rate": 3.480768706391746e-08,
      "loss": 1.4265,
      "step": 8545
    },
    {
      "epoch": 2.9508196721311473,
      "grad_norm": 2.841832160949707,
      "learning_rate": 3.246492963218939e-08,
      "loss": 1.3818,
      "step": 8550
    },
    {
      "epoch": 2.9525452976704054,
      "grad_norm": 3.956585645675659,
      "learning_rate": 3.020373193518555e-08,
      "loss": 1.4131,
      "step": 8555
    },
    {
      "epoch": 2.9542709232096636,
      "grad_norm": 3.225752353668213,
      "learning_rate": 2.802410135941125e-08,
      "loss": 1.4041,
      "step": 8560
    },
    {
      "epoch": 2.9559965487489217,
      "grad_norm": 3.4821903705596924,
      "learning_rate": 2.5926045024909917e-08,
      "loss": 1.2613,
      "step": 8565
    },
    {
      "epoch": 2.9577221742881794,
      "grad_norm": 3.167692184448242,
      "learning_rate": 2.3909569785263154e-08,
      "loss": 1.3918,
      "step": 8570
    },
    {
      "epoch": 2.9594477998274376,
      "grad_norm": 3.4492099285125732,
      "learning_rate": 2.1974682227551836e-08,
      "loss": 1.3193,
      "step": 8575
    },
    {
      "epoch": 2.9611734253666953,
      "grad_norm": 2.6403005123138428,
      "learning_rate": 2.012138867233948e-08,
      "loss": 1.29,
      "step": 8580
    },
    {
      "epoch": 2.9628990509059534,
      "grad_norm": 2.961740732192993,
      "learning_rate": 1.834969517365004e-08,
      "loss": 1.1608,
      "step": 8585
    },
    {
      "epoch": 2.9646246764452115,
      "grad_norm": 2.759666681289673,
      "learning_rate": 1.6659607518959563e-08,
      "loss": 1.3316,
      "step": 8590
    },
    {
      "epoch": 2.9663503019844693,
      "grad_norm": 2.9910378456115723,
      "learning_rate": 1.5051131229157355e-08,
      "loss": 1.3103,
      "step": 8595
    },
    {
      "epoch": 2.9680759275237274,
      "grad_norm": 2.850334882736206,
      "learning_rate": 1.3524271558540414e-08,
      "loss": 1.3609,
      "step": 8600
    },
    {
      "epoch": 2.9680759275237274,
      "eval_loss": 1.7459250688552856,
      "eval_runtime": 302.6337,
      "eval_samples_per_second": 17.024,
      "eval_steps_per_second": 8.512,
      "step": 8600
    },
    {
      "epoch": 2.969801553062985,
      "grad_norm": 3.5316452980041504,
      "learning_rate": 1.2079033494802327e-08,
      "loss": 1.373,
      "step": 8605
    },
    {
      "epoch": 2.9715271786022432,
      "grad_norm": 2.989298105239868,
      "learning_rate": 1.0715421758999976e-08,
      "loss": 1.2092,
      "step": 8610
    },
    {
      "epoch": 2.9732528041415014,
      "grad_norm": 3.2370920181274414,
      "learning_rate": 9.433440805547977e-09,
      "loss": 1.3444,
      "step": 8615
    },
    {
      "epoch": 2.9749784296807595,
      "grad_norm": 2.9057669639587402,
      "learning_rate": 8.233094822210352e-09,
      "loss": 1.218,
      "step": 8620
    },
    {
      "epoch": 2.9767040552200172,
      "grad_norm": 2.7387428283691406,
      "learning_rate": 7.11438773007278e-09,
      "loss": 1.2718,
      "step": 8625
    },
    {
      "epoch": 2.9784296807592754,
      "grad_norm": 3.5821073055267334,
      "learning_rate": 6.077323183539818e-09,
      "loss": 1.3101,
      "step": 8630
    },
    {
      "epoch": 2.980155306298533,
      "grad_norm": 2.813826560974121,
      "learning_rate": 5.121904570315472e-09,
      "loss": 1.346,
      "step": 8635
    },
    {
      "epoch": 2.9818809318377912,
      "grad_norm": 2.7434582710266113,
      "learning_rate": 4.248135011405974e-09,
      "loss": 1.1318,
      "step": 8640
    },
    {
      "epoch": 2.9836065573770494,
      "grad_norm": 2.9295380115509033,
      "learning_rate": 3.4560173610892476e-09,
      "loss": 1.1988,
      "step": 8645
    },
    {
      "epoch": 2.985332182916307,
      "grad_norm": 3.09957218170166,
      "learning_rate": 2.7455542069260155e-09,
      "loss": 1.2368,
      "step": 8650
    },
    {
      "epoch": 2.9870578084555652,
      "grad_norm": 2.7835121154785156,
      "learning_rate": 2.116747869734814e-09,
      "loss": 1.3579,
      "step": 8655
    },
    {
      "epoch": 2.988783433994823,
      "grad_norm": 2.96113657951355,
      "learning_rate": 1.569600403594773e-09,
      "loss": 1.3144,
      "step": 8660
    },
    {
      "epoch": 2.990509059534081,
      "grad_norm": 2.996080160140991,
      "learning_rate": 1.1041135958345105e-09,
      "loss": 1.2397,
      "step": 8665
    },
    {
      "epoch": 2.9922346850733392,
      "grad_norm": 3.30289888381958,
      "learning_rate": 7.202889670293588e-10,
      "loss": 1.3182,
      "step": 8670
    },
    {
      "epoch": 2.993960310612597,
      "grad_norm": 4.138028621673584,
      "learning_rate": 4.181277709930376e-10,
      "loss": 1.2938,
      "step": 8675
    },
    {
      "epoch": 2.995685936151855,
      "grad_norm": 3.118476390838623,
      "learning_rate": 1.9763099477210223e-10,
      "loss": 1.2355,
      "step": 8680
    },
    {
      "epoch": 2.997411561691113,
      "grad_norm": 2.582500457763672,
      "learning_rate": 5.879935865149567e-11,
      "loss": 1.2438,
      "step": 8685
    },
    {
      "epoch": 2.999137187230371,
      "grad_norm": 2.6324825286865234,
      "learning_rate": 1.6333161406700826e-12,
      "loss": 1.2052,
      "step": 8690
    },
    {
      "epoch": 2.9994823123382224,
      "step": 8691,
      "total_flos": 4.101086246859571e+17,
      "train_loss": 1.468505113024169,
      "train_runtime": 47587.6325,
      "train_samples_per_second": 2.923,
      "train_steps_per_second": 0.183
    }
  ],
  "logging_steps": 5,
  "max_steps": 8691,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 3,
  "save_steps": 100,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": true
      },
      "attributes": {}
    }
  },
  "total_flos": 4.101086246859571e+17,
  "train_batch_size": 2,
  "trial_name": null,
  "trial_params": null
}
